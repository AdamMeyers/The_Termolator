
  
    
      
        Background
        There are a growing number of RNA gene families and RNA
        motifs [ 1 2 ] . Many (though not all) RNAs conserve a
        base-paired RNA secondary structure. Computational analyses
        of RNA sequence families are more powerful if they take
        into account both primary sequence and secondary structure
        consensus [ 3 4 ] .
        Some excellent approaches have been developed for
        database searching with RNA secondary structure consensus
        patterns. Exact- and approximate-match pattern searches
        (analogous to PROSITE patterns for proteins) have been
        extended to allow patterns to specify long-range base
        pairing constraints [ 5 6 ] . In several cases, specialized
        programs have been developed to recognize specific RNA
        structures [ 4 ] - for example, programs exist for
        detecting transfer RNA genes [ 7 8 9 ] , group I catalytic
        introns [ 10 ] , and small nucleolar RNAs [ 11 12 ] . All
        of these approaches, though powerful, lack generality, and
        they require expert knowledge about each particular RNA
        family of interest.
        In primary sequence analysis, the most useful analysis
        techniques are general primary sequence alignment
        algorithms with probabilistically based scoring systems -
        for example, the BLAST [ 13 ] , FASTA [ 14 ] , or CLUSTALW
        [ 15 ] algorithms, and the PAM [ 16 ] or BLOSUM [ 17 ]
        score matrices. Unlike specialized programs, a general
        alignment algorithm can be applied to find homologs of any
        query sequence(s). Unlike pattern searches, which give
        yes/no answers for whether a candidate sequence is a match,
        a scoring system gives a meaningful score that allows
        ranking candidate hits by their statistical significance.
        It is of interest to develop general alignment algorithms
        for RNA secondary structures.
        The problem I consider here is as follows. I am given a
        multiple alignment of an RNA sequence family for which I
        know the consensus secondary structure. I want to search a
        sequence database for homologs that significantly match the
        sequence 
        and structure of my query. The
        sequence analysis analogue is the use of profile hidden
        Markov models (profile HMMs) to model multiple alignments
        of conserved protein domains, and to discover new
        homologues in sequence databases [ 18 19 ] . For instance,
        if we had an RNA structure equivalent of the HMMER profile
        HMM program suite http://hmmer.wustl.edu/it would be
        possible to develop and efficiently maintain databases of
        conserved RNA structures and multiple alignments, analogous
        to the Pfam or SMART databases of conserved protein domains
        [ 20 21 ] .
        Stochastic context free grammar (SCFG) algorithms
        provide a general approach to RNA structure alignment [ 22
        23 24 ] . SCFGs allow the strong pairwise residue
        correlations in non-pseudoknotted RNA secondary structure
        to be taken into account in RNA alignments. SCFGs can be
        aligned to sequences using a dynamic programming algorithm
        that guarantees finding a mathematically optimal solution
        in polynomial time. SCFG alignment algorithms can be
        thought of as an extension of sequence alignment algorithms
        (particularly those with fully probabilistic, hidden Markov
        model formulations) into an additional dimension necessary
        to deal with 2D RNA secondary structure.
        While SCFGs provide a natural mathematical framework for
        RNA secondary structure alignment problems, SCFG algorithms
        have high computational complexity that has impeded their
        practical application. Optimal SCFG-based structural
        alignment of an RNA structure to a sequence costs 
        O ( 
        N 3) memory and 
        O ( 
        N 4) time for a sequence of length 
        N, compared to 
        O ( 
        N 2) memory and time for sequence
        alignment algorithms. (Corpet and Michot described a
        program that implements a different general dynamic
        programming algorithm for RNA alignment; their algorithm
        solves the same problem but even less efficiently,
        requiring 
        O ( 
        N 4) memory and 
        O ( 
        N 5) time [ 25 ] .) SCFG-based
        alignments of small structural RNAs are feasible. Using my
        COVE software
        http://www.genetics.wustl.edu/eddy/software#cove, transfer
        RNA alignments (~75 nucleotides) take about 0.2 cpu second
        and 3 Mb of memory. Most genome centers now use an
        COVE-based search program, tRNAscan-SE, for annotating
        transfer RNA genes [ 9 ] . However, many larger RNAs of
        interest are OUTSIDE the capabilities of the standard SCFG
        alignment algorithm. Alignment of a small subunit (SSU)
        ribosomal RNA sequence to the SSU rRNA consensus structure
        would take about 23 GB of RAM and an hour of CPU time.
        Applying SCFG methods to RNAs this large has required
        clever heuristics, such as using a precalculation of
        confidently predicted regions of primary sequence alignment
        to strongly constrain which parts of the SCFG dynamic
        programming matrix need to be calculated [ 26 ] . The steep
        memory requirement remains a significant barrier to the
        practicality of SCFG algorithms.
        Notredame 
        et al. pointed specifically to this
        problem [ 27 ] . They described RAGA, a program that uses a
        genetic algorithm (GA) to optimize a pairwise RNA alignment
        using an objective function that includes base pairing
        terms. Because GAs have an O( 
        N ) memory requirement, RAGA can find
        reasonable solutions for large RNA alignment problems,
        including ribosomal RNA alignments. A different
        memory-efficient approach has also been described [ 28 29 ]
        . However, both approaches are approximate and cannot
        guarantee a mathematically optimal solution, in contrast to
        the (mathematically) optimal but more expensive dynamic
        programming approaches.
        Here, I introduce a dynamic programming solution to the
        problem of structural alignment of large RNAs. The central
        idea is a divide and conquer strategy. For linear sequence
        alignment, a divide and conquer algorithm was introduced by
        Hirschberg [ 30 ] , an algorithm known in the computational
        biology community as the Myers/Miller algorithm [ 31 ] .
        (Ironically, at the time, dynamic programming methods for
        optimal sequence alignment were well known, but were
        considered impractical on 1970's era computers because of
        the "extreme" 
        O ( 
        N 2) memory requirement.)
        Myers/Miller reduces the memory complexity of a dynamic
        programming sequence alignment algorithm from 
        O ( 
        N 2) to 
        O ( 
        N ), at the cost of a roughly
        two-fold increase in CPU time. Here I show that a divide
        and conquer strategy can also be applied to the RNA
        structural alignment problem, greatly reducing the memory
        requirement of SCFG alignments and making optimal
        structural alignment of large RNAs possible.
        I will strictly be dealing with the problem of aligning
        a target sequence of unknown secondary structure to a query
        of known RNA structure. By "secondary structure" I mean
        nested (nonpseudoknotted) pairwise RNA secondary structure
        interactions, primarily Watson-Crick base pairs but also
        permitting noncanonical base pairs. This RNA structural
        alignment problem is different from the problem of aligning
        two known RNA secondary structures together [ 32 ] , and
        from the problem of aligning two RNA sequences of unknown
        structure together under a secondary structure-aware
        scoring system [ 33 34 35 36 37 ] .
      
      
        Algorithm
        
          Prelude: the simpler case of sequence
          alignment
          The essential concepts of a divide and conquer
          alignment algorithm are most easily understood for the
          case of linear sequence alignment [ 30 31 ] .
          Dynamic programming (DP) algorithms for sequence
          alignment fill in an 
          N × 
          M DP matrix of scores 
          F(i,j) for two sequences of lengths
          
          N and 
          M ( 
          N   
          M ) [ 38 39 ] . Each score 
          F(i,j) is the score of the optimal
          alignment of prefix 
          x 
          1 .. 
          x 
          
            i 
           of one sequence to prefix 
          y 
          1 .. 
          y 
          
            j 
           of the other. These scores are calculated
          iteratively, e.g. for global (Needleman/Wunsch)
          alignment:
          
          At the end, 
          F(N, M) contains the score of the
          optimal alignment. The alignment itself is recovered by
          tracing the individual optimal steps backwards through
          the matrix, starting from cell ( 
          N,M ). The algorithm is 
          O(NM) in both time and memory.
          If we are only interested in the score, not the
          alignment itself, the whole 
          F matrix does not have to be kept
          in memory. The iterative calculation only depends on the
          current and previous row of the matrix. Keeping two rows
          in memory suffices (in fact, a compulsively efficient
          implementation can get away with 
          N + 1 cells). A score-only
          calculation can be done in 
          O ( 
          N ) space.
          The fill stage of DP alignment algorithms may be run
          either forwards and backwards. We can just as easily
          calculate the optimal score 
          B(i, j) of the best alignment of
          the 
          suffix i + 1.. 
          N of sequence 1 to the suffix 
          j + 1.. 
          M of sequence 2, until one obtains
          B(0,0), the overall optimal score - the same number as 
          F(N,M). 
          The sum of 
          F(i,j) and 
          B(i,j) at any cell in the optimal
          path through the DP matrix is also the optimal overall
          alignment score. More generally, 
          F(i,j) + B(i,j) at any cell ( 
          i,j ) is the score of the best
          alignment that uses that cell. Therefore, since we know
          the optimal alignment must pass through any given row 
          i somewhere, we can pick some row 
          i in the middle of sequence 
          x, run the forward calculation to 
          i to obtain row 
          F(i), run the backwards calculation
          back to 
          i to get row 
          B(i), and then find argmax 
          
            j 
           
          F ( 
          i , 
          j )+ 
          B ( 
          i , 
          j ). Now I know the optimal
          alignment passes through cell ( 
          i,j ). (For clarity, I am leaving
          out details of how indels and local alignments are
          handled.)
          This divides the alignment into two smaller alignment
          problems, and these smaller problems can themselves be
          subdivided by the same trick. Thus, the complete optimal
          alignment can be found by a recursive series of split
          point calculations. Although this seems laborious - each
          calculation is giving us only a single point in the
          alignment - if we choose our split row 
          i to be in the middle, the size of
          the two smaller DP problems is decreased by about 4-fold
          at each split. A complete alignment thus costs only about
          times as much CPU time as doing the alignment in a single
          DP matrix calculation, but the algorithm is 
          O ( 
          N ) in memory.
          A standard dynamic programming alignment algorithm for
          SCFGs is the Cocke-Younger-Kasami (CYK) algorithm, which
          finds an optimal parse tree (e.g. alignment) for a model
          and a sequence [ 24 40 41 42 ] . (CYK is usually
          described in the literature as a dynamic programming
          recognition algorithm for nonstochastic CFGs in Chomsky
          normal form, rather than as a dynamic programming parsing
          algorithm for SCFGs in any form. The use of the name
          "CYK" here is therefore a little imprecise [ 24 ] .) CYK
          can be run in a memory-saving "score only" mode. The DP
          matrix for CYK can also be filled in two opposite
          directions - either "inside" or "outside", analogous to
          forward and backward DP matrix fills for linear sequence
          alignment. I will refer to these algorithms as CYK/inside
          and CYK/outside (or just inside and outside), but readers
          familiar with SCFG algorithms should not confuse them
          with the SCFG Inside and Outside algorithms [ 43 44 ]
          which sum over all possible parse trees rather than
          finding one optimal parse tree. I am always talking about
          the CYK algorithm in this paper, and by "inside" and
          "outside" I am only referring generically to the
          direction of the CYK DP calculation.
          The CYK/inside and CYK/outside algorithms are not as
          nicely symmetrical as the forward and backward DP fills
          are in sequence alignment algorithms. The splitting
          procedure that one obtains does not generate identical
          types of subproblems, so the divide and conquer procedure
          for SCFG-based RNA alignment is not as obvious.
        
        
          Definition and construction of a covariance
          model
          
            Definition of a stochastic context free
            grammar
            A stochastic context free grammar (SCFG) consists of
            the following:
            • 
            M different nonterminals (here
            called 
            states ). I will use capital
            letters to refer to specific nonterminals; 
            V and 
            Y will be used to refer
            generically to unspecified nonterminals.
            • 
            K different terminal symbols
            (e.g. the observable alphabet, a,c,g,u for RNA). I will
            use small letters 
            a, b to refer generically to
            terminal symbols.
            • a number of 
            production rules of the form: 
            V → γ, where γ can be any string
            of nonterminal and/or terminal symbols, including (as a
            special case) the empty string ε.
            • Each production rule is associated with a
            probability, such that the sum of the production
            probabilities for any given nonterminal 
            V is equal to 1.
          
          
            SCFG productions allowed in covariance
            models
            A covariance model is a specific repetitive "profile
            SCFG" architecture consisting of groups of model states
            that are associated with base pairs and single-stranded
            positions in an RNA secondary structure consensus. A
            covariance model has seven types of states and
            production rules (Table 1).
            Each overall production probability is the
            independent product of an emission probability 
            e 
            
              v 
             and a transition probability 
            t 
            
              v 
             , both of which are position-dependent parameters
            that depend on the state 
            v (analogous to hidden Markov
            models). For example, a particular pair (P) state 
            v produces two correlated letters
            
            a and 
            b (e.g. one of 16 possible base
            pairs) with probability 
            e 
            
              v 
             ( 
            a , 
            b ) and transits to one of
            several possible new states 
            Y of various types with
            probability 
            t 
            
              v 
             ( 
            Y ). A bifurcation (B) state
            splits into two new start ( 
            S ) states with probability 1.
            The E state is a special case ε production that
            terminates a derivation.
            A CM consists of many states of these seven basic
            types, each with its own emission and transition
            probability distributions, and its own set of states
            that it can transition to. Consensus base pairs will be
            modeled by P states, consensus single stranded residues
            by L and R states, insertions relative to the consensus
            by more L and R states, deletions relative to consensus
            by D states, and the branching topology of the RNA
            secondary structure by B, S, and E states. The
            procedure for starting from an input multiple alignment
            and determining how many states, what types of states,
            and how they are interconnected by transition
            probabilities is described next.
          
          
            From consensus structural alignment to guide
            tree
            Figure 1shows an example input file: a multiple
            sequence alignment of homologous RNAs, with a line
            describing the consensus RNA secondary structure. The
            first step of building a CM is to produce a binary 
            guide tree of nodes representing
            the consensus secondary structure. The guide tree is a
            parse tree for the consensus structure, with nodes as
            nonterminals and alignment columns as terminals.
            The guide tree has eight types of nodes (Table
            2).
            These consensus node types correspond closely with a
            CM's final state types. Each node will eventually
            contain one or more states. The guide tree deals with
            the consensus structure. For individual sequences, we
            will need to deal with insertions and deletions with
            respect to this consensus. The guide tree is the
            skeleton on which we will organize the CM. For example,
            a MATP node will contain a P-type state to model a
            consensus base pair; but it will also contain several
            other states to model infrequent insertions and
            deletions at or adjacent to this pair.
            The input alignment is first used to construct a
            consensus secondary structure (Figure 2) that defines
            which aligned columns will be ignored as non-consensus
            (and later modeled as insertions relative to the
            consensus), and which consensus alignment columns are
            base-paired to each other. Here I assume that both the
            structural annotation and the labeling of insert versus
            consensus columns is given in the input file, as shown
            in the line marked "[structure]" in the alignment in
            Figure 1. Alternatively, automatic methods might be
            employed. A consensus structure could be predicted from
            comparative analysis of the alignment [ 22 45 46 ] .
            The consensus columns could be chosen as those columns
            with less than a certain fraction of gap symbols, or by
            a maximum likelihood criterion, as is done for profile
            HMM construction [ 18 24 ] .
            Given the consensus structure, consensus base pairs
            are assigned to MATP nodes and consensus unpaired
            columns are assigned to MATL or MATR nodes. One ROOT
            node is used at the head of the tree. Multifurcation
            loops and/or multiple stems are dealt with by assigning
            one or more BIF nodes that branch to subtrees starting
            with BEGL or BEGR head nodes. (ROOT, BEGL, and BEGR
            start nodes are labeled differently because they will
            be expanded to different groups of states; this has to
            do with avoiding ambiguous parse trees for individual
            sequences, as described below.) Alignment columns that
            are considered to be insertions relative to the
            consensus structure are ignored at this stage.
            In general there will be more than one possible
            guide tree for any given consensus structure. Almost
            all of this ambiguity is eliminated by three
            conventions: (1) MATL nodes are always used instead of
            MATR nodes where possible, for instance in hairpin
            loops; (2) in describing interior loops, MATL nodes are
            used before MATR nodes; and (3) BIF nodes are only
            invoked where necessary to explain branching secondary
            structure stems (as opposed to unnecessarily
            bifurcating in single stranded sequence). One source of
            ambiguity remains. In invoking a bifurcation to explain
            alignment columns 
            i..j by two substructures on
            columns 
            i..k and 
            k + 1.. 
            j, there will be more than one
            possible choice of 
            k if 
            i..j is a multifurcation loop
            containing three or more stems. The choice of 
            k impacts the performance of the
            divide and conquer algorithm; for optimal time
            performance, we will want bifurcations to split into
            roughly equal sized alignment problems, so I choose the
            
            k that makes 
            i..k and 
            k + 1.. 
            j as close to the same length as
            possible.
            The result of this procedure is the guide tree. The
            nodes of the guide tree are numbered in preorder
            traversal (e.g. a recursion of "number the current
            node, visit its left child, visit its right child":
            thus parent nodes always have lower indices than their
            children). The guide tree corresponding to the input
            multiple alignment in Figure 1is shown in Figure 2.
          
          
            From guide tree to covariance model
            A CM must deal with insertions and deletions in
            individual sequences relative to the consensus
            structure. For example, for a consensus base pair,
            either partner may be deleted leaving a single unpaired
            residue, or the pair may be entirely deleted;
            additionally, there may be inserted nonconsensus
            residues between this pair and the next pair in the
            stem. Accordingly, each node in the master tree is
            expanded into one or more 
            states in the CM as follows
            (Table 3)
            Here we distinguish between consensus ("M", for
            "match") states and insert ("I") states. ML and IL, for
            example, are both L type states with L type
            productions, but they will have slightly different
            properties, as described below.
            The states are grouped into a 
            split set of 1-4 states (shown in
            brackets above) and an 
            insert set of 0-2 insert states.
            The split set includes the main consensus state, which
            by convention is first. One and only one of the states
            in the split set must be visited in every parse tree
            (and this fact will be exploited by the divide and
            conquer algorithm). The insert state(s) are not
            obligately visited, and they have self-transitions, so
            they will be visited zero or more times in any given
            parse tree.
            State transitions are then assigned as follows. For
            bifurcation nodes, the B state makes obligate
            transitions to the S states of the child BEGL and BEGR
            nodes. For other nodes, each state in a split set has a
            possible transition to every insert state in the 
            same node, and to every state in
            the split set of the 
            next node. An IL state makes a
            transition to itself, to the IR state in the same node
            (if present), and to every state in the split set of
            the next node. An IR state makes a transition to itself
            and to every state in the split set of the next
            node.
            This arrangement of transitions guarantees that
            (given the guide tree) there is unambiguously one and
            only one parse tree for any given individual structure.
            This is important. The algorithm will find a maximum
            likelihood parse tree for a given sequence, and we wish
            to interpret this result as a maximum likelihood
            structure, so there must be a one to one relationship
            between parse trees and secondary structures [ 47 ]
            .
            The final CM is an array of 
            M states, connected as a directed
            graph by transitions 
            t 
            
              v 
             ( 
            y ) (or probability 1 transitions
            
            v → ( 
            y,z ) for bifurcations) with the
            states numbered such that ( 
            y,z ) ≥ 
            v. There are no cycles in the
            directed graph other than cycles of length one (e.g.
            the self-transitions of the insert states). We can
            think of the CM as an array of states in which all
            transition dependencies run in one direction; we can do
            an iterative dynamic programming calculation through
            the model states starting with the last numbered end
            state 
            M and ending in the root state 1.
            An example CM, corresponding to the input alignment of
            Figure 1, is shown in Figure 3.
            As a convenient side effect of the construction
            procedure, it is guaranteed that the transitions from
            any state are to a 
            contiguous set of child states,
            so the transitions for state 
            v may be kept as an offset and a
            count. For example, in Figure 3, state 12 (an MP)
            connects to states 16, 17, 18, 19, 20, and 21. We can
            store this as an offset of 4 to the first connected
            state, and a total count of 6 connected states. We know
            that the offset is the distance to the next non-split
            state in the current node; we also know that the count
            is equal to the number of insert states in the current
            node, plus the number of split set states in the next
            node. These properties make establishing the
            connectivity of the CM trivial. Similarly, all the
            parents of any given state are also contiguously
            numbered, and can be determined analogously. We are
            also guaranteed that the states in a split set are
            numbered contiguously. This contiguity is exploited by
            the divide and conquer implementation.
          
          
            Parameterization
            Using the guide tree and the final CM, each
            individual sequence in the input multiple alignment can
            be converted unambiguously to a CM parse tree, as shown
            in Figure 4. Counts for observed state transitions and
            singlet/pair emissions are then collected from these
            parse trees. The observed counts are converted to
            transition and emission probabilities by standard
            procedures. I calculate maximum a posteriori
            parameters, using Dirichlet priors.
          
          
            Comparison to profile HMMs
            The relationship between an SCFG and a covariance
            model is analogous to the relationship of hidden Markov
            models (HMMs) and profile HMMs for modeling multiple
            sequence alignments [ 18 19 24 ] . A comparison may be
            instructive to readers familiar with profile HMMs. A
            profile HMM is a repetitive HMM architecture that
            associates each consensus column of a multiple
            alignment with a single type of model node - a MATL
            node, in the above notation. Each node contains a
            "match", "delete", and "insert" HMM state - ML, IL, and
            D states, in the above notation. The profile HMM also
            has special begin and end states. Profile HMMs could
            therefore be thought of as a special case of CMs. An
            unstructured RNA multiple alignment would be modeled by
            a guide tree of all MATL nodes, and converted to an
            unbifurcated CM that would essentially be identical to
            a profile HMM. (The only difference is trivial; the CM
            root node includes a IR state, whereas the start node
            of a profile HMM does not.) All the other node types
            (especially MATP, MATR, and BIF) and state types (e.g.
            MP, MR, IR, and B) are SCFG augmentations necessary to
            extend profile HMMs to deal with RNA secondary
            structure.
            The SCFG inside and outside algorithms are analogous
            to the Forward and Backward algorithms for HMMs [ 24 48
            ] . The CYK/inside parsing algorithm is analogous to
            the Viterbi HMM alignment algorithm run in the forward
            direction. CYK/outside is analogous to a Viterbi DP
            algorithm run in the backwards direction.
          
        
        
          Divide and conquer algorithm
          
            Notation
            I use 
            r, v, w, y, and 
            z as indices of states in the
            model, where 
            r ≤ ( 
            v , 
            w , 
            y ) ≤ 
            z. These indices will range from
            1.. 
            M , for a CM 
            G that contains 
            M states. refers to a subgraph of
            the model, rooted at state 
            r and ending at state 
            z, for a contiguous set of states
            
            r..z. G 
            r , without a subscript, refers
            to a subgraph of the model rooted at state 
            r and ending at the highest
            numbered E state descendant from state 
            r. The complete model is , or 
            G 1, or just 
            G. 
            
            S 
            
              v 
             refers to the 
            type of state 
            v ; it will be one of seven types
            {D,P,L,R,S,E,B}. 
            C 
            
              v 
             is a list of children for state 
            v (e.g. the states that 
            v can transit to); it will
            contain up to six contiguous indices 
            y with 
            v ≤ 
            y ≤ 
            M . 
            P 
            
              v 
             is a list of parents for state 
            v (states that could have
            transited to state 
            v ); it will contain up to six
            contiguous indices 
            y with 1 ≤ 
            y ≤ 
            v. ( 
            P 
            
              v 
             parent lists should not be confused with P state
            types.)
            I use 
            g, h, i, j, k, p, and 
            q as indices referring to
            positions in a sequence 
            x, where 
            g ≤ 
            h ≤ 
            p ≤ 
            q and 
            i ≤ 
            j for all subsequences of nonzero
            length. These indices range from 1.. 
            L, for a sequence of length 
            L . Some algorithms will also use
            
            d to refer to a subsequence
            length, where 
            d = 
            j - 
            i + 1 for a subsequence 
            x 
            
              i 
             .. 
            x 
            
              j 
             .
            The algorithms will have to account for subsequences
            of zero length (because of deletions). By convention,
            these will be in the off-diagonal where 
            j = 
            i - 1 or 
            i = 
            j + 1. This special case (usually
            an initialization condition) is the reason for the
            qualification that 
            i ≤ 
            j for subsequences of 
            nonzero length.
            The CYK/inside algorithm calculates a
            three-dimensional matrix of numbers α 
            
              v 
             ( 
            i , 
            j ), and CYK/outside calculates
            numbers β 
            
              v 
             ( 
            i , 
            j ). I will refer to 
            v (state indices) as 
            deck coordinates in the
            three-dimensional matrices, whereas 
            j and 
            i (sequence positions) are row
            and column coordinates within each deck. α 
            
              v 
             and β 
            
              v 
             refer to whole two-dimensional decks containing
            scores α 
            
              v 
             ( 
            i , 
            j ) and β 
            
              v 
             ( 
            i , 
            j ) for a particular state 
            v. The dividing and conquering
            will be done in the 
            v dimension, by choosing
            particular decks as split points.
          
          
            The CYK/inside algorithm
            The CYK/inside algorithm iteratively calculates α 
            
              v 
             ( 
            i , 
            j ) - the log probability of the
            most likely CM parse subtree rooted at state 
            v that generates subsequence 
            x 
            
              i 
             .. 
            x 
            
              j 
             of sequence 
            x. The calculation initializes at
            the smallest subgraphs and subsequences (e.g. subgraphs
            rooted at E states, generating subsequences of length
            0), and iterates outwards to progessively longer
            subsequences and larger CM subgraphs.
            For example, if we're calculating α 
            
              v 
             ( 
            i , 
            j ) and 
            S 
            
              v 
             = P (that is, 
            v is a pair state), 
            v will generate the pair 
            x 
            
              i 
             , 
            x 
            
              j 
             and transit to a new state 
            y (one of its possible
            transitions 
            C 
            
              v 
             ) which then will have to account for the smaller
            subsequence 
            x 
            
            i +1 .. 
            x 
            
            j -1 . The log probability
            for a particular choice of next state 
            y is the sum of three terms: an
            emission term log 
            e 
            
              v 
             ( 
            x 
            
              i 
             , 
            x 
            
              j 
             ), a transition term log 
            t 
            
              v 
             ( 
            y ), and an already calculated
            solution for the smaller optimal parse tree rooted at 
            y , α 
            
              y 
             ( 
            i + 1, 
            j - 1). The answer for α 
            
              v 
             ( 
            i , 
            j ) is the maximum over all
            possible choices of child states 
            y that 
            v can transit to.
            The algorithm INSIDE is as follows:
            
            Input: A CM subgraph and
            subsequence 
            x 
            
              g 
             .. 
            x 
            
              q 
             .
            
            Output: Scoring matrix decks α 
            
              r 
             ..α 
            
              z 
             .
            INSIDE(r,z; g,q)
            
            for 
            v ← 
            z 
            down to 
            r 
            
            for 
            j ← 
            g - 1 
            to 
            q 
            
            for 
            i ← 
            j + 1 
            down to 
            g 
            
            d ← 
            j - 
            i + 1
            
            if 
            S 
            
              v 
             = D or S:
            α 
            
              v 
             ( 
            i, j ) = [α 
            
              y 
             ( 
            i, j ) + log 
            t 
            
              v 
             ( 
            y )]
            
            else if 
            S 
            
              v 
             = P and 
            d ≥ 2:
            α 
            
              v 
             ( 
            i, j ) = log 
            e 
            
              v 
             ( 
            x 
            
              i 
             , 
            x 
            
              j 
             ) + [α 
            
              y 
             ( 
            i + 1, 
            j - 1) + log 
            t 
            
              v 
             ( 
            y )]
            
            else if 
            S 
            
              v 
             = L and 
            d ≥ 1:
            α 
            
              v 
             ( 
            i, j ) = log 
            e 
            
              v 
             ( 
            x 
            
              i 
             ) + [α 
            
              y 
             ( 
            i + 1, 
            j ) + log 
            t 
            
              v 
             ( 
            y )]
            
            else if 
            S 
            
              v 
             = R and 
            d ≥ 1:
            α 
            
              v 
             ( 
            i, j ) = log 
            e 
            
              v 
             ( 
            x 
            
              j 
             ) + [α 
            
              y 
             ( 
            i , 
            j - 1) + log 
            t 
            
              v 
             ( 
            y )]
            
            else if 
            S 
            
              v 
             = B:
            ( 
            y , 
            z ) ← left and right S children
            of state 
            v 
            α 
            
              v 
             ( 
            i, j ) = [α 
            
              y 
             ( 
            i, k ) + α 
            
              z 
             ( 
            k + 1, 
            j )]
            
            else if 
            S 
            
              v 
             = E and 
            d = 0:
            α 
            
              v 
             ( 
            i , 
            j ) = 0 (initializations)
            
              else 
            
            α 
            
              v 
             ( 
            i , 
            j ) = -∞ (initializations)
            Given a sequence 
            x of length 
            L and a CM 
            G of length 
            M , we could call INSIDE (1, M;
            1, L) to align the whole model (states 1.. 
            M ) to the whole sequence ( 
            x 
            1 .. 
            x 
            
              L 
             ). When INSIDE returns, α 
            1 (1, 
            L ) would contain the log
            probability of the best parse of the complete sequence
            with the complete model.
            We do not have to keep the entire α
            three-dimensional matrix in memory to calculate these
            scores. As we reach higher decks α 
            
              v 
             in the three dimensional dynamic programming
            matrix, our calculations no longer depend on certain
            lower decks. A lower deck 
            y can be deallocated whenever all
            the parent decks 
            P 
            
              y 
             that depend on it have been calculated. (The
            implementation goes even further and recycles decks
            when possible, saving some initialization steps and
            many memory allocation calls; for example, since values
            in all E decks are identical, only one E deck needs to
            be calculated and that precalculated deck can be reused
            whenever 
            S 
            
              v 
             = 
            E .)
            This deallocation rule has an important property
            that the divide and conquer algorithm takes advantage
            of when solving smaller subproblems for CM subgraphs
            rooted at some state 
            w. When the root state 
            w is an S state, the α matrix
            returned by INSIDE contains only one active deck α 
            
              w 
             . (No lower state > 
            w can be reached from any state
  
