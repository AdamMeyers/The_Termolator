Histoire de l'informatique

L’histoire de l'informatique est l’histoire de la science du traitement rationnel, notamment par machines automatiques, de l'information considérée comme le support des connaissances humaines et des communications dans les domaines techniques, économiques et sociaux. 

En 1966, l’informatique a été définie par l'Académie française comme la « science du traitement rationnel, notamment par machines automatiques, de l'information considérée comme le support des connaissances humaines et des communications dans les domaines techniques, économiques et sociaux ».

L’histoire de l’informatique résulte de la conjonction entre des découvertes scientifiques et des transformations techniques et sociales.

Si la plupart des ordinateurs ont été conçus au départ pour exécuter des calculs numériques trop longs ou trop compliqués pour être effectués à la main, des machines similaires ont été construites pour traiter des informations non numériques (par exemple, reconnaître une chaîne de caractères dans un texte, ce que faisait dès 1943 le Colossus du service de cryptanalyse britannique). Les calculateurs devenaient des machines universelles de traitement de l’information, d’où le mot ordinateur, retenu en 1956 pour trouver un équivalent français à l’expression anglaise "data processing machine". Ce terme a progressivement remplacé en français celui de calculateur, au sens trop restreint.

Les premiers ordinateurs datent de 1949. C'est la notion de programme enregistré, due à John von Neumann et à ses collaborateurs, en 1945, qui transforme les machines à calculer en ordinateurs. La machine est composée des éléments suivants :
L’ensemble formé par l’unité arithmétique et logique, d’une part, et l’organe de commande, d’autre part, constitue l’unité centrale ou processeur. L’ensemble des composants physiques, appelé matériel "(hardware)", est commandé par un logiciel "(software)".

Selon l'architecture de von Neumann, les programmes sont enregistrés dans la mémoire de la machine. Ils peuvent comporter des boucles de calcul et des alternatives, contrairement aux programmes exécutés à partir de bandes perforées. Alors qu’on connaît depuis longtemps le codage des procédures par des trous sur des bandes perforées (machines à tisser, orgue de barbarie) lorsque le programme est enregistré en mémoire, il n’y a pas de différence fondamentale entre coder des données et coder des procédures. On peut donc calculer sur les instructions d’une procédure, par exemple pour faire un saut en arrière dans les instructions, ou pour compter le nombre de tours d’une boucle de calcul sur les éléments d’une liste. Ce calcul sur les programmes a permis le développement du logiciel : langages de programmation, systèmes d'exploitation, applications.

L’histoire de l’informatique porte donc seulement sur une soixantaine d'années. Elle peut se décliner selon quatre points de vue :

L'informatique est une nouvelle technologie intellectuelle, comme l'ont été en leur temps l'écriture et l'imprimerie, un moyen de représenter, de structurer et d'exploiter des informations qui en retour structure la pensée de l'homme, selon Pierre Lévy.

L’évolution des machines et des réseaux constitue l’infrastructure physique de l’informatique. L'histoire des ordinateurs est décrite dans un autre article. Celle des réseaux informatiques aussi. Depuis ses débuts, l'informatique a connu des transformations profondes des matériels informatiques en vitesse, puissance, fiabilité, miniaturisation. La transformation du logiciel est au moins aussi profonde, transformation en qualité, sécurité, complexité, réutilisabilité. Les objets traités ont beaucoup changé : d’abord les nombres et les fichiers de gestion des entreprises (textes et nombres), puis les formules, les règles de calcul et de raisonnement, les signaux, les événements, les dessins, les images, le son, la vidéo. L’informatique a de plus en plus d’usages et de plus en plus d’utilisateurs, et l'usage principal passe par les réseaux. Ces transformations du matériel, du logiciel et des usages ont modifié profondément la structure économique politique et sociale des sociétés humaines.

On appelle logiciels les composants logiques et symboliques qui permettent de traiter l’information dans les machines informatiques. Le mot information a un sens très large. On appelle ici information tout ce qui peut être codé sous forme numérique pour être mémorisé, transporté et traité par des machines informatiques : des signes (chiffres, lettres, monnaie, textes), des images, des signaux numérisés (musique, parole, relevés de capteurs sur des instruments). Les fondements du logiciel sont le codage, le calcul et l'interaction.

Les procédés physiques de codage de l’information sont analogiques ou numériques.

Les premiers enregistrements du son étaient analogiques, sur disques ou bandes magnétiques. La télévision a longtemps utilisé la modulation des ondes, donc un phénomène physique continu, pour transporter les images et les sons. Le transport, le codage et le décodage, utilisent alors les caractéristiques continues des machines physiques. Ces codages sont sensibles aux déformations par l’usure matérielle et le bruit. Ils ont l'avantage de rester compréhensibles malgré la dégradation du signal.

Avec le codage numérique, qui supplante peu à peu le codage analogique dans tous les domaines, les machines physiques sont seulement le support d’une information qui est traitée, transformée, codée, décodée par des machines logicielles. Il y a « dématérialisation » de l’information, qui n’est plus liée à son support, qui peut être recopiée facilement et à très faible coût sans perte d’information. Il y a toutefois perte d’information par la compression des fichiers, souvent adoptée pour économiser de la taille mémoire et accélérer le traitement. Autre inconvénient, dans les systèmes numériques fondés sur "tout ou rien", il suffit d'un petit dysfonctionnement pour que l'ensemble des données deviennent incompréhensibles, voire perdues.

Vers 1930, Gödel invente pour faire des démonstrations d’indécidabilité un codage numérique des expressions logiques. Ce travail fournira plus tard des bases théoriques au codage informatique. Dans les codes informatiques ASCII (7 bits), ISO (8 bits), Unicode (16 bits), tous les caractères ont un équivalent numérique qui permet de passer d’une représentation interne sous forme de 0 et de 1 sur laquelle l'ordinateur calcule, à leur présentation lisible par l'homme sur un écran ou une feuille de papier.

Avec ces codes, on transforme des textes en nombres. On peut aussi numériser des images, du son, par discrétisation du signal. Le son est continu, mais un signal sonore peut être découpé en éléments très petits, inférieurs à la sensibilité de l'oreille humaine. Les images numérisées sont découpées en points (pixels) et chaque point est codé en fonction de ses propriétés. Si le pouvoir discriminant de l’œil est inférieur à l’approximation faite, on ne fait pas de différence entre l’image analogique et l’image numérique. C’est la même chose au cinéma pour la reconstruction du mouvement par l’œil à partir des 24 images fixes par seconde.
Une fois toutes ces informations codées sous forme numérique, on va pouvoir calculer sur elles, comme sur les nombres. Non seulement le codage numérique est plus fiable, mais il est transportable et copiable avec un coût négligeable.

Le calcul mécanique s’est développé au , donc bien avant l’informatique, avec les machines de Pascal et de Leibniz, les horloges astronomiques comme celle de Strasbourg ou de Lyon, puis les machines à tisser avec des programmes sur bandes perforées. Avec le codage numérique de toutes les informations, le calcul s'est beaucoup étendu par rapport à son origine. On calcule d'abord sur des nombres, qui ont une représentation binaire exacte ou approchée. On peut décrire des phénomènes par des systèmes d’équations pour la simulation informatique.

On peut aussi calculer sur des symboles, par exemple, concaténer des chaînes de caractères : 
ou leur appliquer des fonctions :

Lorsque les images sont codées numériquement, on peut les redimensionner, les restaurer, les transformer par des calculs.

Le raisonnement a été défini comme un calcul avec la définition en 1965 de l’algorithme d’unification de Robinson. C’est la base du calcul formel, des systèmes experts, qui se développent à partir de 1975 et vont donner le coup d’envoi à l’intelligence artificielle.

L’utilisation des ordinateurs repose sur le principe que tout calcul compliqué peut être décomposé en une suite d’opérations plus simples, susceptibles d’être exécutées automatiquement. C’est la notion d’algorithme de calcul. Plusieurs inventions ont permis le développement de l'algorithmique. En 1936 la machine de Turing définit abstraitement la notion de calcul et permet de définir ce qui est calculable ou non. C’est une machine abstraite qui définit les calculs comme des opérations qu’on peut enchaîner mécaniquement sans réflexion. Le "lambda-calcul" d'Alonzo Church en est l'équivalent. Donald Knuth (né en 1938), est l'auteur du traité "The Art of Computer Programming", paru en plusieurs volumes à partir de 1968. Il décrit de très nombreux algorithmes et pose des fondements mathématiques rigoureux pour leur analyse.

Vers 1960 la théorie des langages de Schutzenberger donne un fondement solide à la définition, à la compilation et à l’interprétation des langages de programmation. Ce sera aussi le départ de la linguistique computationnelle de Chomsky.

La mémoire centrale des machines est volatile et ne conserve les données que pendant la durée d'exécution des programmes. Or les fichiers doivent être conservés d'une exécution à l'autre. D'abord sous forme de paquets de cartes perforées, les fichiers sont conservés ensuite dans des mémoires auxiliaires comme des bandes magnétiques ou des disques durs (1956). Ces fichiers sont soit des programmes, soit des données. On constitue des bibliothèques de programmes et de procédures pour éviter de refaire sans cesse le même travail de programmation (formules usuelles de la physique, calcul statistique).

C'est surtout en informatique de gestion que l'importance des fichiers de données s'est fait sentir : fichier des clients, des fournisseurs, des employés. À partir de 1970, les bases de données prennent un statut indépendant des programmes qui les utilisent. Elles regroupent toutes les informations qui étaient auparavant dispersées dans les fichiers propres à chaque programme. Lorsque les fichiers étaient constitués pour chaque application de manière indépendante, le même travail était fait plusieurs fois et les mises à jour n’étaient pas toujours répercutées. L'accès de chaque programme aux données dont il a besoin et à celles qu'il construit ou modifie se fait par l'intermédiaire d'un système de gestion de bases de données (SGBD).

Vers 1975, avec les systèmes experts, on découvre la possibilité de constituer des bases de connaissances sur des sujets variés : médecine, chimie, recherche pétrolière, stratégies pour les jeux. Les bases de connaissances ne sont pas des encyclopédies, elles codent la connaissance sous une forme qui permet le raisonnement des machines.

À partir de 1990, on constitue des bases de documents en numérisant des archives (livres, enregistrements sonores, cinéma). On assure ainsi une plus grande disponibilité et une plus grande fiabilité de la conservation. À travers le réseau Internet, le Web offre un espace public de mise à disposition de documents sans passer par des éditeurs et des imprimeurs. Il permet d’emblée l’accès à des informations et des créations mondiales. Mais il pose des questions importantes à l’organisation sociale : possibilité d’interdire et de juger les infractions, paiement des droits d’auteur, sécurité des échanges commerciaux, qualité des documents trouvés (les images peuvent être truquées, les informations mensongères ou partiales).
En parallèle, avec les langages à objets (voir ci-dessous), les composants réutilisables pour la programmation ne sont plus tant des procédures que des objets, encapsulant des données et des fonctions, ou même des processus complets s'articulant les uns avec les autres par leurs interfaces.

Jusqu’aux années 1970, les ordinateurs recevaient les programmes et les données sur des cartes, des rubans perforés ou des bandes magnétiques. Ils renvoyaient leurs résultats quand ils étaient terminés ou retournaient des messages d’erreur énigmatiques. Il n’y avait pas de possibilité d’interaction pendant l’exécution. Dans cette période, l’informatique s’est surtout développée pour calculer des fonctions en référence à une théorie du calcul ou pour la gestion des entreprises. La question des entrées-sorties est alors vue comme une question secondaire. Le temps compte en tant que durée des processus de calcul. Toutefois le traitement et l'interaction en temps réel se sont développés dans le contrôle des processus industriels ou des systèmes d'armes, où le temps compte comme l'instant de la décision.

Dans les années 1970 sont apparus de nouveaux types de machines informatiques avec disque dur, écran et clavier. Elles ont d’abord fonctionné en mode « ligne de commande », purement textuel et asynchrone. Elles ont pu employer les premiers langages interprétés comme Lisp et BASIC, élaborés une décennie plus tôt. Au lieu d'écrire un programme, l'utilisateur tape une commande qui est exécutée. Il garde le contrôle du processus de calcul et peut tenir compte des résultats précédents pour enchaîner.

Puis l'écran est devenu graphique et la souris a permis la manipulation directe. Les machines deviennent interactives. Le développement des interfaces ou IHM (interface homme-machine) introduit les notions d’action et d’événement dans la programmation. De ce fait, le temps compte et il n’est pas possible de recommencer une exécution à l’identique.
Pour construire des interfaces facilitant l'interaction, de nouveaux concepts sont apparus comme les fenêtres, les menus déroulants, les boutons à cliquer, les cases à cocher, les formulaires. La métaphore du bureau a fait le succès du Macintosh d'Apple : elle transfère les objets (dossiers, fichiers, corbeille) et les actions du travail de secrétariat (couper, coller) dans l’univers de l’interface.

L’IHM doit assurer la relation entre le logiciel et les usagers. La manipulation directe des objets visibles sur l'écran prend le pas sur la description verbale des actions. Avec l'arrivée de la couleur et du son, le focus a été mis sur la composition graphique des interfaces, sur les rapports entre les applications et les interfaces, sur l'ergonomie, l’esthétique et les chartes graphiques car c’est à travers les interfaces que les usagers ont ou non une bonne opinion du logiciel.

Le paradigme objet, créé pour gérer les interfaces graphiques et les simulations, a permis de réunifier trois points de vue, celui des fonctions, celui des données, et celui des entrées-sorties. En considérant les types d’objets comme comportant à la fois une structure de données et des procédures de manipulation de ces données, il réunifie le paradigme du calcul avec le paradigme des données. En considérant les entrées-sorties, les constructions et les transformations d’objets comme des effets de bord utiles des calculs, il transforme fondamentalement la façon de concevoir le logiciel, des machines de Turing aux machines interactives .

On appelle logiciels les composants logiques et symboliques qui permettent de traiter l’information dans les machines informatiques : programmes, systèmes d'exploitation, SGBD, applications (traitements de texte, tableurs, jeux).

À la fin des années 1970, les coûts des logiciels occupent une part prépondérante dans le prix de revient des systèmes, alors que jusqu’ici les matériels représentaient la majeure partie des investissements. Les utilisateurs ont le souci de protéger leurs investissements logiciels et redoutent les changements de machines entraînant une réécriture des programmes. La compatibilité des machines vis-à-vis des programmes existants devient un impératif majeur. Les constructeurs s'étaient déjà préparés à se plier à cette exigence, en concevant depuis 1960 des langages et des systèmes d'exploitation indépendants des systèmes matériels.

L’indépendance entre les langages d'écriture des programmes d’une part et les machines d’autre part s’est appuyée sur la disponibilité de langages évolués, comme Fortran ou Cobol, indépendants d'une machine particulière. La compilation des programmes sources dans le langage de la machine permet leur exécution sous forme de programme objet. L'évolution des langages de programmation a suivi l'évolution des objets manipulés par les machines et l'évolution des usages.

Les premiers systèmes d'exploitation datent des années 1950, mais leur emploi s'est généralisé vers 1965. Avec la deuxième génération d'ordinateurs, la gestion des périphériques s’est alourdie. Il devint impossible pour un programmeur de concevoir à la fois les logiciels d’application et les logiciels de gestion de la machine. Une distinction s’établit donc entre les applications (programmes de l’utilisateur) et les programmes système (logiciel de gestion des ressources de la machine). Dans une première étape, les logiciels système sont composés de programmes de gestion des entrées-sorties. Ils permettent une simultanéité apparente entre l’unique programme utilisateur et la gestion des entrées-sorties. 
Le passage d’un programme à un autre nécessitait une intervention humaine. Dans une deuxième étape, les moniteurs d’enchaînement permettent l’enchaînement automatique des applications : les programmes à exécuter sont placés les uns derrière les autres, chaque programme et ses jeux de données constituant un lot. Chaque lot s'exécute à son tour. Ce mode de fonctionnement, dénommé traitement par lots minimisait les interventions manuelles et, dès la fin de la première génération, la quasi-totalité des ordinateurs fonctionnent dans ce mode. 
Mais à cause des disparités de vitesses entre le processeur et les accès aux périphériques, l’unité centrale reste sous-employée. Dans une troisième étape, la multiprogrammation remédia à la sous-utilisation chronique de l’unité centrale. Lorsque l’ordinateur fonctionne en multiprogrammation, plusieurs applications sont chargées en mémoire et se partagent le processeur. Dès que se manifeste une attente dans l’exécution du programme en cours (demande d’entrée-sortie, par exemple), le processeur abandonne l’exécution de ce programme et démarre (ou poursuit) l’exécution d’un autre programme. Ce dernier est alors exécuté jusqu’à sa fin ou jusqu’à ce qu’il demande une entrée-sortie.
En autorisant l’exécution pseudo-simultanée de plusieurs programmes, la multiprogrammation crée le besoin de systèmes d’exploitation des ordinateurs. Sur des machines multi-utilisateurs il ne s'agit plus seulement de gérer les périphériques et l'enchaînement des programmes, mais d’allouer l’unité centrale à un travail, de contrôler l’exécution des programmes, de transférer les programmes à exécuter avec leur pile d'exécution, de protéger les programmes contre les erreurs des autres programmes.
Dans l’étape suivante, celle du temps partagé, les utilisateurs retrouvent l’accès direct aux ordinateurs, qu’ils avaient perdu avec le traitement par lots. Chaque utilisateur du système se voit alloué périodiquement le processeur, pendant un laps de temps déterminé. Pour un utilisateur, la fréquence d’allocation du processeur est suffisamment élevée pour lui donner l’illusion d’être seul à travailler sur la machine. L’accès à l’ordinateur s’effectue au moyen de terminaux, d’abord des machines à écrire puis, des machines clavier-écran.

Les premiers systèmes d'exploitation sont étroitement dépendants des machines. Ils présentent de profondes différences d’un constructeur à l’autre, voire d’une machine à l’autre chez un même constructeur. L’indépendance entre le système d’exploitation et la machine a été posée comme principe dans le développement du système Unix en 1980. Il s'organise en couches de logiciel. Les couches basses encapsulent les caractéristiques propres d’une machine pour présenter une interface unique aux couches hautes. Pour l'utilisateur, qui interagit avec la couche la plus haute, le système a un comportement indépendant de la machine. Les mêmes programmes vont fonctionner, en principe, quand on change de machine. Mais les couches hautes non plus ne dépendent pas d'une machine particulière et elles sont réutilisables quand les machines évoluent.

Dès les années 1960, les systèmes ont permis aux utilisateurs d’accéder aux machines à distance : par l’intermédiaire de terminaux reliés à ces machines par des liaisons téléphoniques, l’utilisateur éloigné pouvait soumettre des lots de travaux ou encore utiliser le temps partagé. Leur histoire est décrite dans l'article sur les systèmes d'exploitation.

Les nombres sont les premiers objets qui ont été manipulés par les machines à cause de la facilité à formaliser des calculs. Les premières machines calculaient en décimal (chaque chiffre décimal est codé en binaire), mais le codage binaire s'est vite imposé car il facilite considérablement les opérations. Il amène à faire des transformations pour les entrées (décimal → binaire) et les sorties (binaire → décimal).

Le codage binaire des caractères est apparu avec COBOL en 1959 et a permis de stocker et de manipuler des libellés : nom et prénom, adresse… Les fichiers, puis les bases de données comportaient des zones numériques et des zones alphanumériques. On peut calculer aussi sur les zones de libellé, par exemple les ranger par ordre croissant en fonction de l'ordre induit par le code des caractères.

Les langages de programmation évolués, comme Fortran en 1956 pour le calcul scientifique ou Algol en 1960, plus généraliste, ont facilité le travail des programmeurs. Un programme en langage évolué est un texte qui doit être compilé dans le langage de chaque machine avant d'être exécuté. Les programmes informatiques sont donc des textes analysés et transformés par d'autres programmes (éditeurs, analyseurs lexicaux et syntaxiques, compilateurs, calcul des références croisées).

Les formules peuvent aussi être codées et manipulées et le langage LISP a été conçu en 1960 par John Mac Carthy, au MIT, pour faire du traitement symbolique. Les formules sont des chaînes de caractères particulières où certains symboles représentent des opérations et des relations. Elles obéissent à une syntaxe stricte (formules bien formées). Elles peuvent tantôt être manipulées par des règles de transformation (compilation des programmes par exemple) et tantôt être évaluées. C'est la base des systèmes de calcul formel comme Maple ou Mathematica.

Le traitement de textes s'est développé peu à peu avec l'apparition des machines personnelles dans les années 1980. On code non seulement le texte mais sa mise en forme matérielle. Les directives de mise en forme du texte : police de caractère, gras et italique, taille des caractères, justification, format de la feuille, numéros de page, permettent de calculer la mise en page à partir des directives. Il existe deux types de traitement de texte, ceux où on voit aussitôt la mise en forme de ce qu'on tape (WYSIWYG1) comme Word, Works… et ceux où on voit des chaînes de caractères dont certains sont des directives et d'autres du texte (TEX, HTML)

Avec les écrans graphiques, expérimentés chez Xerox et d'autres constructeurs dès les années 1970, puis popularisés par le Macintosh d'Apple en 1984, on peut gérer les pixels de l'écran au lieu de gérer seulement les caractères et les nombres.

Avec les écrans noir et blanc (1 bit par pixel), on a introduit des logiciels de dessin. Les dessins vectoriels sont obtenus par visualisation de directives de tracés : une droite est déterminée par son origine et son extrémité, un cercle par son centre et son rayon. On peut construire des courbes, faire du dessin industriel, des schémas.

Avec les écrans et les imprimantes à niveau de gris, puis en couleur, on a pu utiliser des images construites point à point. Chaque pixel de l'écran est obtenu par mélange des niveaux de chacune des 3 teintes fondamentales, il faut donc plusieurs bits par pixel : 3 bits pour 8 niveaux de gris, 16 bits pour différencier couleurs. Si l'écran est ×, une image à couleurs occupe environ 2M octets. L'importance des bonnes techniques de compression d'image comme JPEG est évidente. Les images ne sont pas seulement conservées et transportées par les machines, on dispose de logiciels de restauration d'images, de transformations variées sur les couleurs, les formes, les découpages et superpositions. Les environnements graphiques permettent la création de cartes de géographie, de plans d’architecture, des dessins techniques (aménagements routiers, véhicules, matériel industriel).

Après les images fixes, on traite ensuite des images animées à partir des années 1990. Le cinéma et la vidéo sont numérisés. On construit des mondes virtuels. Des monuments ayant existé ou devant exister sont reproduits, dans lesquels on peut entrer, se promener, se déplacer pour les voir sous d'autres angles. On construit des mondes imaginaires : environnements de jeu, pilotage de voitures ou d'avion, installation de cuisines. Toutes les images de ses applications ne sont pas stockées, elles sont calculées à partir d'un modèle et de la position de l'usager.

Les signaux sont d'autres types d'objets manipulés par les machines : des capteurs enregistrent des appareils divers, du son, des signaux radio, radar. Pendant longtemps, les signaux étaient enregistrés et traités sous forme analogique, ils sont maintenant de plus en plus enregistrés et traités sous forme numérique. Des formats d'échange ont été définis avec des interfaces pour des appareils : avec le format MIDI pour la synthèse de la musique, on peut reproduire de la musique sur des synthétiseurs qui jouent la musique ; avec le codage MP3 pour la musique enregistrée, on l'envoie sur un ampli et des haut-parleurs.

La variété des objets manipulés par les machines a amené à concevoir de nouveaux langages de programmation adaptés, d'abord des langages spécifiques, puis avec les langages à objets, des langages qui s'adaptent à tous les types d'objets qu'on y décrit.

Chaque machine a un langage propre correspondant à son jeu d'instructions. Les langages d'assemblage sont d'abord apparus pour écrire des programmes moins dépendants des machines en nommant les opérations et les registres. Dès la seconde moitié des années 1950, la création de langages de programmation évolués : COBOL pour la gestion des entreprises, Fortran pour les ingénieurs et les scientifiques, permet de s'affranchir du jeu d'instructions et de registres d'une machine particulière. Les programmes peuvent être réutilisés d'une machine à l'autre en écrivant des compilateur appropriés.

En 1960, deux langages très différents, Lisp et Algol sont créés à partir de bases mathématiques plus solides. Lisp est créé pour la manipulation récursive de listes et il sera le langage de prédilection de l'intelligence artificielle. Les listes représentant des expressions mathématiques peuvent être manipulées formellement ou évaluées. Les fonctions sont des objets manipulables. Interlisp en 1980 est un des premiers langages interprétés. Algol est créé pour exprimer des algorithmes. Il utilise aussi la récursivité et définit des blocs pour la portée des variables.

Le langage C a été développé vers 1970 pour la programmation du système Unix. C'est un langage impératif, orienté par les fonctionnalités des machines plus que par les facilités d'expression des programmeurs. C'est le langage de référence pour les applications de bas niveau et beaucoup de bibliothèques de programmes en C sont utilisées par les autres langages pour implanter les fonctions répétitives à cause de son efficacité.

Parmi tous les langages de programmation qui ont été inventés dans les vingt années suivantes, citons Simula en 1967, extension d'Algol pour la simulation par événements discrets, Prolog en 1980, basé sur la logique et l'algorithme d'unification, et Smalltalk en 1976 qui a introduit les notions d'objets et d'interaction.

La programmation par objets est née avec Smalltalk-80, mais elle hérite aussi des concepts d'événement et de processus de Simula. Elle a été popularisée par CLOS (Common Lisp Object System) en 1994 puis par Java en 1997. Elle a été développée au départ pour programmer facilement les interfaces hommes machines avec des fenêtres, des menus et des boutons. Elle va peu à peu amener une transformation profonde des habitudes des informaticiens dans tous les domaines. Au lieu de penser les programmes à partir des fonctions, ils vont les penser à partir des classes d’objets, regroupant une structure de donnée et des procédures (constructeurs, opérations, actions). Les classes d’objets sont beaucoup plus stables que les procédures car les besoins fonctionnels des utilisateurs varient beaucoup plus vite que les objets sur lesquels ils portent. On peut constituer des bibliothèques de classes et d’objets plus faciles à retrouver et à réutiliser que les procédures des bibliothèques de programmes.

Avec le Web, les langages de scripts ont trouvé un terrain de développement imprévu. Au départ, les langages de scripts sont inclus dans les systèmes d'exploitation et servent à décrire des enchaînements de tâches du système. C'est le cas par exemple du Shell d'Unix.
Un nouveau langage de scripts, Perl est créé en 1987 pour ajouter des services dynamiques aux pages Web, par exemple pour compter le nombre de visites sur une page. Puis PHP est créé en 1994 avec une bibliothèque de fonctions écrites en langage C pour l'efficacité. Il est aussi utilisé pour développer des applications client-serveur.

De très nombreux langages de programmation ont été conçus, beaucoup n'ayant eu qu'une existence éphémère. Ceux qui ont été cités ici ont été choisis parce qu'ils marquent une évolution conceptuelle. Chaque type de langage de programmation est le support d'une méthode de conception des programmes.

La complexité croissante des systèmes d’exploitation, des interfaces, des réseaux, doit être maîtrisée par les ingénieurs du logiciel. Le génie logiciel pose les problèmes de développement du logiciel dans des contextes professionnels en termes de maîtrise des délais et des coûts, de fiabilité et d’évolutivité des produits, de satisfaction des usagers, tant dans l’appropriation facile des logiciels que dans les services rendus. Il s’est beaucoup développé depuis 1980, et les méthodes de conception du logiciel comme Merise, puis UML sont devenues incontournables. Elles ont apporté des outils graphiques et une planification des tâches plus formalisés que dans les premiers modèles de conception.

C’est une approche ascendante, qui va du particulier au général. C’est la méthode qui est encore utilisée pour le développement d’UNIX. Le noyau du logiciel comporte les fonctionnalités de base, qui n’ont pas besoin des services des autres couches. Sur ce noyau sont construites des couches de plus en plus éloignées du fonctionnement matériel de la machine. Chaque couche a accès aux services fournis par les couches inférieures. La dernière couche assure les interfaces avec les usagers. Cette approche est orientée machine, elle suppose que les différentes fonctionnalités sont organisables par couches ordonnées et elle traite par des exceptions les cas où il n’est pas possible d’assurer un ordonnancement strict parce que deux éléments ont besoin l’un de l’autre pour fonctionner.

En réaction à la programmation ascendante, qui consiste à commencer par ce qu’on sait faire et à bricoler ensuite pour que le problème soit résolu, l’idée de la conception descendante est de résoudre le problème par décomposition en problèmes plus simples et de prévoir comment ils se raccordent. Il s’agit de s’attaquer dès le départ au problème posé dans son ensemble et de s’occuper des détails ensuite. On décompose chaque tâche en sous tâches par abstraction de procédures et on recommence tant que les sous-tâches sont trop compliquées pour être résolues directement. C’est une méthode de décomposition fonctionnelle qui sous-tend la plupart des cours de base en programmation.

Dans la conception par couches, on est obligés d’accepter des exceptions aux relations strictes entre les couches. Dans la conception ascendante comme dans la conception descendante on s’intéresse seulement aux procédures et les données sont vues uniquement comme des paramètres d’entrée et de sortie. Les variables sont globales et elles provoquent souvent des effets non désirés très difficiles à retrouver. Dans la conception descendante, si on cherche trop tôt à utiliser des procédures connues, on sort du principe de la méthode, or la réutilisabilité apparaît très tôt comme une exigence importante.
En organisant la conception à partir de modules de programmes, qui sont des unités ayant leurs données propres et réalisant des traitements, la bonne liaison entre données et procédures est assurée. Les modules exportent et importent des fonctionnalités. Ils sont reliés entre eux par un diagramme de flux des données. Les modules sont autonomes et réutilisables. Le langage Modula-2 permet de mettre en œuvre ce modèle, qui est un ancêtre du modèle objet.
Dans les systèmes d’information des entreprises, les données prennent le pas sur les procédures. Les modèles de données sont moins changeants que les procédures, ils sont communs à plusieurs procédures. En faisant porter d’abord l’attention sur les données, leurs relations et leur organisation, la conception des bases de données a été mise au premier plan. Les méthodes allaient prendre deux aspects complémentaires :
Merise est un bon exemple de méthode de conception correspondant à ce modèle. Cette méthode, utilisée très largement en France, a fait progresser la qualité du logiciel et permis une bonne interface entre maître d’ouvrage et maître d’œuvre. En voyant la première étape du développement du logiciel comme une modélisation, elle a incité au développement d’ateliers de génie logiciel (AGL). Ce sont des outils informatiques d’aide à la construction de schémas suivant un certain standard. Ils comportent des mécanismes de vérification de la syntaxe et de la cohérence et dans certains cas une partie du code peut être générée à partir de ces schémas.
Merise a comme inconvénient de conduire à développer des grandes applications fortement intégrées qui évoluent difficilement.

Les méthodes précédentes étaient valables tant qu’on voyait les programmes comme des processus de traitement de données, visant à produire un résultat en combinant des calculs. L’arrivée des systèmes interactifs dans les années 1980 allait bouleverser le domaine de la conception du logiciel parce que les logiciels n’avaient plus une fonction unique, mais qu’ils créaient un environnement de travail. Les actions des usagers pilotent le comportement du logiciel.
La première approche centrée sur les objets a été Simula 67 qui a introduit la notion de classe comme une structure comportant à la fois des données et des procédures pour les manipuler. Puis Simula 70 a introduit la notion d’héritage entre les classes. Smalltalk 80 a généralisé cette approche et l’a rendue populaire en introduisant le polymorphisme des fonctions, la liaison dynamique entre les fonctions et les méthodes, les métaclasses et l’encapsulation.
En conception par objets, on construit un modèle du monde avant de développer du logiciel. On définit les structures de données, leurs relations, les opérations et les fonctions avant de s’occuper du contrôle du processus. On obtient ainsi des composants réutilisables dans plusieurs projets. La maintenance et l’évolution des logiciels sont mieux assurées. UML est un langage de modélisation graphique à base de pictogrammes pour la conception par objets très largement utilisé.

Des composants informatiques se trouvent actuellement dans toutes les sphères de l’activité humaine. Ils entrent dans la composition de nombreuses machines (avions, voitures, électroménager…). Les ordinateurs modifient les situations de travail (traitement de textes, tableurs, mail) et les processus de travail (imprimerie, banque). L'informatique a permis la dématérialisation de nombreux phénomènes sociaux : argent, actions bancaires, documents électroniques. La diffusion massive des ordinateurs s'accompagne de la création de logiciels de toutes natures : Web, jeux, aides à l’enseignement, découverte artistique ou scientifique qui transforment la vie quotidienne et l'enseignement.

Ces transformations peuvent être décrites selon trois étapes.
Au départ, l’informatique a principalement deux usages, le calcul scientifique (pour des usages aussi bien civils que militaires, stimulés par la Guerre Froide) et l’aide à la gestion des entreprises, en prenant la relève de la mécanographie. En anglais on distingue deux domaines : "Computer Science" et "Information Systems".

La fin des années 1960 est marquée par une bulle spéculative sur les sociétés d'électronique découlant de l'apparition des circuits intégrés produits à grande échelle, et surtout de nouveaux modes d'utilisation de l'ordinateur, notamment en temps partagé sur des terminaux distants.

De nouvelles machines permettent d'automatiser des calculs faits précédemment par des pools de calculateurs humains dans les entreprises, les universités, les organismes de recherche. Le ministère de la Défense aux États-Unis a subventionné de gros programmes de recherche en programmation, en reconnaissance des formes et intelligence artificielle, en codage et cryptographie, en traduction automatique des langues, qui ont permis le décollage des applications informatiques.

En France, à plus petite échelle, au milieu des années 1960, la diversification des applications se combine avec l'attrait de l'ordinateur pour divers projets scientifiques (de la logique à la linguistique). Mais il faudra plus de dix ans pour que les grandes institutions scientifiques admettent officiellement l'idée que l’informatique est une nouvelle science .

Il est à noter que dès 1968, l'artiste Vera Molnár utilise l'ordinateur et l'art algorithmique dans ses créations artistiques.

Avec les développements en puissance et en fiabilité des ordinateurs, toutes les pratiques sociales de recherche, de conception, de fabrication, de commercialisation, de publication, de communication ont été envahies et transformées par l’informatique. La micro-informatique a permis une grande diffusion des composants informatiques à base de microprocesseurs dans les systèmes techniques et la création des micro-ordinateurs. Les réseaux font communiquer les machines, permettent la décentralisation des machines près des postes de travail et les premières communications à travers les machines. Les constructeurs conçoivent des architectures pour les connecter, comme la DSA (CII-Honeywell-Bull), Decnet, de DEC, et SNA d'IBM.
La bureautique intègre toutes les nouvelles technologies de l'information et la communication (NTIC) dans l'environnement qu'elle couvre. Sa définition est :

""Ensemble des techniques et des moyens tendant à automatiser les activités de bureau et principalement le traitement et la communication de la parole, de l'écrit et de l'image." Définition du Journal Officiel de la République Française (arrêté du ) du ."

Les applications bureautiques permettent d’utiliser du matériel et des logiciels sans dépendre des informaticiens. Ce sont des produits spécifiques qui répondent aux besoins du travail à réaliser dans les bureaux, d'où son appellation. Le premier ou plus ancien d'entre eux est le traitement de texte dédié qui a commencé à s'implanter dans les entreprises dans les années 1970. Ces systèmes avaient pour but de pouvoir réaliser des documents de taille importante, modifiable et personnalisable rapidement (rapports, notices, devis, tous les documents juridiques, etc.), de faire du publipostage (courrier personnalisé automatiquement à l'aide d'un fichier de données), de gérer des bibles de paragraphe (mise en mémoire et insertion sur demande de bloc de texte) et - dans certains cas - de construire des tableaux qui effectuaient des calculs sur demande. Le premier traitement de texte sur micro-ordinateurs grand public est Wordstar de Micropro en 1979. Après le traitement de texte, les tableurs permettant de réaliser des feuilles de calcul électroniques et les Systèmes de Bases de Données Relationnelles ont été progressivement introduits dans les entreprises. Aujourd'hui tout le monde utilise les outils issus de la bureautique, que ce soit pour faire des documents, des statistiques, des graphiques ou des présentations assistées par ordinateur (PREAO).

On peut citer comme exemple de bouleversement produit par l'informatique la transformation de l’imprimerie : tout le processus a été transformé. La saisie informatique a remplacé la machine à écrire et le texte produit par les auteurs peut être réutilisé. La composition des textes pour l'imprimerie avec des matrices de caractères en plomb a été remplacée par la composition sur ordinateur. Les directives de mise en page contrôlent la taille et la police des caractères, les espacements, les sauts. Le journal ou le livre sont un fichier avant d'être imprimés. D’où la transformation des métiers du livre et des possibilités de diffusion des textes imprimés.
De manière analogue, la conception des machines et des bâtiments a été transformée par la conception assistée par ordinateur (CAO), qui dématérialise le dessin technique en permettant des modifications et des mises à jour plus faciles, la réutilisabilité et la disponibilité des documents de conception.
La commande de machines physiques nécessite le recueil d’information sur des capteurs (de pression, de température…) et l’envoi de commandes à des effecteurs (ouvrir une vanne, allumer une lampe…). Elle consiste à choisir à tout instant une action pertinente en fonction d’un but et des informations acquises. C'est le domaine de l'automatique.

La commande de machines physiques s’est d’abord faite par des procédés mécaniques, puis par des procédés électromécaniques : un tableau de commande câblé fait le lien entre les entrées des capteurs et les sorties sur les effecteurs. Tous les appareils ménagers sont équipés de microprocesseurs, plus fiables que la commande câblée. Les machines et les équipements industriels sont contrôlés par des systèmes informatiques complexes : tableau de bord des usines, pilote automatique d’avion ou de métro. L'assistance à la conduite des véhicules, les horloges électroniques sont d'autres exemples de cette automatisation.

Les nouveaux systèmes de commande de machines physiques sont dits « intelligents » parce qu’on remplace le tableau de commandes câblé par un processus programmé qui peut envisager un nombre de cas beaucoup plus important. La commande logicielle est beaucoup plus souple et plus fiable que la commande mécanique. On peut introduire de nombreux contrôles, des raisonnements sur l’état de l’environnement, sur les temps de réaction des acteurs humains, traiter des exceptions et prévenir les erreurs. Elle est reconfigurable sans transformation physique des machines. Elle peut même se reconfigurer par apprentissage (logique floue, réseaux connexionnistes).

Les systèmes « temps réel » ont été conçus pour gérer des processus industriels, par exemple la conduite d’un haut-fourneau. Un système temps réel doit réagir à certains événements en un temps borné supérieurement. Un événement spécifique du processus à piloter provoque l’exécution du programme de gestion de l’événement correspondant. Il faut remarquer la précision nécessitée par certains systèmes temps réel comme la commande des satellites, des stations orbitales. On peut transformer leurs fonctions longtemps après leur lancement grâce à leur contrôle par programmes reconfigurables (atterrissage de la station MIR).

Ces transformations s’accompagnent d’une transformation profonde des métiers de l’informatique. L’usager accomplit lui-même de nombreuses tâches qui nécessitaient autrefois des spécialistes (saisie, lancement des processus, observation des résultats), et de nombreuses tâches sont automatisées. Il y a disparition d’emplois peu qualifiés fréquents dans la première période au profit d'emplois très qualifiés qui ne correspondent pas à de l’encadrement : les informaticiens sont payés comme des cadres mais n’ont personne sous leurs ordres. Ils viennent en Jeans et tee-shirts à leur travail. Tout le monde du travail en a été transformé.

Dans les années 1970, les réseaux informatiques se constituent autour de puissants ordinateurs centraux contenant les applications et les données, auxquels les terminaux étaient raccordés par des liaisons spécialisées ou des lignes du réseau téléphonique.

Trois architectures, de conceptions différentes, sont créées par trois grands constructeurs : la DSA lancée par CII-Honeywell-Bull innove dans l'informatique distribuée en mettant en avant les mini-ordinateurs Mitra 15 puis Mini 6, tandis que Decnet, de DEC, et SNA d'IBM donnent une plus grande place au site central, qui contrôle l’ensemble des ressources matérielles et logicielles, les utilisateurs y accédant pour une « session » via des terminaux passifs.

Puis les ordinateurs personnels ont proliféré de manière anarchique au sein des entreprises. Très vite, leurs utilisateurs ont souhaité accéder aux services et aux applications disponibles sur les systèmes hôtes. Pour cela, des matériels et des logiciels spécifiques leur permirent de fonctionner comme des terminaux passifs.

Avec le développement considérable du parc des micro-ordinateurs dans la seconde moitié des années 1980, les utilisateurs ont souhaité mettre en commun les ressources de leurs machines, tant matérielles (imprimantes laser) que logicielles. Apparaissent alors les réseaux locaux, par opposition aux réseaux grande distance comme Ethernet. Ils offrent des débits de plusieurs millions de bits par seconde au lieu de quelques dizaines de kilobits par seconde, et fonctionnent en mode client/serveur, pour exploiter leurs capacités de traitement et de stockage des données. Tout ordinateur appartenant à un réseau peut proposer des services comme serveur et utiliser les services d’autres systèmes en tant que client. Ce modèle client-serveur ayant permis l’émergence de nouvelles techniques de travail décentralisées, l'augmentation de la puissance des machines a eu raison des ordinateurs centraux.

L’interconnexion des systèmes informatiques a fait l’objet de normalisations internationales comme le Modèle OSI de 1978, inspiré par la DSA de CII-Honeywell-Bull, qui innove dans l'informatique distribuée. Leur but, offrir aux utilisateurs l’accès à un système quelconque, indépendamment de son constructeur et de sa localisation géographique, avec des protocoles de communication identiques, ce qui va inspirer l'internet.

L'Internet découle directement de recherches démarrées au milieu des années 1970, sous l’impulsion de l’agence du ministère de la Défense américain gérant les projets de recherche avancée. Au début des années 1980, Arpanet, le réseau de télécommunications issu de ces travaux, se scinde en deux parties : l’une, réservée à la recherche, conserve le nom d’ARPANET, l'autre, réservée aux activités militaires, prend le nom de MILNET. En 1985, la NSF (National Science Foundation) subventionne NFSNET, un réseau grande distance relié à ARPANET qui interconnectait les plus gros centres de calcul. NFSNET devient rapidement le réseau fédérateur des réseaux des universités et des centres de recherche américains.

À partir des années 1990, les réseaux (Web, mail, chats) accroissent fortement la demande pour numériser photos, musique et données partagées, ce qui déclenche une . Les capacités de stockage, de traitement et de partage des données explosent et les sociétés qui ont parié sur la croissance la plus forte l'emportent le plus souvent.

L'ordinateur pénètre dans tous les milieux sociaux, et dans tous les systèmes technique, financier, commercial, d'information, administratif. L'informatique n'est plus séparable des autres champs de l'activité humaine.

À partir des années 1990, l'internet, réseau des réseaux à l'échelle du monde, devient d'accès public. L'internet propose divers services à ses utilisateurs (courrier électronique, transfert de fichiers, connexion à distance sur un serveur quelconque). Pour cela, il s’appuie sur les protocoles TCP (Transmission Control Protocol) et IP (Internet Protocol), qui définissent les règles d’échange des données ainsi que la manière de récupérer les erreurs. Ces protocoles, souvent appelés protocoles TCP/IP, offrent un service de transport de données fiable, indépendamment des matériels et logiciels utilisés dans les réseaux.

Le , le CERN, le laboratoire européen de recherches nucléaires, basé à Genève, autorisait l'utilisation du protocole "World Wide Web" - sur lequel s'appuie la majeure partie des contenus créés sur l'internet - et, ce faisant, mettait en ligne le premier site internet au monde. Avec les réseaux informatiques le Web, ensemble de pages documentaires reliées, offre un espace public de mise à disposition de documents sans passer par des éditeurs et des imprimeurs. Il permet l'interaction entre les usagers de machines distantes et le téléchargement de documents. 
Le Web permet d’emblée l’accès à des informations et des créations mondiales. Mais il pose des questions importantes à l’organisation sociale : possibilité d’interdire et de juger les infractions, paiement des droits d’auteur, sécurité des échanges commerciaux, qualité des documents trouvés (les images peuvent être truquées, les informations mensongères ou partiales).

À partir des années 2000, la vente en ligne et développe. Les banques et l'administration utilisent le Web pour la circulation des documents. Un protocole sécurisé HTTPS est mis en œuvre pour ces usages.

On constitue d'abord des bases de documents en numérisant des archives (livres, enregistrements sonores, cinéma). On leur assure ainsi une plus grande disponibilité à distance. Mais la production des documents devient directement numérique avec des logiciels adaptés à l'écriture et à la publication. Les appareils photo, les caméras, les micros passent au numérique. La télévision passe au numérique en 2010 et offre ses émissions en différé par l'internet. Les mondes virtuels créent des environnements de travail, d'exploration ou de jeu qui n'avaient pas d'analogue. Tout ce qui relève du dessin technique en conception de machines, les plans architecture, les cartes de géographie sont maintenant produites par des logiciels.

Dans les années 1980 sont apparus de nouveaux types de machines informatiques avec disque dur, écran et clavier. Elles ont d’abord fonctionné en mode « ligne de commande », purement textuel et asynchrone. Puis l'écran est devenu graphique et la souris, la couleur, le son sont apparus comme constituants de ces machines interactives. Pour construire des interfaces facilitant l'interaction, de nouveaux concepts sont apparus comme les fenêtres, les menus déroulants, les boutons à cliquer, les cases à cocher, les formulaires. La métaphore du bureau a fait le succès du MacIntosh : elle transfère les objets (dossiers, fichiers, corbeille) et les actions du travail de secrétariat (couper, coller) dans l’univers de l’interface. La manipulation directe prend le pas sur la description verbale des actions. C’est à travers les interfaces que les usagers ont ou non une bonne opinion du logiciel. La composition graphique des interfaces, les rapports entre les applications et les interfaces, l'ergonomie, l’esthétique et les chartes graphiques font l'objet de travaux de recherche soutenus.

Depuis ses débuts en 1950, l’informatique a connu des transformations profondes. La transformation des matériels informatiques en vitesse, puissance, fiabilité, miniaturisation est impressionnante : il y a dans un téléphone portable l'équivalent des plus grosses machines existant dans les années 1960. La transformation du logiciel est au moins aussi profonde, transformation en qualité, sécurité, complexité, réutilisabilité. La normalisation des objets, des processus, des langages informatiques a été indispensable pour que toutes ces transformations puissent avoir lieu. L’informatique a des usages professionnels dans tous les domaines et des usages privés : jeux, mail, Web. Ces transformations du matériel, du logiciel et des usages ont modifié profondément la structure économique politique et sociale des sociétés humaines. La sécurité des systèmes d'information devient un enjeu majeur.

L'informatique a permis la création de langages de programmation, de bibliothèques de programmes de calcul scientifiques, statistiques et autres, de traitements de textes, de bases de données. Ce sont des outils qui étendent notre pensée et notre mémoire et qui amènent de nouvelles façons de penser, de calculer, d'écrire, de communiquer.






http://histoire.info.online.fr/


