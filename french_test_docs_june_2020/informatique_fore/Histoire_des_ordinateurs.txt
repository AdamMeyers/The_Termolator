Histoire des ordinateurs

L'histoire des ordinateurs commence au milieu du . Si les premiers ordinateurs ont été réalisés après la Seconde Guerre mondiale, leur conception héritait de diverses expériences comme l'Harvard Mark I et le Z3, machines électromécaniques programmables commencées en 1939, et surtout de deux calculateurs électroniques : le Colossus du service de cryptanalyse britannique en 1943, l'ENIAC en 1945. À l'arrière-plan on peut mentionner des théories comme la « machine de Turing », ou des combinaisons de techniques bien plus anciennes comme les premières machines à calculer mécaniques () et les premières machines à tisser automatisées par la lecture de cartes et de rubans perforés ().

Depuis des milliers d'années, l'homme a créé et utilisé des outils l'aidant à calculer. Les plus anciens sont peut-être de simples notations de grandeurs comme les os d'Ishango, mais leur interprétation est très controversée : ne projetons-nous pas sur ces artefacts préhistoriques des notions mathématiques qui étaient impensables à l'époque ? Au départ, la plupart des sociétés utilisent sans doute la main (d'où le système décimal ou le système duodécimal), ainsi que d'autres parties du corps, comme auxiliaires de calcul. Puis apparaissent les entailles dans du bois, les entassements de cailloux, de coquillages ou d'osselets (il est intéressant de remarquer que le mot « calcul » provient du mot latin, "calculi" qui signifie « cailloux »). Le premier exemple d'outil plus complexe est l'abaque, qui connait diverses formes, jusqu'au boulier toujours utilisé en Chine et en Russie.

Un calculateur analogique est un calculateur qui utilise des mesures physiques continues (par exemple électriques, mécaniques ou hydrauliques) pour modéliser un problème à résoudre, comme le passage du temps, le déplacement d'un véhicule ou le déplacement des planètes. C'est un calculateur mais ce n'est pas une machine à calculer dont les touches sont toutes indépendantes bien qu'elles soient toutes liées par les règles de l'arithmétique.

Une fois que les conditions initiales d'un calculateur analogique sont entrées il n'est plus modifié que par l'action continue de son stimulateur (manivelle, pendule/poids, roue d'un véhicule). Un calculateur analogique produit toujours le même résultat pour des conditions initiales identiques.

Selon le physicien américain Derek Price, la machine d'Anticythère est un calculateur analogique, le plus ancien connu à ce jour est daté de 87 av. J.-C. Ce mécanisme à engrenages de bronze synthétise l'ensemble des connaissances astronomiques accumulées par les savants grecs permettant entre autres de prévoir la date et l'heure des éclipses lunaires et solaires des siècles à venir.

Un odomètre, décrit pour la première fois par le Romain Vitruve vers - 25} servait à mesurer la distance parcourue sur une route ; il était installé dans un chariot et il faisait tomber une petite boule dans un sac à chaque mille romain parcouru. Ce n'était pas un "calculateur analogique", mais un simple compteur.

Les algorithmes les plus anciens sont attestés par des tables datant de l'époque d'Hammurabi (env. 1750 av. J.-C.).

Vers des algorithmes plus généraux ont été publiés par des mathématiciens grecs de l'Antiquité, notamment Euclide et Eratosthène.

Vers le , Mohamed Ybn Moussa al-Khawarezmi passe pour être le père de la théorie des algorithmes ainsi que de l'algèbre (de l'arabe « Al-jabr » signifiant « compensation »).

Vers 1617, John Napier invente une sorte d'abaque perfectionné. Sa formulation des logarithmes démontre que la multiplication et la division peuvent se ramener à une série d'additions.

Cela permet en 1625 à William Oughtred de développer la règle à calcul qui est utilisée par de nombreux ingénieurs jusqu'à l'apparition des calculatrices de poche. Ainsi, une grande partie des calculs nécessaires au programme Apollo ont été - dit-on - effectués avec des règles à calcul.

Une horloge à calculer a été dessinée en 1623 par Wilhelm Schickard (1592-1635) professeur d'astronomie et de mathématiques à l'université de Heidelberg. Ce dispositif baptisé « horloge à calcul » aurait pu exécuter mécaniquement les additions et les soustractions, et presque aussi facilement les multiplications et les divisions. Mais on ignore s'il a été vraiment construit et, surtout, s'il était assez fiable pour être utilisé au-delà d'une démonstration.

Blaise Pascal, indépendamment de Schickard qu'il ne connaissait probablement pas, réalise en 1642, à l'âge de , une machine à calculer. Cette présentation d'une machine arithmétique ayant été attestée par des témoins dignes de foi, Pascal est crédité de l'invention de la machine à calculer, la pascaline, en 1642. Une "pascaline", signée par Pascal en 1652, est visible au musée des arts et métiers du Conservatoire national des arts et métiers à Paris. Une reproduction géante au Musée des Arts et métiers permet d'observer et comprendre les mécanismes internes.

Sa machine effectue les quatre opérations arithmétiques sans utiliser l'intelligence humaine, mais multiplications et divisions sont effectuées par répétitions. La pascaline est améliorée par Samuel Morland puis en 1673 par Gottfried Wilhelm Leibniz qui perfectionne le principe pour la rendre capable d'effectuer des multiplications, des divisions et même des racines carrées, le tout par une série d'additions sous la dépendance d'un "compteur".

Leibniz est le premier à réaliser la simplicité du système de numération binaire (vieux de plus de quatre mille ans !) dans les opérations arithmétiques ; Thomas Fantet de Lagny, un contemporain de Leibniz, remarqua qu'en arithmétique binaire les multiplications et divisions s'exécutent par de simples additions et soustractions : « Tout se passe comme si les nombres étaient leurs propres logarithmes ». Le système binaire est parfaitement adapté aux opérations logiques et arithmétiques et sera utilisé dans les futurs ordinateurs (soit sous forme de binaire pur pour les machines scientifiques, soit sous forme de "décimal codé binaire", ou DCB, pour les machines commerciales qui font plus d'entrée-sortie que de calcul). Néanmoins, jusqu'en 1945, la plupart de la dizaine de machines construites furent basées sur le système décimal, plus difficile à implanter.

La principale marque d'un ordinateur est sa "programmabilité". Celle-ci permet à l'ordinateur d'émuler toute autre machine à calculer en changeant la séquence des instructions disponibles.

En 1725, Basile Bouchon, un Lyonnais, met au point le premier système de programmation d'un métier à tisser grâce à un ruban perforé. En 1728, Jean-Baptiste Falcon, son assistant, remplace le ruban par une série de cartes perforées reliées entre elles. Jacques de Vaucanson reprend cette idée en remplaçant ruban et cartes perforées par un cylindre métallique et enfin Joseph Marie Jacquard lie le tout dans son métier à tisser qui fut adopté dans le monde entier et qui démontra qu'une machine pouvait être minutieuse, constante et dépendante.

En 1833, Charles Babbage, passionné par le métier Jacquard, propose une machine mécanique à calculer, très évoluée, « "la machine analytique"».

La machine qu'il conçoit « devait permettre de résoudre n'importe quelle équation et d'exécuter les opérations les plus compliquées de l'analyse mathématique. » C'est un calculateur mécanique programmable, fonctionnant à la vapeur, qui utilise des cartes perforées pour ses données et ses instructions. Bien que la théorie et le projet technique de Babbage aient été remarquablement pensés, bien qu'il en ait confié la réalisation à un atelier capable de produire les pièces mécaniques de la précision voulue, et bien qu'il ait été soutenu au début par l'Académie des sciences britannique, Babbage finit par lasser son constructeur comme ses financeurs, par son arrogance et ses changements de plans successifs : la construction de cette machine analytique, pas plus que les précédentes, n'aboutit.

Charles Babbage avait une collaboratrice la mathématicienne Ada Lovelace, comtesse et fille du poète britannique Lord Byron. Elle conçoit une série de programmes (suite de cartes perforées) destinés à cette machine, faisant figure rétrospectivement de « première programmeuse du monde ». De plus, elle pointe que les « diagrammes » ou « programmes » établis pour faire fonctionner la machine analytique constituent un capital immatériel mais réel de connaissances.
Elle montre également que la machine peut résoudre des équations algébriques ou manipuler des nombres imaginaires.

En 1885, les calculateurs sont agrémentés de claviers qui facilitent l'entrée des données. Par la suite, l'électricité permet de motoriser les calculateurs mécaniques et de remplacer certains mécanismes, (comme les manivelles) par de l'électromécanique.

Le recensement de la population des États-Unis de 1880 prit sept ans (précisément sept ans deux mois et 13 jours) à analyser. Un appel d'offres pour un système d'analyse plus rapide fut lancé avant le recensement de 1890. Des trois offres soumises, c'est la solution d'Herman Hollerith qui fut choisie car elle utilisait des cartes perforées qui la rendait deux fois plus rapide que les deux autres qui utilisaient un système de cartes de couleur. Herman Hollerith travailla pour le bureau du recensement de 1890 à 1894, puis en 1896, il créa "the Tabulating Machine company" qui sera une des trois compagnies dont la fusion est à l'origine d'IBM. Herman Hollerith utilisa pour les statistiques le principe de la carte perforée, rendu populaire par le métier à tisser de Jacquard.


Avant la Seconde Guerre mondiale, les ordinateurs analogiques, qu'ils fussent mécaniques ou électriques, étaient considérés comme le dernier cri de la technologie et beaucoup pensaient qu'ils seraient le futur de l'informatique. Ces ordinateurs analogiques utilisaient des quantités physiques, telles que la tension, le courant ou la vitesse de rotation des axes, pour représenter les nombres. Ainsi, ils devaient être reprogrammés manuellement à chaque nouveau problème. Leur avantage par rapport aux premiers ordinateurs numériques était leur capacité à traiter des problèmes plus complexes, avec une certaine forme de parallélisme.

Les "calculateurs stochastiques", où la grandeur physique était remplacée par une probabilité, parurent sur le moment être l'avenir du calculateur analogique : ils étaient en effet bon marché, faciles à produire en masse, et rapides (en particulier pour les multiplications). Mais les ordinateurs numériques, plus faciles encore à programmer, remplacèrent ces ordinateurs analogiques.

En 1936, la publication d'un article de logique mathématique "" constitue, avec d'autres recherches fondamentales menées notamment par A. Church et K. Gödel, un cadre théorique qui intéressera plus tard les fondateurs de la "science informatique". Mais il n'a guère d'influence sur la conception des premiers calculateurs programmables. La machine de Turing est une abstraction modélisant un « être calculant » pour démontrer une proposition de logique pure, et n'a rien à voir avec un projet de machine.
L'ère des ordinateurs modernes commença avec les grands développements de la Seconde Guerre mondiale. Les circuits électroniques, tubes à vide, condensateurs et relais remplacèrent leurs équivalents mécaniques et le calcul numérique remplaça le calcul analogique. Les ordinateurs conçus à cette époque forment la "première génération d'ordinateurs".

Vers 1954, les mémoires magnétiques (tores de ferrite pour la mémoire vive, bandes, ensuite disques amovibles puis fixes pour la mémoire de masse) supplantèrent toute autre forme de stockage et étaient dominantes au milieu des années 1960.

De nombreuses machines électromécaniques furent construites avec des capacités diverses. Elles n'eurent qu'un impact limité sur les constructions à venir.

En 1938, Konrad Zuse commença la construction des premières séries-Z, des calculateurs électromécaniques comportant une mémoire et une programmation limitée. Zuse fut soutenu par la Wehrmacht qui utilisa ces systèmes pour des missiles guidés. Le Z1 (ou "Versuchsmodell"), ne fonctionna jamais vraiment correctement, mais donna à son inventeur l'expérience nécessaire pour développer d'autres modèles, désormais avec l'appui de l'industrie et de l'armée. Les séries-Z furent les précurseurs de nombreuses avancées technologiques telles que l'arithmétique binaire et les nombres en virgule flottante.

Durant la même période, en 1938, John Vincent Atanasoff et Clifford E. Berry, de l'université de l'État de l'Iowa, développèrent l'ordinateur Atanasoff-Berry, un additionneur à . Cette machine avait pour but de résoudre des systèmes d'équations linéaires. La mémoire était stockée à l'aide de condensateurs fixés à un tambour rotatif.

En novembre 1939, John Vincent Atanasoff et Clifford E. Berry achevèrent l'ABC "(Atanasoff Berry Computer)". Composé de lampes et de tambours pour la mémoire, il est construit pour résoudre des systèmes d'équations linéaires. Bien que n'étant pas programmable, il est basé sur trois idées propres aux ordinateurs modernes :
Il pouvait stocker de dans ses deux tambours, fonctionnait à une vitesse d'horloge de et réalisait . Cette petite machine expérimentale ne fut jamais opérationnelle et ses auteurs se tournèrent vers d'autres sujets.

En 1940, aux Bell Laboratories, et Samuel Williams achèvent le "Complex Number Calculator" (ou Model I), un calculateur à base de relais téléphoniques, est la première machine, contrôlée à distance, "via" une ligne de téléphone, qui permet de réaliser une multiplication en une minute.

En 1941, Konrad Zuse construit le Z3 basé sur de téléphone, lisait les programmes sur bandes magnétiques et fonctionnait parfaitement, ce qui en fit le premier calculateur programmable fonctionnel. Il utilisait l'arithmétique binaire et les nombres à virgule flottante. Le Z3 pouvait enregistrer de , avait une fréquence de et réalisait quatre additions par seconde ou par minute. "A posteriori", il a été déterminé qu'il était Turing-complet, bien que rien n'indique qu'il ait été conçu pour cela (très peu de spécialistes du calcul avaient entendu parler d'Alan Turing à l'époque).

En 1944, le Harvard Mark I (ou l'ASCC, "") fut mis au point par Howard Aiken chez IBM. C'était une machine de calcul décimal qui lisait les programmes depuis une bande de papier. Elle pesait cinq tonnes et occupait une place de . Elle était composée de plusieurs calculateurs qui travaillaient en parallèle et réalisait trois opérations sur par seconde.

Pendant la Seconde Guerre mondiale, le Royaume-Uni fit de grands efforts à Bletchley Park pour déchiffrer les codes des communications militaires allemands. Le principal système de chiffrement allemand, Enigma (et ses différentes variantes), fut attaqué avec l'aide de machines appelées "bombes", créées par les services secrets polonais et améliorées par les Britanniques, qui permettaient de trouver les clés de chiffrement après que d'autres techniques en eurent réduit le nombre possible. Les Allemands créèrent également une autre série de systèmes de chiffrement (appelés FISH par les Britanniques) très différents d'Enigma. Pour casser ces systèmes, le professeur Max Newman et ses collègues développèrent Colossus. Il n'était pas Turing-complet. À la fin de la guerre, du fait de leur importance stratégique, la plupart des exemplaires furent détruits, le reste continuant à servir dans le plus grand secret.

En 1945, le Z4 de Konrad Zuse a été achevé et l'ETH de Zurich a loué et utilisé cette machine de 1950 à 1955.

Colossus était la première machine totalement électronique, elle utilisait uniquement des tubes à vide et non des relais. Elle était composée de tubes à vide et lisait des rubans perforés à la vitesse de caractères par seconde. Colossus implémentait les branchements conditionnels. Neuf machines ont été construites sur le modèle Mk II ainsi qu'une dixième lorsque la seule Mk I a été convertie en Mk II. L'existence de cette machine a été tenue secrète jusque dans les années 1970 ce qui explique pourquoi de nombreuses histoires de l'informatique n'en font pas mention. Il a été dit que Winston Churchill a personnellement donné l'ordre de leur destruction en pièces de moins de vingt centimètres pour conserver le secret. Une copie de cette machine Colossus a été reconstituée en 1994-1995 et fonctionne. Cette machine est au musée historique de Bletchley Park.

En septembre 1945, Presper Eckert et John William Mauchly achevèrent l'ENIAC ("Electronic Numerical Integrator and Computer"), gros calculateur entièrement électronique. Il avait été commandé en 1943 par l'armée américaine afin d'effectuer les calculs de balistique. L'ENIAC utilisait des tubes à vide (au nombre de ) et faisait ses calculs en système décimal. Malgré la véhémence de ses détracteurs qui auguraient de sa fragilité (celles des tubes à vide), il était très fiable pour l'époque et pouvait calculer plusieurs heures entre deux pannes. La machine est également célèbre pour ses dimensions physiques imposantes : elle pesait plus de , occupait et consommait une puissance de . Elle tournait à , était composée de fonctionnant en parallèle et pouvait effectuer ou par seconde.

À partir de 1948 apparurent les premières machines à architecture de von Neumann : contrairement à toutes les machines précédentes, les programmes étaient stockés dans la même mémoire que les données et pouvaient ainsi être manipulés comme des données. La première machine utilisant cette architecture était le Small-Scale Experimental Machine (SSEM) construit à l'université de Manchester la même année.

Le SSEM fut suivi en 1949 par le Manchester Mark I qui inaugura un nouveau type de mémoire composée de tubes cathodiques. La machine était programmée avec le programme stocké en mémoire dans un tube cathodique et les résultats étaient lus sur un deuxième tube cathodique.

Parallèlement, l'université de Cambridge développa l'EDSAC, inspiré des plans de l'EDVAC, le successeur de l'ENIAC. Contrairement à l'ENIAC qui utilisait le calcul en parallèle, l'EDVAC et l'EDSAC possédaient une seule unité de calcul. Il utilisait un type de mémoire différent du , constitué de lignes à retard de mercure. L'EDSAC tournait à une vitesse d'horloge de .

En 1950 naquit le premier ordinateur soviétique, le MESM (МЭСМ en russe, abréviation de "Malaïa Elektronnaïa tchetnaïa Machina", « petit calculateur électronique »), sous la direction de Sergeï A. Lebedev à l'Institut d'électrotechnologie de Kiev. Il était composé de à vide, consommait et réalisait .

En février 1951, le premier modèle de Ferranti Mark I (Sebastian Pietro Innocenzo Adhemar Ziani de Ferranti (né le 9 avril 1864 à Liverpool et décédé le 13 janvier 1930 à Zurich) est un ingénieur inventeur anglais, d'origine italienne, spécialisé dans l'électrotechnique)., version commerciale du Manchester Mark I et premier ordinateur commercial de l'histoire, est vendu. Il s'en vendra 9 jusqu'en 1957.

Quatre mois plus tard, P. Eckert et J. Mauchly de Remington Rand commercialisèrent l'UNIVAC I ("Universal Automatic Computer"). Contrairement aux machines précédentes, il ne lisait pas des cartes perforées mais des cassettes métalliques. Il possédait à vide, avait une mémoire à lignes à retard de mercure de de et consommait . Il exécutait ou par seconde. furent vendus au total, à plus d'un million de dollars l'unité.

En avril 1952, IBM produit son premier ordinateur scientifique, l'IBM 701, pour la défense américaine. L'IBM 701 utilisait une mémoire à tubes cathodiques (tubes de William) de de . Il effectuait ou par seconde. seront installées au total. La même année, IBM est contacté pour mettre en chantier la production des ordinateurs du réseau SAGE. Une cinquantaine de machines, portant le nom AN/FSQ7, sera produite. Chaque machine comportait , pesait et consommait .

En juillet 1953, IBM lance l'IBM 650, ordinateur commercial (ou de gestion). Réalisé à partir de tubes à vide, l'IBM 650 avait une mémoire à tambour de de , mais il était relativement lent. Il se présentait en de , l'un de contenant l'ordinateur, l'autre de contenant son alimentation électrique. Il coûtait ou pouvait être loué par mois. Environ furent produites jusqu'en 1962.

En 1954, la Société d'électronique et d'automatisme (SEA) livre son premier ordinateur, CAB 1011, au service français du Chiffre. D'autres suivront : CUBA pour le Laboratoire central de l'Armement, CAB 2000 et 3000.

En avril 1955, IBM lance l'IBM 704, ordinateur scientifique (son pendant commercial sera le 705) capable aussi de calculer sur des nombres à virgule flottante. L'architecture du 704 a été significativement améliorée par rapport au 701. Il utilisait une mémoire à tores de ferrite de de , bien plus fiable et plus rapide que les tubes cathodiques et les autres systèmes utilisés jusqu'alors. D'après IBM, le 704 pouvait exécuter . seront vendues jusqu'en 1960.

La deuxième génération d'ordinateurs est basée sur l'invention du transistor en 1947. Cela permit de remplacer le fragile et encombrant tube électronique par un composant plus petit et fiable. Les ordinateurs composés de transistors sont considérés comme la deuxième génération et ont dominé l'informatique dans la fin des années 1950 et le début des années 1960. Toutefois la notion de "génération", qui est à l'origine un argument commercial, est contestée par les historiens : elle ne tient compte que des technologies de l'unité logique, non des mémoires, de l'architecture ou de la programmation.

En 1955, Maurice Wilkes inventa la microprogrammation, désormais universellement utilisée dans la conception des processeurs. Le jeu d'instructions du processeur est défini par ce type de programmation.
En 1956, IBM sortit le premier système à base de disque dur, le Ramac 305 ("Random Access Method of Accounting and Control"). L'IBM 350 utilisait 50 disques de 24 pouces en métal, avec 100 pistes par face. Il pouvait enregistrer cinq mégaoctets de données et coûtait par mégaoctet.

Le premier langage de programmation universel de haut niveau à être implémenté, le Fortran (Formula Translator), fut aussi développé par IBM à cette période (le "Plankalkül", langage de haut niveau développé par Konrad Zuse en 1945 n'avait pas encore été implémenté à cette époque).

En 1958, la Compagnie des Machines Bull (France) annonce Le Gamma 60, livré en une quinzaine d'exemplaires à partir de 1960 : "Premier ordinateur multitâches dans le monde" et l'un des premiers à comporter plusieurs processeurs (voir multiprocesseur), cet ordinateur transistorisé présente cependant de graves défauts de conception, typiques d'une machine expérimentale. Comme la plupart des ordinateurs de son temps, il comporte diverses unités d'entrée et de sortie : bandes magnétiques, lecteurs de cartes, perforateurs de cartes, imprimantes, lecteurs de bande papier, perforateurs de bande papier.

En 1959, IBM lança l'IBM 1401 (commercial), qui utilisait des cartes perforées. Il fut le premier ordinateur vendu à plus de exemplaires. Il utilisait une mémoire magnétique de caractères (étendue plus tard à caractères).

En 1960, IBM lança l'IBM 1620 (scientifique). Il écrivait à l'origine sur des rubans perforés, mais évolua rapidement pour utiliser des lecteurs de cartes perforées comme le "1442". unités furent vendues. Il utilisait une mémoire magnétique de caractères décimaux. Un exemplaire opérationnel fut longtemps présent au Palais de la découverte.

La même année, Digital Equipment Corporation (DEC) lança le PDP-1 ("Programmed Data Processor"). Le PDP-1 était le premier ordinateur interactif et a lancé le concept de mini-ordinateur. Il avait une vitesse d'horloge de et pouvait stocker mots de 18 bits. Il effectuait opérations par seconde. Vendu pour seulement environ, il était l'un des premiers ordinateurs accessibles sur le simple budget d'un (gros) service sans remonter à la direction générale.

En 1960, la Société d'électronique et d'automatisme (SEA) commercialise la CAB500, véritable ordinateur personnel. Ses caractéristiques – interactivité, souplesse d'emploi, compacité et faible prix – la différencient des mainframes de l'époque. Le travail de l'utilisateur est facilité par le langage Programmation Automatique des Formules (PAF), qui traduit les fonctions explicites en langage machine. Plus d'une centaine d'exemplaires sont commercialisés, notamment dans les universités ou les écoles d'ingénieurs, et contribuent à former la première génération d'informaticiens français.

En 1960, des Français sortaient le Serel OA-1001, une machine 18 bits +signe +parité, 4 kmots, , purement binaire dédiée au contrôle de processus ou aux calculs scientifiques. Elle sera bientôt suivie par une version plus petite le Serel ODP-505, 3 fois plus rapide.

La troisième génération d'ordinateurs est celle des ordinateurs à circuit intégré qui ont été inventés par Jack St. Clair Kilby en 1958. C'est à partir de cette date que l'utilisation de l'informatique a explosé.

Les premiers ordinateurs utilisant les circuits intégrés sont apparus en 1963. L'un de leurs premiers usages a été dans les systèmes embarqués, notamment par la NASA dans l'ordinateur de guidage d'Apollo et par les militaires dans le missile balistique intercontinental LGM-30. Le circuit intégré autorise alors le développement d'ordinateurs plus compacts que l'on appele les mini-ordinateurs.

En 1964, IBM annonça la série 360, première gamme d'ordinateurs compatibles entre eux et première gamme aussi à couvrir l'ensemble des domaines d'applications commerciales et scientifiques. Plus de furent vendus jusqu'en 1970, date où on les remplaça par la série 370 beaucoup moins chère à puissance égale (mémoires bipolaires à la place des mémoires à tores magnétiques).

En 1965, DEC lance le PDP-8, machine bien moins encombrante destinée aux laboratoires et à la recherche. Avec une mémoire de de fonctionnant à il pouvait effectuer par seconde. Le PDP-8 se taille rapidement une place de choix dans les laboratoires, aidé par son langage "FOCAL" facile à maîtriser.

En 1966, Hewlett-Packard entre dans le domaine des mini-ordinateurs "universels" avec son HP-2116 fonctionnant avec une mémoire à tores. Celui-ci supportait de nombreux langages, dont l'Algol et le Fortran, « comme les grands », et le BASIC y sera adjoint plus tard.

En 1966, le gouvernement français lance le Plan Calcul destiné à assurer l'indépendance du pays en matière d'ordinateurs.

En 1969, Data General vend Nova à l'unité. Le Nova était dans les premiers mini-ordinateurs . La version "Supernova" qui lui succédera en 1971 effectuait une multiplication en "une microseconde". Le processeur principal était contenu sur un circuit imprimé de . Dans le même temps, grâce à une politique de mise en commun gratuite de logiciels particulièrement innovateurs (et vue aujourd'hui comme l'ancêtre de l'Open Source), 

À la même époque, Phillips (marque hollandaise bien connue de produits grand public) lance une série d'ordinateurs du type « 360 » pour concurrencer IBM, ils étaient plus rapides et largement aussi fiables (c'est-à-dire assez peu...) mais comme ils utilisaient un système d'exploitation spécifique, ils disparurent rapidement du marché. Siemens et tentent de supplanter IBM sur ce créneau du « 360 » mais sans grand succès. Seuls Control-Data et Cray réussissent à rivaliser avec les haut de gamme d'IBM dans les années 1970-80.

Le mini-ordinateur a été une innovation des années 1970 qui devint significative vers la fin de celles-ci. Il apporta la puissance de l'ordinateur à des structures décentralisées, non seulement grâce à un encombrement plus commode, mais également en élargissant le nombre de constructeurs d'ordinateurs.

En 1972, le réseau Cyclades français est développé à l'IRIA avec le soutien de la Délégation à l'informatique, dans le cadre du Plan Calcul. Parallèlement, la CII présente sa Distributed System Architecture. Ces réseaux numériques permettent de partager les ressources informatiques des centres universitaires et de grandes organisations comme EDF ou le Commissariat à l'énergie atomique : on commence à parler de calcul distribué. À partir de là, la conception du grand système est concurrencé par les mini-ordinateurs en réseau, comme le DEC PDP8, le Mitra 15 puis le Mini 6. Aux États-Unis, IBM et DEC créent les architectures SNA et DECnet, en profitant de la numérisation du réseau d'AT&T (voir Réseau téléphonique commuté).

En 1972, Hewlett-Packard lance le HP 3000, mini-ordinateur de gestion fonctionnant en multi-tâches temps réel et multi-utilisateur, suivit en 1974 d'ordinateurs techniques et de contrôle de processus multi-tâches temps réel la série 21MX.

L'intégration de circuits intégrés à grande échelle conduisit au développement de processeurs très petits, comme celui qui analyse les données de vol dans les avions F14A Tomcat de l'US Navy. On ignorait alors encore que l'explosion à distance d'une charge nucléaire les rendrait instantanément inopérants (effet EMP).

En 1973, le "TV Typewriter" de Don Lancaster permit le premier d'afficher des informations alphanumériques sur une télévision ordinaire. Il était composé de de composants électroniques, incluait deux cartes mémoires et pouvait générer et stocker 512 caractères. Une cassette optionnelle fournissait une capacité de 100 pages de textes supplémentaires. Clive Sinclair se basera plus tard sur cette approche pour construire son Sinclair ZX80.

Dans les années 1970, IBM a sorti une série de mini-ordinateurs, la série 3 : 3/6, 3/8, 3/10, 3/12, 3/15.
Ensuite, dans les années 1980, la série 30 : 32, 34, 36, 38.
Une troisième série a succédé à la série 30 : les AS/400. La première version du système d'exploitation n'était qu'une évolution de celui du 38. Le but était de ne conserver qu'un seul système d'exploitation pour l'ensemble de la gamme mini. Il était capable d'émuler les 34 et les 36.

Dans les années 1980, DEC devint le deuxième fabricant d'ordinateurs derrière IBM (avec un chiffre d'affaires représentant le cinquième de celui-ci) grâce à ses ordinateurs populaires PDP (surtout le PDP-11, première machine de DEC à utiliser des mémoires de 16 bits et non de 12, et machine "sur" laquelle et "pour" laquelle fut développé le langage C) et VAX, qui apportera le confort du système VMS.

Une définition non universellement acceptée associe le terme de quatrième génération à l'invention du microprocesseur par Marcian Hoff et Federico Faggin (physicien et inventeur italien, spécialisé en physique du solide). En pratique et à la différence des autres changements de génération, celle-ci constitua plus une évolution (presque passée inaperçue) qu'une révolution : les circuits s'étaient miniaturisés de plus en plus depuis l'invention du circuit intégré.

C'est pour cette raison que certains considèrent que les générations sont devenues des questions de "type de logiciel" :

On peut aussi considérer que la notion de « générations » est un concept marketing, lancé en 1964 par IBM, et n'a aucun intérêt historique : il ne tient compte ni des technologies de mémoire (tambours magnétiques, tores de ferrite…), ni des périphériques, ni de l'évolution du logiciel. Surtout, il n'explique rien de la logique de développement de ces techniques. Aujourd'hui, on peut considérer que l'immense majorité des ordinateurs relèvent toujours de la "4e génération", qui serait donc à elle seule plus longue que les trois précédentes !

Le 15 novembre 1971, Intel dévoile le premier microprocesseur commercial, le 4004. Il a été développé pour Busicom, un constructeur japonais. Un microprocesseur regroupe la plupart des composants de calcul (horloge et mémoire mises à part pour des raisons techniques) sur un seul circuit. Couplé à un autre produit, la "puce mémoire", le microprocesseur permet une diminution nouvelle des coûts. Le 4004 ne réalisait que opérations par seconde, mais la puissance de ses successeurs répondit à la loi de Moore.

Les "superordinateurs" intégrèrent aussi souvent des microprocesseurs.

En 1976, le Cray-1 fut développé par Seymour Cray, qui avait quitté Control Data en 1972 pour créer sa propre société. C'était l'un des premiers ordinateurs à mettre en pratique le traitement vectoriel, qui appliquait la même instruction à une série consécutive d'opérandes (évitant ainsi des coûts de décodage répétés). Le Cray-1 pouvait calculer 150 millions d'opérations à virgule flottante par seconde. 85 exemplaires furent vendus à cinq millions de dollars l'unité.

Parmi ses clients en France : l'École polytechnique (simulations et calculs numériques) ; Michelin (étude de résistance des pneumatiques par la méthode des éléments finis) ; Peugeot (simulations intensives de déformations de l'habitacle d'une voiture en cas de choc frontal ou latéral).

En 2010, la hiérarchie s'est modifiée avec l'arrivée des constructeurs asiatiques et surtout chinois :

À noter que le TERA-100 (construit par le fabricant français Bull pour le Commissariat à l'énergie atomique est classé en première position en Europe et sixième en position mondiale (puissance de )

Eux aussi bénéficièrent de l'usage des microprocesseurs et l'on peut même dire que la "généralisation des réseaux informatiques" n'a été possible que par l'invention des microprocesseurs. Les contrôleurs 3745 (IBM) utilisaient intensivement cette technologie. Dans le même temps, aux États-Unis, la compagnie AT&T se rendit compte qu'avec tous ses standards téléphoniques interconnectés, elle se trouvait sans l'avoir cherché disposer du "plus grand réseau d'ordinateurs des États-Unis" (un standard téléphonique, depuis l'invention des microprocesseurs, tient beaucoup plus de l'ordinateur que du dispositif câblé, et nombre d'entre eux se commandent en UNIX).

En janvier 1973 est présenté le premier micro-ordinateur, le Micral conçu par François Gernelle de la société R2E dirigée par André Truong Trong Thi. Basé sur le premier microprocesseur 8 bits d', le i8008, ses performances en font le plus petit ordinateur moderne de l'époque (, mémoire RAM de en version de base), correspondant à son prix : , soit le prix d'un bon portable d'aujourd'hui.
La machine a été développée pour un laboratoire d'agronomie qui ne pouvait s'offrir un mini-ordinateur DEC PDP8. Elle est rapidement mise en production industrielle, annoncée dans la presse professionnelle française et américaine, présentée au Sicob et vendue pour équiper des installations chimiques ou des péages d'autoroute. De nouvelles versions seront développées ensuite, au total une vingtaine de machines multi-utilisateurs, parfois multiprocesseurs, sous systèmes d'exploitation temps réel Prologue et CP/M. Le succès nécessitant de nouveaux capitaux, R2E passe sous le contrôle de Bull à partir de 1978. En 1982, la conversion de Bull à la compatibilité IBM provoque le départ de l'ancienne équipe R2E, qui fonde de nouvelles entreprises de micro-informatique.

Au Sicob 1973 est également apparu un micro-ordinateur allemand. Le DIEHL Alphatronic utilise lui aussi le microprocesseur Intel 8008.Il comprend une unité centrale équipée d'un Intel 8008 ( extensible à ), d'un lecteur enregistreur de mini-cassette magnétique et d'une imprimante à boule IBM. Il ne comportait pas d'écran. La programmation en mini-basic était visualisée sur une mini imprimante (bande papier en rouleau). Prix de vente de l'ensemble .

Présenté en avril 1974, le processeur Intel 8080 va conduire à la première vague d'ordinateurs personnels, à la fin des années 1970. La plupart d'entre eux utilisait le bus S-100 et le système d'exploitation CP/M-80 de Digital Research. CP/M-80 était le premier système d'exploitation à être utilisé par plusieurs fabricants d'ordinateurs différents, et de nombreux logiciels furent développés pour lui. Le système MS-DOS de Microsoft, acheté par Microsoft à Tim Paterson de la société Seattle Computer Products (qu'il avait appelé QDOS pour "Quick and Dirty Operating System") s'en inspira fortement (en inversant l'ordre de certains opérandes pour ne pas encourir de procès, ce qui provoqua quelques catastrophes chez ceux qui utilisaient les deux systèmes).

En janvier 1975, sort l'Altair 8800. Développé par des amateurs, frustrés par la faible puissance et le peu de flexibilité des quelques ordinateurs en kit existant sur le marché à l'époque, ce fut certainement le premier ordinateur personnel en kit produit en masse. Il était le premier ordinateur à utiliser un processeur Intel 8080. L'Altair inaugura le bus S-100. Ce fut un énorme succès et unités furent vendues. C'est l'Altair qui inspira le développement de logiciels à Bill Gates et Paul Allen, qui développèrent un interpréteur BASIC pour cette machine.
En 1975 sortira aussi l'IBM 5100, machine totalement intégrée avec son clavier et son écran, qui se contente d'une prise de courant pour fonctionner.

Toujours en 1975, le fabricant de terminaux programmables TRW se rend compte que son terminal Datapoint 2200 à disquettes (de huit pouces) "est" un ordinateur si on l'équipe d'un langage évolué (BASIC) et d'un système d'exploitation (CP/M), et commence à le commercialiser comme tel, en inventant le premier réseau local pour micros : "ARCnet". Ce système, commercialisé en France par Matra, ne sera cependant jamais proposé au grand public.

De nombreux amateurs tentent à cette époque de créer leurs propres systèmes. Ces passionnés se rencontrent lors de réunions au Homebrew Computer Club, où ils montrent leurs réalisations, comparent leurs systèmes et échangent des plans ou des logiciels. Certains de ces amateurs s'intéressent à construire quelque chose de prêt à l'emploi que Monsieur tout le monde puisse s'offrir.

En 1976, Steve Wozniak, qui fréquentait régulièrement le Homebrew Computer Club, conçoit l'Apple I, doté d'un processeur MOS Technologie 6502 à . Il vend avec Steve Jobs environ 200 machines à l'unité. Il est doté d'un microprocesseur et d'un clavier.

En 1977, sort l'Apple II. Malgré son prix élevé (environ ), il prend rapidement l'avantage sur les deux autres machines lancées la même année, le TRS-80 et le Commodore PET, pour devenir le symbole du phénomène de l'ordinateur personnel. D'une très grande qualité, l'Apple II a de gros avantages techniques sur ses concurrents : il dispose d'une architecture ouverte, d'un lecteur de disquettes, et utilise des graphismes en couleur. Grâce à l'Apple II, Apple domine l'industrie de l'ordinateur personnel entre 1977 et 1983. Plus de deux millions d'Apple II sont vendus.

En 1978, devant le succès de l'Apple II, IBM décide de renouer avec le marché de l'ordinateur personnel (le marché avait trouvé le 5100 trop lent, le 5110 trop lourd physiquement, et le System 23 Datamaster – créé pour faire pendant au TRW-2200 – n'avait pas bénéficié d'un support marketing suffisant à l'époque). Frank Cary confie une équipe, un budget et donne carte blanche à Don Estridge.

En août 1981 sort l'IBM PC "(Personnal Computer)" qui utilise un processeur Intel 8088 tournant à et peut faire tourner trois systèmes d'exploitation différents : PC-DOS, CP/M-86 et PC/IX. L'UCSD p-System sera également utilisable, mais non supporté par IBM. Microsoft s'est réservé, contre réduction de la facture à IBM, le droit de commercialiser sa propre version du PC-DOS pour d'autres ordinateurs de marque non-IBM, et qui sera nommée le MS-DOS. .

, dévoilé par Commodore International en septembre 1982. Il utilise un processeur MOS Technology 6510 à et coûte . Il avait un écran et possédait une carte son. Entre 17 et d'unités sont vendues jusqu'en 1993.

Après le 64, Commodore sortit l'Amiga. Ses possibilités en matière de graphisme et la rapidité de son processeur permettaient de programmer des jeux, en particulier en utilisant le langage Amos.

À cette époque apparurent les premiers « clones » compatibles, comme le Franklin 1000 compatible avec l'Apple II ou le premier PC compatible lancé par Compaq en mars 1983. Cette concurrence sur le marché des ordinateurs personnels permit de faire baisser les prix et de rendre ces machines populaires.

En 1982, Intel lance le 80286, et IBM le PC/AT qui l'utilise. C'est à cette époque que le PC devint l'architecture dominante sur le marché des ordinateurs personnels. Seul le Macintosh d'Apple continua à défier l'IBM PC et ses clones, qui devinrent rapidement le standard.

En 1983, Apple lance le Lisa, le premier ordinateur personnel doté d'une interface graphique. Le Lisa utilisait un processeur Motorola 68000, un disque dur de , deux lecteurs de disquette et de RAM. Son interface graphique s'inspirait de celle du Xerox Star. Malgré son caractère révolutionnaire pour l'époque, ce fut un échec commercial, principalement à cause de son prix élevé () et de sa relative lenteur.
Le , Apple lance le Macintosh, le premier micro-ordinateur à succès utilisant une souris et une interface graphique. Il reprenait plusieurs caractéristiques du Lisa, comme le processeur Motorola 68000, mais pour un prix bien plus abordable : , grâce à l'abandon de quelques fonctionnalités du Lisa comme le multitâche. Il était fourni avec plusieurs applications utilisant la souris, comme MacPaint et MacWrite.

On peut citer aussi l'Atari ST, apparu en 1985, qui connait un grand succès dans le monde musical en raison de la présence d'une interface MIDI.

Malgré ses nombreuses innovations dans le domaine, Apple perdit peu à peu des parts de marché pour se stabiliser à environ 4 % des ventes d'ordinateurs dans les années 2000. Et ce, malgré le succès de l'iMac, premier ordinateur conçu par des designers, qui s'écoula à plus de six millions d'exemplaires, en en faisant le modèle d'ordinateur personnel le plus vendu au monde. Parallèlement, le PC Compatible s'imposa de plus en plus au grand public avec des assembleurs tel que Hewlett-Packard, Compaq, Dell ou NEC.

Les années 1990 ont été marquées par la préparation à la correction du problème de l'an 2000 (ou « bug de l'an 2000 », appelé "Y2K" dans le monde anglo-saxon), qui affectait presque tous les logiciels, système d'exploitation compris, pour cause de place mémoire disponible, codait la date sur soit deux caractères pour les années (99 pour 1999), de sorte qu'au passage à l'an 2000, le codage des dates allait revenir à 00 et être interprétée comme 1900. Ce défaut de conception systémique se manifestait également dans la plupart des logiciels, dont les sous-programmes de gestion de date reprenant la date système le plus souvent sans modification du format.

La résolution de ce problème s'est faite soit par la conversion des logiciels, sans changement du matériel, soit aussi par le remplacement complet du matériel et du logiciel, en profitant des progrès techniques de diminution de taille des ordinateurs rendus possibles par la miniaturisation des composants ("downsizing"). Cela a permis de remplacer les logiciels spécifiques affectés par le problème, par des logiciels ou des progiciels le plus souvent sous UNIX avec des ordinateurs de taille réduite.

Cette décennie a aussi été marquée bien sûr par l'ouverture de l'Internet au commerce, fin 1992, puis par l'expansion de la Toile. La convergence de l'informatique, de l'Internet, et des télécommunications a été décrite par une nouvelle expression, les « technologies de l'information et de la communication » (TIC), que la DGLFLF préfère appeler techniques de l'information et de la communication, afin d'éviter l'usage abusif du mot technologie. Leur expansion a entraîné une . Avec Internet s'ouvre surtout un nouveau chapitre, non seulement de l'histoire de l'informatique, mais aussi de l'économie mondiale : numérisation de l'économie (biens et services), délocalisation, « uberisation », ces termes expriment à la fois des bouleversements de l'économie et des inquiétudes socio-politiques.

Avec les progrès de la miniaturisation, le premier « smartphone », l'IBM Simon, fut conçu en 1992 puis commercialisé en août 1994. Ensuite se sont développés les PDA (Personal Digital Assistants), comme le Psion ou le Palm Pilot et surtout le Blackberry, qui combinaient les fonctions d'agenda électronique, de bloc-note, de téléphone et de terminal Internet. Le terme "PDA" a été ensuite remplacé par "smartphone".

L'année 2001 a vu le lancement du WA3050 de Sagem qui fut l'un des premiers à combiner les fonctions d'un téléphone mobile et d'un PDA tactile. Il était compatible avec la nouvelle norme de l'époque, le GPRS.

En 2014, La 4G est présente dans plusieurs dizaines de pays dont la France.

Les principaux fabricants de téléphones se lancent dans l'aventure (comme Nokia, le leader à cette époque, LG ou Samsung), ainsi que de nouvelles sociétés spécialisées dans les smartphones (comme Research In Motion avec le BlackBerry). L'OS de référence est alors Symbian utilisé principalement par Nokia et Ericsson.
Ces constructeurs européens seront ensuite balayés par la concurrence chinoise, coréenne et américaine.



