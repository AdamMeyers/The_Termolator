Informatique

L'informatique est un domaine d'activité scientifique, technique, et industriel concernant le traitement automatique de l'information par l'exécution de programmes informatiques par des machines : des systèmes embarqués, des ordinateurs, des robots, des automates

Ces champs d'application peuvent être séparés en deux branches, l'une, de nature théorique, qui concerne la définition de concepts et modèles, et l'autre, de nature pratique, qui s'intéresse aux techniques concrètes de mise en œuvre. Certains domaines de l'informatique peuvent être très abstraits, comme la complexité algorithmique, et d'autres peuvent être plus proches d'un public profane. Ainsi, la théorie des langages demeure un domaine davantage accessible aux professionnels formés (description des ordinateurs et méthodes de programmation), tandis que les métiers liés aux interfaces homme-machine sont accessibles à un plus large public.

Le terme « informatique » résulte de l'association du terme « information » au suffixe « -ique » signifiant « qui est propre à ». Comme adjectif, il s'applique à l'ensemble des traitements liés à l'emploi des ordinateurs et systèmes numériques. Comme substantif, il désigne les activités liées à la conception et à la mise en œuvre de ces machines. Des questions de télécommunications comme le traitement du signal ou la théorie de l'information, aussi bien que des problèmes mathématiques comme la calculabilité s'y rattachent. Dans le vocabulaire universitaire américain, l'informatique () désigne surtout l'informatique théorique : un ensemble de sciences formelles qui ont pour objet d'étude la notion d'information et des procédés de traitement automatique de celle-ci, l'algorithmique.

Les applications de l'informatique depuis les années 1950 forment la base du secteur d'activité des technologies de l'information et de la communication. Ce secteur industriel et commercial est lié à la fois aux procédés (logiciel, architectures de systèmes) et au matériel (électronique, télécommunication). Le secteur fournit également de nombreux services liés à l'utilisation de ses produits : développement, maintenance, enseignement, assistance, surveillance et entretien.

En 1957, l'ingénieur allemand Karl Steinbuch crée le terme « » pour son essai intitulé , pouvant être rendu en français par « Informatique : traitement automatique de l'information ».

En mars 1962, Philippe Dreyfus, ancien directeur du Centre national de calcul électronique de Bull, utilise pour la première fois en France le terme « Informatique » pour son entreprise « Société d'informatique appliquée » (SIA). Selon certains, ce néologisme est un mot-valise qui agglomère « information » et « automatique », pour désigner le traitement automatique des données.

. . En 1985 Sterling Software rachète la société Informatics Inc. qui cesse ses activités en 1986. .

En 1966, l'Académie française consacre l'usage officiel du mot pour désigner la . La presse, l'industrie et le milieu universitaire l'adoptent dès cette époque.

En juillet 1968, le ministre fédéral de la Recherche scientifique d'Allemagne, Gerhard Stoltenberg, prononce le mot « » lors d'un discours officiel sur la nécessité d'enseigner cette nouvelle discipline dans les universités de son pays ; on emploie ce même terme pour nommer certains cours dans les universités allemandes. Le mot fait alors son apparition en Italie et en Espagne, de même qu’ au Royaume-Uni.

Les fondateurs de la Compagnie Générale d'Informatique (CGI) reprennent le mot « informatique » en 1969.

Dans l'usage contemporain, le substantif « informatique » devient un mot polysémique qui désigne autant le domaine industriel en rapport avec l'ordinateur (au sens de calculateur fonctionnant avec des algorithmes), que la science du traitement des informations par des algorithmes.

Les expressions « science informatique », « informatique fondamentale » ou « informatique théorique » désignent sans ambiguïté la science, tandis que « technologies de l'information » ou « technologies de l'information et de la communication » désignent le secteur industriel et ses produits. Des institutions assimilent parfois la compétence des utilisateurs dans la manipulation des appareils à l'alphabétisation ou à la conduite automobile, comme veut le faire entendre l'expression (traduction littérale : « permis de conduire un ordinateur »).

Plusieurs termes en anglais désignent l'informatique :

Depuis des millénaires, l'Homme a créé et utilisé des outils l'aidant à calculer (abaque, boulier), exigeant, comme les opérations manuelles, des algorithmes de calcul, dont des tables datant de l'époque d'Hammourabi (environ -1750) figurent parmi les exemples les plus anciens.

Si les machines à calculer évoluent constamment depuis l'Antiquité, elles n'exécutent pas elles-mêmes l'algorithme : c'est l'homme qui doit apprendre et exécuter la suite des opérations, comme pour réaliser les différentes étapes d'une division euclidienne). En 1642, Blaise Pascal imagine une machine à calculer, la "Pascaline", qui fut commercialisée. Sept exemplaires subsistent dans des musées comme celui des arts et métiers et dont deux, qui sont dans des collections privées (IBM en possède une). Joseph Marie Jacquard avec ses métiers à tisser à cartes perforées illustre en premier le concept de programmation, comme enchaînement automatique d'opérations élémentaires. George Boole et Ada Lovelace esquissent une théorie de la programmation des opérations mathématiques.

Dans les années 1880, Herman Hollerith, futur fondateur d'IBM, fonde la mécanographie en inventant une machine électromécanique destinée à faciliter le recensement en stockant les informations sur une carte perforée. Le gouvernement des États-Unis utilise pour la première fois à grande échelle les trieuses et les tabulatrices lors du recensement de 1890, à la suite de l'afflux des immigrants dans ce pays dans la seconde moitié du .

L'ingénieur norvégien Fredrik Rosing Bull a créé la première entreprise européenne qui a développé et commercialisé des équipements mécanographiques. Installé en Suisse dans les années 1930 il est ensuite venu en France pour s'attaquer au marché français. Pendant la Seconde Guerre mondiale, René Carmille utilisait des machines mécanographiques Bull.

Les Allemands étaient équipés de machines mécanographiques avant la Seconde Guerre mondiale. Ces équipements étaient installés dans des ateliers composés de trieuses, interclasseuses, perforatrices, tabulatrices et calculatrices connectées à des perforateurs de cartes. Des machines électromécaniques utilisant aussi des lampes radio comme les triodes effectuaient les traitements. Ces lampes dégageaient de la chaleur qui attirait les insectes, et les (terme anglais pour "insectes", francisé en « bogue ») étaient une cause de panne courante. L'informatique moderne n'a pu émerger qu'à la suite de l'invention du transistor en 1947 et son industrialisation dans les années 1960.

L'informatique moderne commence avant la Seconde Guerre mondiale, lorsque le mathématicien Alan Turing pose les bases d'une théorisation de ce qu'est un ordinateur, avec son concept de machine universelle de Turing. Turing pose dans son article les fondements théoriques de ce qui sépare la machine à calculer de l'ordinateur : la capacité de ce dernier à réaliser un calcul en utilisant un algorithme conditionnel.

Après la Seconde Guerre mondiale, l'invention du transistor, puis du circuit intégré permettront de remplacer les relais électromécaniques et les tubes à vide, qui équipent les machines à calculs pour les rendre à la fois plus petites, plus complexes, plus économiques et plus fiables. Le capital-risque finance des dizaines de sociétés électroniques.

Avec l'architecture de von Neumann, mise en application de la machine universelle de Turing, les ordinateurs dépassent la simple faculté de calculer et peuvent commencer à accepter des programmes plus évolués, de nature algorithmique.

Dans les années 1970, l'informatique se développe avec les télécommunications, avec Arpanet, le réseau Cyclades et la Distributed System Architecture (DSA) de réseau en couches, qui donnera naissance en 1978 au modèle OSI, appelé aussi « OSI-DSA », puis aux protocoles TCP-IP dans les années 1990, grâce à la baisse des prix des microprocesseurs. Les concepts de datagramme et d'informatique distribuée, d'abord jugés risqués, s'imposeront grâce à l'Internet.

La série de livres de Donald Knuth, publiée à partir des années 1960, fait ressortir les aspects mathématiques de la programmation informatique. Edsger Dijkstra, Niklaus Wirth et Christopher Strachey travaillent et publient vers un même axe. Ces travaux préfigurent d'importants développements en matière de langage de programmation.

L'amélioration de l'expressivité des langages de programmation a permis la mise en œuvre d'algorithmes toujours plus sophistiqués, appliqués à des données de plus en plus variées. La miniaturisation des composants et la réduction des coûts de production, associées à une augmentation de la demande en traitements des informations de toutes sortes (scientifiques, financières, commerciales), ont eu pour conséquence une diffusion de l'informatique dans tous les secteurs économiques, ainsi que dans la vie quotidienne des individus.

Dans les années 1970, Xerox fait réaliser des études en psychologie cognitive et en ergonomie en vue de simplifier l'utilisation des outils informatiques. L'interface graphique propose un accès à la machine plus proche des objets ordinaires que l'interface en ligne de commande existant jusque-là. Les constructeurs souhaitant concurrencer le géant IBM promeuvent une informatique plus décentralisée.

La démocratisation de l'utilisation d'Internet – réseau basé sur ARPANET – depuis 1995, a amené les outils informatiques à être de plus en plus utilisés dans une logique de réseau comme moyen de télécommunication, à la place des outils tels que la poste ou le téléphone. Elle s'est poursuivie avec l'apparition des logiciels libres, puis, des réseaux sociaux et des outils de travail collaboratif dont Wikipédia n'est qu'un des nombreux exemples. Face à la demande pour numériser photos et musiques, les capacités de stockage, de traitement et de partage des données explosent et les sociétés qui ont parié sur la croissance la plus forte l'emportent le plus souvent, en profitant d'une .

En France, l'informatique n'a commencé à se développer que dans les années 1960, avec le Plan Calcul. Depuis lors, les gouvernements successifs ont mené des politiques diverses en faveur de la Recherche scientifique, l'Enseignement, la tutelle des Télécommunications, la nationalisation d'entreprises clés.

La science informatique est une science formelle, dont l'objet d'étude est le calcul au sens large, c'est-à-dire, non pas exclusivement arithmétique, mais en rapport avec tout type d'information que l'on peut représenter par une suite de nombres. Ainsi, textes, séquences d'ADN, images, sons ou formules logiques peuvent faire l'objet de calculs. Selon le contexte, on parle d'un calcul, d'un algorithme, d'un programme, d'une procédure.

Un algorithme est une manière systématique de procéder pour arriver à calculer un résultat. Un des exemples classiques est l'algorithme d'Euclide du calcul du « Plus grand commun diviseur » (PGCD) qui remonte au moins à 300 ans , mais il s'agit déjà d'un calcul complexe. Avant cela, le simple fait d'utiliser un abaque demande d'avoir réfléchi à un moyen systématique (et correct) d'utiliser cet outil pour réaliser des opérations arithmétiques.

Des algorithmes existent donc depuis l'Antiquité, mais .

Il existe de nombreux modèles de calcul, dont les deux principaux sont la « machine de Turing » et le « lambda calcul ».
Ces deux systèmes formels définissent des objets qui peuvent représenter ce qu'on appelle des procédures de calcul, des algorithmes ou des programmes. Ils définissent ensuite, un moyen systématique d'appliquer ces procédures, c'est-à-dire de calculer.

Le résultat le plus important de la calculabilité est probablement le fait que les principaux modèles de calcul ont exactement la même puissance. C'est-à-dire qu'il n'existe pas de procédure que l'on pourrait exprimer dans un modèle mais pas dans un autre. La thèse de Church postule que ces modèles de calcul équivalents décrivent complètement et mathématiquement tout ce qui est physiquement calculable.

Un deuxième résultat fondamental est l'existence de fonctions incalculables, une fonction étant ce que calcule une procédure ou un algorithme (ceux-ci désignant plutôt comment faire le calcul). On peut montrer qu'il existe des fonctions, bien définies, pour lesquelles il n'existe pas de procédure pour les calculer. L'exemple le plus connu étant probablement le problème de l'arrêt, qui montre qu'il n'existe pas de machine de Turing calculant si une autre machine de Turing donnée s'arrêtera (et donc donnera un résultat) ou non.

Tous les modèles de calcul étant équivalents, ce résultat s'applique aussi aux autres modèles, ce qui inclut les programmes et logiciels que l'on peut trouver dans les ordinateurs courants. Il existe un lien très fort entre les fonctions que l'on ne peut pas calculer et les problèmes que l'on ne peut pas décider (voir Décidabilité).

L'algorithmique est l'étude comparative des différents algorithmes. Tous les algorithmes ne se valent pas : le nombre d'opérations nécessaires pour arriver à un même résultat diffère d'un algorithme à l'autre. Ce nombre d'opérations, appelé la complexité algorithmique est le sujet de la théorie de la complexité des algorithmes, qui constitue une préoccupation essentielle en algorithmique.

La complexité algorithmique sert en particulier à déterminer comment le nombre d'opérations nécessaires évolue en fonction du nombre d'éléments à traiter (la taille des données) :

Nous arrivons maintenant à un problème ouvert fondamental en informatique : « P est-il égal à NP ? ». En simplifiant beaucoup : P est « l'ensemble des problèmes pour lesquels on connaît un algorithme efficace » et NP « l'ensemble des problèmes pour lesquels on connaît un algorithme efficace pour vérifier une solution à ce problème ». Et en simplifiant encore plus : existe-t-il des problèmes difficiles ? Des problèmes pour lesquels il n'existe pas d'algorithme efficace ?

Cette question est non seulement d'un grand intérêt théorique mais aussi pratique. En effet, un grand nombre de problématiques courantes et utiles sont des problèmes que l'on ne sait pas résoudre de manière efficace. C'est d'ailleurs un des problèmes du prix du millénaire et le Clay Mathematics Institute s'est engagé à verser un million de dollars aux personnes qui en trouveraient la solution.

C'est un problème ouvert, donc formellement, il n'y a pas de réponse reconnue. Mais, en pratique, s'accordent pour penser que P≠NP, c'est-à-dire qu'il existe effectivement des problèmes difficiles qui n'admettent pas d'algorithme efficace.

Ce type de problème de complexité algorithmique est directement utilisé en cryptologie. En effet, les méthodes de cryptologie modernes reposent sur l'existence d'une fonction facile à calculer qui possède une fonction réciproque difficile à calculer. C'est ce qui permet de chiffrer un message qui sera difficile à décrypter (sans la clé).

La plupart des chiffrements (méthode de cryptographie) reposent sur le fait que la procédure de Décomposition en produit de facteurs premiers n'a pas d'algorithme efficace connu. Si quelqu'un trouvait un tel algorithme, il serait capable de décrypter la plupart des cryptogrammes facilement. On sait d'ailleurs qu'un calculateur quantique en serait capable, mais ce genre d'ordinateur n'existe pas, en tout cas pour le moment.

Plus récemment, et à la frontière avec la logique mathématique : la correspondance de Curry-Howard a jeté un pont entre le monde des démonstrations formelles et celui des programmes.

Citons aussi l'étude de la mécanisation des procédés de calcul et de pensée qui a permis de mieux comprendre la réflexion humaine, et apporté des éclairages en psychologie cognitive et en linguistique, par exemple, à travers la discipline du traitement automatique du langage naturel.

Le terme "technologies de l'information et de la communication" désigne un secteur d'activité et un ensemble de biens qui sont des applications pratiques des connaissances scientifiques en "informatique" ainsi qu'en électronique numérique, en télécommunication, en sciences de l'information et de la communication et en cryptologie.


Les appareils en électronique numérique utilisent tous un système logique. Les entrées et sorties des composants électroniques n'ont que deux états ; l'un correspondant à "vrai", l'autre à "faux". On démontre qu'en assimilant "vrai" au nombre 1 et "faux" au nombre 0, on peut établir les règles logiques qui fondent un système de numération binaire. Les appareils représentent toute l'information sous cette forme.

Les appareils informatiques se décomposent en quatre ensembles qui servent respectivement à entrer des données, les stocker, les traiter, puis, les faire ressortir de l'appareil, selon les principes de la machine de Turing et l'architecture de von Neumann. Les données circulent entre les pièces des différentes unités par des lignes de communication, les bus. Le processeur est la pièce centrale qui anime l'appareil en suivant les instructions des programmes qui sont enregistrés à l'intérieur.

Il existe aujourd'hui une gamme étendue d'appareils capables de traiter automatiquement des informations. De ces appareils, l'ordinateur est le plus connu, le plus ouvert, le plus complexe et un des plus anciens. L'ordinateur est une machine modulable et universelle qui peut être adaptée à de nombreuses tâches par ajout de matériel ou de logiciel.

Un système embarqué est un appareil équipé de matériel et de logiciel informatique, et affecté à une tâche bien précise.

Exemples d'appareils :

L'ensemble des composants électroniques, nécessaires au fonctionnement des appareils numériques, est appelé « en anglais ». Dans un boîtier se trouvent les pièces centrales, par exemple, le processeur et des pièces périphériques servant à l'acquisition, au stockage, à la restitution et la transmission d'informations. L'appareil est un assemblage de pièces qui peuvent être de différentes marques. Le respect des normes industrielles par les différents fabricants assure le fonctionnement de l'ensemble.

La carte mère est un circuit imprimé avec de nombreux composants et ports de connexion constituant le support principal des éléments essentiels d'un ordinateur (Supports des microprocesseur, mémoires, connecteurs divers et autres ports d'entrée-sortie).

L'intérieur du boîtier d'un appareil informatique contient un ou plusieurs circuits imprimés sur lesquels sont soudés des composants électroniques et des connecteurs. La carte mère est le circuit imprimé central, sur lequel sont connectés tous les autres équipements.

Un bus est un ensemble de lignes de communication qui servent aux échanges d'information entre les composants de l'appareil informatique. Les informations sont transmises sous forme de signaux électriques. Le plus petit élément d'information manipulable en informatique correspond à un bit. Les bus transfèrent des bytes d’informations composés de plusieurs bits en parallèle.

Les périphériques sont par définition, les équipements situés à l'extérieur du boîtier.

Les périphériques d'entrée servent à commander l'appareil informatique ou à y envoyer des informations.

L'envoi des informations se fait par le procédé de numérisation. Il s'agit de transformer des informations brutes (une page d'un livre, les listes des éléments périodiques, etc.) en suite de nombres binaires pouvant être manipulées par un appareil informatique. La transformation est faite par un circuit électronique. La construction du circuit diffère en fonction de la nature de l'information à numériser.

L'ensemble des dispositifs de commande et les périphériques de sortie directement associés forment une façade de commande appelée interface homme-machine.

Une mémoire est un dispositif électronique (circuit intégré) ou électromécanique destiné à conserver des informations dans un appareil informatique.

Un processeur est un composant électronique qui exécute des instructions. Un appareil informatique contient un processeur, voire deux, quatre, ou plus. Les ordinateurs géants contiennent des centaines jusqu'à des milliers de processeurs.

L'acronyme CPU (pour l'anglais ) désigne le ou les processeurs centraux de l'appareil. L'exécution des instructions par le ou les CPU influence tout le déroulement des traitements.

Un microprocesseur multi-cœur réunit plusieurs circuits intégrés de processeur dans un seul boîtier. Un composant électronique construit de cette manière effectue le même travail que plusieurs processeurs.

Les équipements de sortie servent à présenter les informations provenant d'un appareil informatique sous une forme reconnaissable par un humain.

Les équipements de réseau servent à la communication d'informations entre des appareils informatiques, en particulier, à l'envoi d'informations, à la réception, à la retransmission, et au filtrage. Les communications peuvent se faire par câble, par onde radio, par satellite, ou par fibre optique.

Un protocole de communication est une norme industrielle relative à la communication d'informations. La norme établit autant le point de vue électronique (tensions, fréquences) que le point de vue informationnel (choix des informations, format), ainsi que le déroulement des opérations de communication (qui initie la communication, comment réagit le correspondant, combien de temps dure la communication, etc.). Selon le modèle OSI – qui comporte sept niveaux –, une norme industrielle (en particulier un protocole de communication) d'un niveau donné, peut être combinée avec n'importe quelle norme industrielle d'une couche située en dessus ou en dessous.

Une carte réseau est un circuit imprimé qui sert à recevoir et envoyer des informations conformément à un ou plusieurs protocoles.

Un modem est un équipement qui sert à envoyer des informations sous forme d'un signal électrique modulé, ce qui permet de les faire passer sur une ligne de communication analogique telle une ligne téléphonique.

Un "logiciel" est un ensemble d'informations relatives à un traitement automatisé, qui correspond à la d'une Machine de Turing. La mécanique de cette machine correspondant au processeur. Le logiciel peut être composé d'instructions et de données. Les instructions mettent en application les algorithmes en rapport avec le traitement d'information voulu. Les données incluses dans un logiciel sont les informations relatives à ce traitement ou exigées par lui (valeurs clés, textes, images, etc.).

Le logiciel peut prendre une forme exécutable (c'est-à-dire, directement compréhensible par le micro-processeur) ou source, c'est-à-dire que la représentation est composée d'une suite d'instructions directement compréhensible par un individu. Ainsi donc, on peut considérer le logiciel comme une abstraction qui peut prendre une multitude de formes : il peut être imprimé sur du papier, conservé sous forme de fichiers informatiques ou encore stocké dans une mémoire (une disquette, une clé USB).

Un appareil informatique peut contenir de très nombreux logiciels, organisés en trois catégories :



Un "logiciel embarqué", un "logiciel libre", un "logiciel propriétaire" font référence à une manière de distribuer le logiciel. Voir « distribution de logiciels ».

Lire en ligne : "IEEE Computer Society - Keywords".

Un logiciel applicatif ou application informatique contient les instructions et les informations relatives à une "activité" automatisée par un appareil informatique ("informatisée"). Il peut s'agir d'une activité de "production" (exemple : activité professionnelle), de recherche, ou de loisir.

Un logiciel système contient les instructions et les informations relatives à des opérations de routine susceptibles d'être exécutées par plusieurs logiciels applicatifs. Un logiciel système sert à fédérer, unifier et aussi simplifier les traitements d'un logiciel applicatif. Les logiciels systèmes contiennent souvent des bibliothèques logicielles.

Lorsqu'un logiciel applicatif doit effectuer une opération de routine, celui-ci fait appel au logiciel système par un mécanisme appelé appel système. La façade formée par l'ensemble des appels systèmes auquel un logiciel système peut répondre est appelée Interface de programmation ou API (acronyme de l'anglais ).

Un logiciel applicatif effectue typiquement un grand nombre d'appels système, et par conséquent, il peut fonctionner uniquement avec un système d'exploitation dont l'interface de programmation correspond. Le logiciel est alors dit "compatible" avec ce système d'exploitation, et inversement.

Le système d'exploitation est un logiciel système qui contient l'ensemble des instructions et des informations relatives à l’"utilisation commune" du matériel informatique par les logiciels applicatifs.

Les traitements effectués par le système d'exploitation incluent : répartition du temps d'utilisation du processeur par les différents logiciels (multitâche), répartition des informations en mémoire vive et en mémoire de masse. En mémoire de masse, les informations sont groupées sous formes d'unités logiques appelées fichiers.

Les traitements effectués par le système d'exploitation incluent également les mécanismes de protection contre l'utilisation simultanée par plusieurs logiciels applicatifs d'équipements de matériel informatique qui par nature "ne peuvent pas" être utilisés de manière partagée (voir Exclusion mutuelle).

POSIX est une norme industrielle d'une interface de programmation qui est appliquée dans de nombreux systèmes d'exploitation, notamment la famille UNIX.

L’environnement graphique est le logiciel système qui organise automatiquement l'utilisation de la surface de l'écran par les différents logiciels applicatifs et redirige les informations provenant des dispositifs de pointage (souris). L'environnement graphique est souvent partie intégrante du système d'exploitation.

Une base de données est un stock structuré d'informations enregistré dans un dispositif informatique.

Un système de gestion de base de données (sigle : SGBD) est un logiciel système dont les traitements consistent à l'organisation du stockage d'informations dans une ou plusieurs bases de données. Les informations sont disposées de manière à pouvoir être facilement modifiées, triées, classées, ou supprimées. Les automatismes du SGBD incluent également des protections contre l'introduction d'informations incorrectes, contradictoires ou dépassées.


Le micrologiciel est souvent distribué sur une puce de mémoire morte faisant partie intégrante du matériel en question. Il peut être mis à jour soit en changeant la ROM ou pour les systèmes les plus récents en réécrivant la mémoire flash.

Le traitement de l'information s'applique à tous les domaines d'activité et ceux-ci peuvent se trouver associés au mot « informatique », comme dans « informatique médicale », où les outils informatiques sont utilisés dans l'aide au diagnostic (ce champ d'activité se rapportera plutôt à l'informatique scientifique décrite ci-dessous), ou dans « informatique bancaire », désignant des systèmes d'information bancaire qui relèvent plutôt de l'informatique de gestion, de la conception et de l'implantation de produits financiers qui relèvent plutôt de l'informatique scientifique et des mathématiques, ou encore de l'automatisation des salles de marché qui en partie relève de l'informatique temps réel.

Les grands domaines d'utilisation de l'informatique sont :


Les différents domaines d'utilisation de l'informatique sont les suivants :

L'informatique est un secteur d'activité scientifique et industriel important aux États-Unis, en Europe et au Japon. Les produits et services de cette activité s'échangent dans le monde entier. Les produits immatériels tels que les connaissances, les normes, les logiciels ou les langages de programmation circulent très rapidement par l'intermédiaire des réseaux informatiques et de la presse spécialisée, et sont suivis par les groupes de veille technologique des entreprises et des institutions. Les matériels informatiques peuvent être conçus sur un continent et construits sur un autre.

L'anglais international est la langue véhiculaire du secteur d'activité. Il est enseigné dans les écoles. C'est la langue des publications scientifiques ainsi que de nombreux ouvrages techniques. La grande majorité des langages de programmation utilisent le vocabulaire anglais comme base. Les termes peuvent provenir des instituts de recherche, des entreprises, ou des organismes de normalisation du secteur. De nombreux néologismes sont des abréviations ou des mots-valise basés sur des mots en anglais. Le grand nombre d'anglicismes reflète la domination actuelle des États-Unis sur ce marché.

L'usage d'abréviations joue le même rôle que celui des formules chimiques : l'ébauche d'une nomenclature internationale qui facilite l'accès des lecteurs non anglophones à la littérature informatique. Il existe en outre, un phénomène d'emprunt lexical réciproque entre les langages de programmation – dont le lexique est basé sur l'anglais – et le jargon informatique.

On trouve dans le monde environ un milliard de micro-ordinateurs, trois cent mille stations de travail, quelques dizaines de milliers de , et deux mille superordinateurs en état de marche.

On ne connaît pas avec certitude la part de marché occupée par l'industrie des systèmes embarqués, mais on estime que l'informatique représente le tiers du coût d'un avion ou d'une voiture.

La distribution des produits informatiques est faite sous la forme de multiples canaux de distribution, parmi lesquels on compte la vente directe, le e-commerce, les chaînes de revendeurs, les groupements de revendeurs, la vente par correspondance.

Les grossistes informatiques ont un rôle clef dans la distribution informatique et sont un point de passage quasi obligé pour les sociétés qui ont choisi la vente indirecte (par un réseau de revendeurs). Les grossistes, qu'ils soient généralistes ou spécialisés, adressent la multitude de petits points de vente ou les sociétés de service pour lesquelles l'activité de négoce représente un volume d'activité faible.

Aujourd'hui la plupart des constructeurs sont spécialisés soit dans le matériel, soit dans le logiciel, soit dans les services.
Apple et Oracle (Sun) sont parmi les seuls constructeurs spécialisés à la fois dans le matériel et le logiciel. IBM et HP sont parmi les seuls constructeurs spécialisés à la fois dans le matériel et les services.

Dans le sultanat d'Oman entre 2002 et 2005, 16 % des ventes concernaient du logiciel, 30 % concernait des ordinateurs, 28 % concernait des services, et 25 % concernait des équipements de transmission.

En Autriche, en 2007, 21 % des ventes concernent le logiciel, 34 % concernent le matériel, et 45 % concernent des services.

Historiquement, le matériel informatique était distribué par les grands constructeurs qui traitaient en direct avec leurs clients ; la plupart de ceux-ci étant de grandes entreprises ou des organismes publics. Les logiciels étaient créés par les clients. Les constructeurs fournissaient uniquement un système d'exploitation, et assistaient leurs clients par l'organisation de cours de programmation à la formation des analystes programmeurs. Au fur et à mesure de la baisse des prix des systèmes, le marché s'est élargi, obligeant plusieurs constructeurs à se structurer pour mieux diffuser leur produit et à s'appuyer sur des partenaires.

Ces partenaires étaient au départ mono-marque et travaillaient souvent sous la forme d'agent semi-exclusif, puis ils se sont transformés au fil du temps en revendeurs indépendants multi-marques.

Dans les années 1980, en même temps que les premiers micro-ordinateurs, sont apparus les premiers éditeurs spécialisés dans le logiciel.

Depuis 1987, le marché du micro-ordinateur est le principal secteur du marché informatique, et les micro-ordinateurs, initialement utilisés à des fins domestiques, sont désormais largement utilisés dans les entreprises et les institutions, où ils tendent à remplacer les stations de travail et les .

Du fait de la croissance très rapide du marché, vecteur de forte concurrence, de nombreuses sociétés ont disparu dans les années 1980. Des quatorze grands fabricants de l'époque, en 1997 il n'en reste plus que deux (Intel et AMD).

L'ordinateur est un appareil modulable, construit par assemblage de composants de différentes marques.

Le développement et la construction des composants est le fait de quelques marques très spécialisées. La majorité des constructeurs d'ordinateurs sont des assembleurs : un "assembleur" est une société qui vend des ordinateurs construits par assemblage de composants provenant d'autres marques, y compris de concurrents.

En 1965, Gordon Earle Moore, cofondateur d'Intel, un grand fabricant de microprocesseurs, émettait la Loi de Moore. Cette loi, basée sur l'observation, prédit que la complexité des microprocesseurs devrait doubler tous les deux ans. Quarante ans plus tard, cette observation se confirme toujours. Selon le magazine "Ligne de crédit", l'alignement à la Loi de Moore n'est pas le fait du hasard, mais une volonté de l'industrie informatique.

Le matériel informatique est aujourd'hui produit par diverses multinationales, majoritairement du Japon et de Taïwan. Exemples :

En Autriche par exemple, les principales marques d'ordinateur sont, en 2007 : Hewlett-Packard (Palo Alto, États-Unis), Dell, (Round Rock, États-Unis), Fujitsu (Japon), Siemens (Berlin, Allemagne), Sony (Tokyo, Japon) et Acer (Taïwan).

Les principales marques de consoles de jeux sont en 2007 : Sony (Tokyo, Japon), Nintendo (Kyoto, Japon), et Microsoft (Redmond, États-Unis).

La fabrication d'un logiciel (développement) demande très peu de moyens techniques, et par contre beaucoup de temps et de savoir-faire.

Il existe aujourd'hui un très grand nombre d'auteurs de logiciels, il peut s'agir de multinationales comme Microsoft, de petites entreprises locales, voire de particuliers ou de bénévoles.

Les grosses entreprises, utilisant du matériel informatique pour leurs propres besoins, ont souvent des équipes spécialisées, qui créent des logiciels sur mesure pour les besoins de l'entreprise. Ces logiciels ne seront jamais mis sur le marché. Un progiciel est un logiciel prêt-à-porter et générique prévu pour répondre à un besoin ordinaire. Par opposition à un logiciel spécifique, qui est développé sur mesure en vue de répondre au besoin d'un client en particulier. La création de logiciels spécifique est le principal sujet de contrats de services des entreprises informatiques.

Dans des secteurs industriels comme l'aviation, des équipes créent des logiciels pour les "systèmes embarqués" de ce secteur. Ces logiciels ne sont jamais mis sur le marché séparément.

Un logiciel étant un ensemble d'informations, il peut être transmis par les moyens de télécommunications. Le téléchargement est l'opération qui consiste à utiliser un réseau de télécommunication pour récupérer un logiciel en provenance d'un autre appareil. Le e-commerce est l'activité qui consiste à vendre des logiciels (ou d'autres biens) en les distribuant par des réseaux de télécommunication comme Internet.

On peut distinguer quatre grands types de logiciels : libres, propriétaires, , , en fonction du type de contrat de licence qui régit leur distribution, utilisation et copie.

Il existe aujourd'hui une offre très large de logiciels, de tous les types : libres, propriétaires, et .

L'industrie du logiciel est un des principaux secteurs économiques en Europe et aux États-Unis. De nombreux constructeurs de logiciels sont aux États-Unis. La création de logiciels applicatifs représente 52 % de l'activité.

Si le Japon est un des pays les mieux équipés en matériel informatique, on y trouve les plus grands fabricants de matériel, il n'en va pas de même pour le logiciel, et de nombreux logiciels posent des problèmes pour l'écriture de textes en utilisant l'alphabet japonais.

Il existe en 2008 environ quatre-vingts systèmes d'exploitation différents. Le marché est largement occupé par la famille Windows : cette famille de systèmes d'exploitation, propriété de Microsoft (Redmond, États-Unis) occupe environ 90 % du marché des systèmes d'exploitation pour "ordinateurs personnels". La société Microsoft a fait l'objet de divers procès pour monopolisation du marché.
En 2019, le marché des smartphones, tablettes et objet connectés a fortement évolué et utilise très majoritairement le système Androïd dévelloppé par Google 

"GNU" est un projet de système d'exploitation lancé en 1985, entièrement basé sur des produits . Linux est un système d'exploitation , écrit par une équipe de plus de bénévoles. La valeur de revente de Linux est estimée à plus de de dollars.

L'offre en logiciels libres consiste notamment en des ensembles qui contiennent à la fois des produits GNU et Linux. Ils sont distribués avec des magazines, ou mis à disposition pour le téléchargement.

Aujourd'hui la majorité des téléphones portable sont basés sur des systèmes d'exploitation libres : OS X a été développé à partir de Free BSD, Android est quant à lui basé sur un système Linux classique. Ce qui fait des systèmes Open Source Linux et Free BSD les systèmes les plus répandus sur le marché du téléphone portable.

La Contrefaçon numérique consiste à utiliser ou à mettre à disposition tout ou partie d'un logiciel alors que sa licence ne l'autorise pas, les éditeurs logiciel parlent volontiers de pirates pour désigner les auteurs voir, les utilisateurs de ces contrefaçons.

La licence d'utilisation s'apparente à un contrat (dont la valeur juridique varie selon les pays) accepté implicitement par tout acheteur d'un logiciel (ou explicitement lors de l'installation ou du premier lancement de celui-ci).

Par une licence propriétaire, l'éditeur octroie le droit, généralement exclusif et non transmissible, à l'acheteur d'utiliser le logiciel. Si une copie de ce logiciel est mise à disposition d'autrui, l'utilisation par autrui est alors une violation des clauses du contrat de licence et la mise à disposition est considérée comme un acte de contrefaçon.

La vente de licences d'utilisation est la première source de revenus de nombreux éditeurs logiciels et la copie voir la diffusion illégale représente pour eux un important manque à gagner. La contrefaçon touche le marché du logiciel comme les marchés d'autres biens immatériels tels que la musique ou la vidéo.

Les éditeurs vendent souvent leur logiciel accompagné de services tels que garantie et mises à jour, des services qui ne sont, la plupart du temps, disponibles que sur les logiciels légalement utilisés.

Le nombre de copies de logiciels vendues par des contrefacteurs est plus ou moins élevé selon les pays. Selon la Business Software Alliance, en Algérie 85 % des logiciels vendus en 2008 seraient issus du piratage. Toujours selon la Business Software Alliance, au Luxembourg, ce taux aurait été de 21 % en 2007, ce qui serait le taux le plus bas du monde.

Le passage d'un marché industriel de produits à un marché des services est relativement récent et en forte progression. Le commerce de services consiste principalement en la vente et l'exécution de mandats concernant des modifications sur des systèmes d'information d'entreprises ou de collectivités.

Les systèmes d'information des entreprises sont parfois composés de centaines d'ordinateurs, sur lesquels sont exécutés des centaines de logiciels de manière simultanée. Il existe de nombreux liens entre les différents logiciels et les différents ordinateurs, et le simple fait d'arrêter un seul des éléments risque de déranger des milliers d'usagers, voire de provoquer le chômage technique de l'entreprise.

Selon le cabinet Gartner Dataquest, les services informatiques ont généré dans le monde en 2006. Soit un marché en augmentation de 6,4 % par rapport à 2005.

Un consultant est une personne chargée d'une mission de services.


De nombreuses SSII se trouvent aux États-Unis et en Inde. Parmi les leaders du marché on trouve IBM – la plus ancienne société d'informatique encore en activité –, ainsi que EDS, Accenture et Hewlett-Packard, toutes originaires des États-Unis.

Les principaux sujets des mandats sont la création de logiciels sur mesure, la mise en place de progiciels et la modification des fichiers de configuration en fonction des besoins, des opérations de réglage, d'expertise et de surveillance du système informatique. En France la majorité des constructeurs de logiciels sont des SSII.


L'informaticien est d'une manière générale une personne qui travaille dans le secteur de l'informatique. Il existe dans ce secteur diverses activités qui sont orientées vers la création de logiciels ou la maintenance d'un système informatique – matériel et logiciels.

Le secteur dépend également des activités des fabricants de semi-conducteurs et de pièces détachées, des assembleurs, ainsi que des fournisseurs de télécommunications et des services d'assistance.

La maintenance d'un système informatique consiste à la préparation d'ordinateurs tels que serveurs, ordinateurs personnels, ainsi que la pose d'imprimantes, de routeurs ou d'autres appareils. L'activité consiste également au dépannage des machines, à l'adaptation de leur configuration, l'installation de logiciels tels que systèmes d'exploitation, systèmes de gestion de base de données ou logiciels applicatifs, ainsi que divers travaux de prévention des pannes, des pertes ou des fuites d'informations telles que l'attribution de droits d'accès ou la création régulière de copies de sauvegarde ("" en anglais).

Le directeur informatique décide des évolutions du système informatique dans les grandes lignes, conformément à la politique d'évolution de la société qui l'emploie. Il sert d'intermédiaire entre les fournisseurs et les clients (employés de l'entreprise), ainsi que la direction générale. Il propose des budgets, des évolutions, puis mandate des fournisseurs pour des travaux.

L'ingénieur système travaille à la mise en place et l'entretien du système informatique : la pose de matériel informatique, l'installation de logiciels tels que systèmes d'exploitation, systèmes de gestion de base de données ou logiciels applicatifs, et le réglage des paramètres de configuration des logiciels.

L'administrateur de bases de données est chargé de la disponibilité des informations contenues dans des bases de données et la bonne utilisation des systèmes de gestion de base de données – les logiciels qui mettent à disposition les informations et qui occupent une place stratégique dans de nombreuses entreprises. Il s'occupe des travaux de construction, d'organisation et de transformation des bases de données, ainsi que du réglage des paramètres de configuration du système de gestion de base de données et de l'attribution de droit d'accès sur le contenu des bases de données.

Le responsable d'exploitation veille à la disponibilité constante du système informatique. Il effectue des tâches de sauvegarde régulière en vue de prévenir la perte irrémédiable d'informations, organise les travaux de transformation du système informatique en vue de limiter la durée des mises hors service et attribue des droits d'accès en vue de limiter les possibilités de manipulation du système informatique au strict nécessaire pour chaque usager – ceci en vue de prévenir des pertes ou des fuites d'information.

Le développement de logiciels consiste à la création de nouveaux logiciels ainsi que la transformation et la correction de logiciels existants. En font partie la définition d'un cahier des charges pour le futur logiciel, l'écriture du logiciel dans un ou l'autre langage de programmation, le contrôle du logiciel créé, la planification et l'estimation du budget des travaux.

Dans une équipe d'ingénieurs, le chef de projet est chargé d'estimer la durée des travaux, d'établir un planning, de distribuer les tâches entre les différents membres de l'équipe, puis de veiller à l'avancée des travaux, au respect du planning et du cahier des charges. Le chef de projet participe également à la mise en place du logiciel chez le client et récolte les avis des usagers.

L'analyste-programmeur est chargé d'examiner le cahier des charges du futur logiciel, de déterminer la liste de toutes les tâches de programmation nécessaire pour mettre en œuvre le logiciel. Il est chargé de déterminer les automatismes les mieux appropriés en fonction du cahier des charges et des possibilités existantes sur le système informatique. L'analyste-programmeur est ensuite chargé d'effectuer les modifications nécessaires dans le logiciel, de rédiger ou de modifier le code source du logiciel et de vérifier son bon fonctionnement.

L'architecte des systèmes d'informations est chargé de déterminer, d'organiser et de cartographier les grandes lignes de systèmes informatiques ou de logiciels. Il réalise des plans d'ensemble, détermine les composants (logiciel et matériel) principaux de l'ensemble, ainsi que les flux d'informations entre ces composants. Lors de la création de nouveaux logiciels il est chargé de découper le futur logiciel en composants, puis d'organiser et de cartographier le logiciel et les produits connexes.

Les entreprises et les institutions qui ont un système informatique de grande ampleur ont souvent une équipe d'informaticiens qui travaillent à la maintenance du système ainsi qu'à la création de logiciels pour le compte de l'entreprise. Cette équipe, dirigée par le directeur informatique peut faire appel à des éditeurs de logiciel ou des sociétés de services en ingénierie informatique (abréviation SSII) pour certains travaux. Par exemple, lorsque l'équipe interne est trop peu nombreuse ou ne possède pas les connaissances nécessaires. Les entreprises peuvent également faire appel à des consultants – des employés d'une société tierce – pour prêter main-forte ou conseiller leur équipe sur un sujet précis.

L'infogérance consiste à déléguer toute la maintenance du système d'information à une société de services. Ces services sont parfois réalisés "offshore" : des équipes délocalisées (parfois situées dans un pays lointain) pilotent les ordinateurs à travers les réseaux informatiques (télémaintenance).

L'intégration verticale consiste pour une société informatique à non seulement créer un logiciel, mais également travailler sur des opérations antérieures et postérieures au développement du logiciel en question, tels que le management du système d'information, l'aide à la décision de la direction des systèmes d'information, les opérations de migration ou les services d'assistance.

En cloud computing, un site informatique - matériel, logiciel et raccordements réseau - appartenant à un fournisseur, est mis à disposition des consommateurs en self-service payé à l'usage. Selon le service offert, la responsabilité du système d'exploitation, des logiciels moteurs et des logiciels applicatifs incombe soit au fournisseur soit au consommateur.

On applique souvent l'adjectif « virtuel » ou « immatériel » aux produits de l'informatique, ce qui pourrait laisser croire que l'informatique est peu consommatrice de ressources naturelles. Jean-Marc Jancovici montre que la dématérialisation, souvent présentée comme une solution pour le développement durable de l'économie, ne s'est pas accompagnée d'une diminution des flux physiques par rapport aux flux d'information. En pratique, dans les années 2010, les directions des systèmes d'information sont généralement tenues à l'écart des programmes de développement durable des entreprises.

On se rend compte aujourd'hui, avec les premières études des experts en informatique verte (TIC durables), que l'informatique serait directement à l'origine de 5 % des émissions de gaz à effet de serre de la France. L'informatique générerait également une forte consommation d'électricité. Mais les impacts environnementaux sont surtout concentrés lors de la fabrication des équipements et leur fin de vie. Les principaux impacts sont l'épuisement des ressources naturelles non renouvelables et les pollutions (eau, air, sol) qui dégradent les écosystèmes.

L'application des principes de développement durable à l'informatique donne naissance aux TIC durables. Elle englobe les trois piliers du développement durable (environnement, social, économique) et se caractérise par une double démarche (souvent menée en parallèle) :


À terme, le développement durable devrait faire évoluer les modèles employés en informatique. Il est, en effet, nécessaire d'expliciter la sémantique des données, documents ou modèles, ce qui relève de la branche de l'informatique appelée représentation des connaissances. Plusieurs projets en écoinformatique se déroulent dans le cadre d'initiatives telles que le web sémantique.



