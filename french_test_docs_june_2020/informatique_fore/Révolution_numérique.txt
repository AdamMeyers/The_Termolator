Révolution numérique

La « révolution numérique » (ou plus rarement « révolution technologique », « révolution Internet » ou « révolution digitale ») le bouleversement profond des sociétés provoqué par l'essor des techniques numériques telles que l'informatique et le développement du réseau Internet.

Cette mutation se traduit par une potentielle mise en réseau planétaire des individus au travers de nouvelles formes de communication telles que le courriel, les réseaux sociaux, la messagerie instantanée, les blogs et autres sites web privés et publics, commerciaux ou non. Sur le Web, la révolution numérique s'accompagne d'une originelle décentralisation dans la circulation des idées qui devient de plus en plus .

La révolution numérique se caractérise aussi par le développement de l'intelligence artificielle et l'essor du domaine de la robotique et est perçue par certains intellectuels et militants comme un ensemble de faits et une construction idéologique que l'on peut remettre en cause.

En France, la plus ancienne occurrence connue de l'expression « révolution numérique » remonte au numéro spécial de la revue "Sciences et avenir" (), du .
L'expression « révolution numérique » est entrée non seulement dans le langage usuel, mais aussi dans celui des sciences humaines et l'abondante littérature reprenant cette expression peut donner l'impression d'une conception non seulement consensuelle mais unique du sujet : la « révolution numérique » serait un "fait établi", institué, et rien d'autre.

Pour autant, les analyses de l'historien François Jarrige démontrent l'émergence discrète, car progressive (s'élaborant au fil du ), d'un mouvement de pensée "technocritique" et le fait que l'association des mots « révolution » et « numérique » pose des questions d'ordre philosophique, dans un contexte qui dépasse donc largement l'avènement de l'informatique, qui porte sur le progrès technique et plus largement encore sur le concept même de progrès. Selon la mouvance technocritique, « la révolution numérique » ne pourrait pas se définir uniquement comme un ensemble de faits (l'invention de l'ordinateur, l'apparition d'internet, etc.), mais aussi comme une construction idéologique, au même titre que « le progrès ».

C'est donc parce que l'attention se focalise sur les "faits" que l'expression « révolution numérique » est répandue, mais c'est aussi parce qu'elle n'est généralement pas (ou peu souvent) identifiée comme "construction idéologique" qu'elle est ambigüe.


Comme la « révolution industrielle », deux siècles plus tôt, la « révolution numérique » est provoquée par une évolution sensible, très marquée, des techniques. Elle est en effet directement associée à la naissance puis au développement de l'informatique, c'est-à-dire au "fait" que toute information (caractère d'imprimerie, son, forme, couleur, puis mot, texte, photographie, film, musique, etc.) peut être numérisée, c'est-à-dire s'exprimer par une combinaison de chiffres (en l'occurrence des 0 et des 1), puis stockée, modifiée, éditée (sur des sites ou des blogs) et transmise (par mails, sur des forums, etc.) au moyen de toutes sortes d'appareils comme des ordinateurs, des tablettes ou des smartphones. Ces premiers étant, depuis les années 1960, équipés de circuits intégrés, qui sont de taille réduite et peu consommateurs en énergie, ils permettent à des millions d'individus d'effectuer de façon de plus en plus automatique des tâches sans cesse plus nombreuses, complexes et dans des délais de plus en plus courts, au point qu'ils sont qualifiés comme dotés d'intelligence artificielle.

Selon Marcello Vitali-Rosati et Michaël E. Sinatra, la « révolution numérique » engage à une « réinterprétation des structures conceptuelles à travers lesquelles l’homme se rapporte au monde et, surtout, structure et organise sa connaissance. » Du fait qu'elle marque l'entrée dans un nouveau paradigme de la connaissance, la numérisation des informations est souvent comparée à l'invention de l'imprimerie.

Dans l'histoire de ce processus, trois tournants sont habituellement distingués :
Ces innovations permettant aux échanges de s'opérer sous une forme électronique, les barrières géographiques et culturelles cessent d'être aussi contraignantes que par le passé. Cette mutation bouleverse l'ensemble des règles géopolitiques mondiales (mondialisation), l'économie planétaire (avènement de la Nouvelle économie) et, plus radicalement, la façon dont les individus perçoivent le monde, se comportent avec autrui et se considèrent eux-mêmes.

Le sociologue Jacques Ellul, à la fois spécialiste de la révolution et penseur technocritique, affirme que, dès lors que la technique prend une part croissante dans l'histoire de l'humanité, la révolution n'est plus qu'un mythe :

L'expression « révolution numérique » accole deux termes d'origines très différentes : le mot « révolution » définit l'idée d'une émancipation des individus au prix d'un bouleversement politique organisé et théorisé mais se déroulant de façon rapide et mouvementée, voire violente ; le mot « numérique » renvoie quant à lui à une "évolution" de la technique (les informations se transmettent non plus par des signaux analogiques mais précisément par des signaux numériques) mais cette mutation s'opère "progressivement", au fil des avancées scientifiques et techniques, et sans concertation préalable avec le milieu politique.

Or c'est au moment où l'on parle beaucoup de « révolution numérique » que l'économie mondiale est façonnée par les GAFAM (Google, Apple, Facebook, Amazon, Microsoft... mais aussi Twitter, LinkedIn, Netflix, Airbnb, Uber, etc.), les « géants du Web » dont les chiffres d'affaires dépassent les PIB de nombreuses nations. Cette prégnance de l'économie sur la sphère politique rend l'expression « révolution numérique » problématique, voire ambigüe. Selon le journaliste Xavier de La Porte ironise alors sur les personnalités politiques qui ne cessent d'utiliser l'expression :
L'expression « révolution numérique » désigne donc "à la fois" un ensemble de faits réels et une idéologie.



Il est nécessaire de saisir, dans un premier temps, la genèse de l'expression « révolution numérique » : quels sont notamment les liens entre « la révolution industrielle » et « la révolution numérique », phénomènes fréquemment comparés ?

On analysera ensuite les raisons pour lesquelles cette expression s'est imposée dans le langage usuel et pourquoi elle est davantage interprétée comme un fait que comme une construction idéologique.

Les premiers ordinateurs étaient de simples machines à calculer : les informations qu'ils avaient à traiter étaient exclusivement des "nombres". Comprendre l'histoire du numérique nécessite donc de saisir l'histoire du calcul.
Très tôt, les humains ont conçu et fabriqué des outils les aidant à calculer (abaque, boulier...). Mais c'est à partir du qu'ils ne cessent de les perfectionner, quand s'amorce (en Angleterre puis en France) la Révolution industrielle. Alors que la société s'était bâtie sur une économie à dominante agraire et artisanale, elle s'urbanise de façon croissante, devenant de plus en plus commerciale et industrielle. Dans le but de rendre la production toujours plus efficace, les machines sont conçues et fabriquées à un rythme exponentiel. Au fur et à mesure que la société se mécanise, émerge l'idée selon laquelle la machine ne doit pas seulement aider les hommes, mais aussi, autant que possible, les remplacer. Le goût pour les automates, qui se développe à cette époque, traduit un désir plus ou moins conscient : celui que "toutes" les étapes d'un processus de production (conception, fabrication, maintenance, commercialisation, etc.) soient prises en charge par une « machinerie intelligente », c'est-à-dire habilitée à traiter un maximum d'information automatiquement et à la place de l'homme. Il est donc d'usage de considérer « la révolution numérique » comme le prolongement logique de la révolution industrielle.

L'esprit des télécommunications s'institutionnalise. En 1603, en France, le roi Henri IV fait créer un corps de courriers (estafettes) chargé de transporter les correspondances aussi bien administratives que privées : c'est la naissance officielle de « la poste », administration détenant le monopole de ce service. En 1612, est mis en place un service de diligences transportant à la fois du courrier, des paquets et des voyageurs.

Dans la deuxième moitié du siècle, deux philosophes, l'Allemand Gottfried Wilhelm Leibniz et l'Anglais Thomas Hobbes, émettent l'hypothèse que la pensée peut se formuler de façon systématique par le biais d'un langage mathématique. Le premier imagine un langage assimilant l'argumentation à un calcul, afin qu'« il n'y ait pas plus de besoin de se disputer entre deux philosophes qu'entre deux comptables ». Selon Hobbes, « la raison n'est rien d'autre que le fait de calculer » ». Mais c'est un autre philosophe, le Français Blaise Pascal, qui entreprend de concrétiser ces principes en inventant la pascaline dès 1642 : la toute première machine à calculer dont le fonctionnement permet de traiter un algorithme.

Durant la première moitié du siècle émergent des inventions qui relèvent de l'automation et qui annoncent ce qui deviendra plus tard l'informatique : en 1728, dans le but d'automatiser le fonctionnement des métiers à tisser, le français Jean-Baptiste Falcon invente le système de la carte perforée : morceau de papier rigide contenant des informations représentées par la présence ou l'absence de trou dans une position donnée. En 1735, pour les plaisirs de la Cour, Jacques Vaucanson construit son premier automate, le flûteur automate. Puis, en 1744, il en construit un autre plus sophistiqué et qui fait forte impression sur le public : le canard digérateur. Nommé inspecteur général des manufactures de soie et chargé de réorganiser cette industrie, il perfectionne le métier à tisser de Falcon en l'automatisant par hydraulique, la commande étant assurée par des cylindres analogues à ceux de ses automates.

Durant la seconde moitié du siècle, en Grande-Bretagne, la machine à vapeur, mise au point par l'ingénieur écossais James Watt, et le réseau ferré transforment peu à peu les structures économiques et sociales du pays. Lewis Mumford voit dans la révolution industrielle la préfiguration de la « révolution numérique ».

En France, les principaux acteurs de cette mutation sont issus de la bourgeoisie, une nouvelle classe sociale qui « détrône » l'aristocratie. Pleinement conscients et désireux de fonder une civilisation moderne, « éclairée », ils consignent par écrit l'ensemble de toutes les innovations scientifiques et techniques. Éditée de 1751 à 1772, l'"Encyclopédie" de Diderot et d'Alembert (ou "Dictionnaire raisonné des sciences, des arts et des métiers") promeut l'universalisme, lequel préfigure les notions de réseau et de village global aujourd'hui associées à l'idée de « révolution numérique ».
La Grande-Bretagne affirme sa suprématie sur le reste du monde : 500 pompes à vapeur y fonctionnent en 1800, la première locomotive y circulant en 1803. Dans le premier quart du siècle, l'électricité reste une curiosité de laboratoire (pile inventée en 1801) en regard du développement de l'énergie thermique. À partir de 1835, la fièvre du rail s'empare de l'Europe. Le réseau ferroviaire peut être considéré comme une préfiguration du réseau Internet.

Ces mutations engendrent de tout nouveaux rapports entre la science et la technique : le scientifique cesse d'être un amateur et devient un professionnel formé par des études supérieures, accédant au statut d'ingénieur. L'industrie et la recherche se stimulent mutuellement, la première devenant l"'application" de la seconde, dynamique qui s'accentuera plus tard avec la « révolution numérique ».

C'est dans ce contexte de perpétuelle innovation technique qu'émerge peu à peu une nouvelle vision du monde, le scientisme : non seulement la science supplante la religion dans sa vocation d'interpréter l'univers, mais certains estiment qu'elle doit s'arroger celle d'« organiser scientifiquement l'humanité » (la formule est du philosophe Ernest Renan). En France, les saint-simoniens considèrent que l'industrie doit prendre le pas dans la société et invitent les industriels à constituer un parti afin de prendre le pouvoir.

Les rapports à l'économie sont également bouleversés, car le progrès technique "contraint" les industriels à innover pour améliorer les taux de profit en abaissant les prix de revient. Par suite, l'économie devient de plus en plus productiviste et détermine le monde des idées, comme le démontre l'économiste allemand Karl Marx dans son étude sur les rapports entre superstructures et infrastructures.

Comme au siècle précédent, les signes annonciateurs de l'informatique sont encore très limités :

En revanche, le siècle est marqué par des inventions décisives dans le domaine des télécommunications. En 1844, Samuel Morse effectue la première démonstration publique du télégraphe sur une longue distance en envoyant un message sur une distance de , entre Philadelphie et Washington. En 1858, le premier câble transatlantique est tiré entre les États-Unis et l'Europe pour interconnecter les systèmes de communications télégraphiques des deux continents. En 1876, l'Américain Graham Bell invente le téléphone et fonde la compagnie Bell Telephone Company. Par ailleurs, l'énergie électrique est de mieux en mieux maîtrisée. En 1879, l'américain Thomas Edison invente la lampe à incandescence et en 1892, l'Allemand Karl Ferdinand Braun invente le tube cathodique qui servira aux premiers écrans de télévision puis d'ordinateurs.

Durant les cinquante premières années du siècle, un grand nombre d'inventions voient le jour et sont aussitôt mises en application par l'industrie. Toutes contribueront plus tard à la « révolution numérique ». Retenons principalement trois d'entre elles :

En parallèle, les travaux préparant l'avènement de l'informatique se poursuivent. Dans les années 1930, Fredrik Bull crée en Suisse la première entreprise développant et commercialisant des équipements mécanographiques en utilisant le principe des cartes perforées. L'Allemagne nazie s'intéresse de près à ce procédé. En 1941, à Berlin, l'ingénieur Konrad Zuse met au point le Z3, calculateur électromécanique qui constitue la première machine programmable pleinement automatique. À Londres en 1944, Colossus est le premier calculateur fondé sur le système binaire.

Mais c'est aux États-Unis, plus précisément en Californie, à quelques kilomètres de San Francisco, très exactement à Palo Alto, que s'amorce véritablement la « révolution numérique ». C'est là qu'en 1939, William Hewlett et David Packard y fondent dans un simple garage l'entreprise qui deviendra plus tard une multinationale. Cette vallée, qui sera baptisée Silicon Valley en 1971, constitue la première technopole mondiale. La fin de la Seconde Guerre mondiale marque le début d'une hégémonie des États-Unis en matière de progrès technique. En 1945, l'ingénieur Vannevar Bush imagine une machine à mémoriser stockant des microfilms. En 1946, à l'Université de Pennsylvanie, ENIAC devient le tout premier ordinateur mondial. Pesant , occupant , utilisant des tubes à vide et consommant , il effectue par seconde. En 1948 est inventé le transistor, composant semi-conducteur de très petite taille et peu consommateur en énergie : il ouvre la voie à la miniaturisation des composants, ce qui fera par la suite de l'électronique l'un des principaux secteurs de l'économie.

Alors que l'informatique est encore balbutiante, la télévision symbolise le progrès dans l'imaginaire collectif. Aux États-Unis, le nombre de récepteurs s'accroît de façon fulgurante : en 1947, en 1948, en 1949, en 1952. Témoin de l""', elle façonne les mentalités et crée la « société de consommation ». Les spots publicitaires qui y sont diffusés accentuent d'autant le phénomène de l'achat compulsif, lequel se porte en priorité sur les objets techniques.

Dès cette période, les avancées techniques donnent naissance aux premières réflexions relatives à leur impact et leur signification dans les mentalités. De 1942 à 1953 se déroulent à New York les conférences Macy, qui réunissent des mathématiciens, logiciens, anthropologues, psychologues et économistes se donnant pour objectif d'édifier une science générale du fonctionnement de l'esprit. Parmi les participants, deux courants s'opposent : d'un côté le cercle « personnalité et culture », qui établit une réciprocité entre les sciences mathématiques et physiques et les sciences psychologiques (psychanalyse, psychologie du développement...) ; de l'autre, les « cybernéticiens », comme Norbert Wiener qui introduit en science la notion de "feedback" (rétroaction) qui aura des implications lourdes dans de nombreux domaines, notamment en ingénierie, en informatique et en biologie. Wiener expose ses théories dans deux livres. Dans la seconde partie du second livre, « Cybernétique et société », il affirme que « de même qu'une révolution est en cours, permettant aux machines de remplacer les muscles de l'homme, une autre est en train de poindre qui leur permettra de se substituer à son cerveau ». Les idées de Wiener contribueront à une "adaptation" au progrès technique.

La machine occupant une place croissante dans le monde ouvrier, celui-ci se mobilise pour ne pas en être esclave et lutte pour améliorer ses conditions de travail. En 1936, dans son film "Les Temps modernes", le cinéaste anglo-américain Charles Chaplin décrit l'aliénation du travailleur par le machinisme. En 1949, l'écrivain anglais George Orwell dresse quant à lui un portrait très sombre de l'avenir. Son roman "1984" décrit un nouveau type de totalitarisme, caractérisé par la télésurveillance et le contrôle social.

Les innovations techniques successives ne sont pas sans inspirer les techniciens eux-mêmes. En 1950, dans son article « Computing Machinery and Intelligence », le mathématicien et informaticien anglais Alan Turing jette les bases de l'intelligence artificielle et fait « le pari que d'ici cinquante ans, il n'y aura plus moyen de distinguer les réponses données par un homme ou un ordinateur, et ce sur n'importe quel sujet ». Mesurant l'ampleur de cette mutation et de son impact sur les mentalités, Jacques Ellul publie, en 1954, "La Technique ou l'Enjeu du siècle" qui constitue la toute première approche anthropologique du phénomène technicien. Selon lui, le développement de l'automation conduit la technique à se développer de façon "autonome" : celle-ci échappe à tout contrôle des hommes dès lors qu'ils s'obstinent à croire qu'elle n'est qu'un moyen neutre à leur service.

Au milieu de la décennie, naît aux États-Unis l'activité radioamateur citizen-band (ou « CB », de l'anglais, « bande des citoyens »), première implication d'amateurs dans le domaine des télécommunications.

En 1957, les Soviétiques mettent sur orbite le premier satellite artificiel, . Cet événement ouvre une nouvelle étape dans l'ère des télécommunications : les satellites de télécommunications joueront plus tard un rôle indispensable dans la mise en place d'Internet.

L'année 1958 est marquée par deux événements majeurs :

En 1959, le physicien américain Richard Feynman anticipe l'exploration de l'infiniment petit et considère comme possible d'écrire de grandes quantités d'informations sur de très petites surfaces. Il déclare d'ailleurs : « Pourquoi ne pourrions-nous pas écrire l'intégralité de l"'Encyclopædia Britannica" sur une tête d'épingle ? ». Il ouvre ainsi une réflexion qui conduira aux recherches en nanotechnologie.

Les scientifiques élaborant eux-mêmes des théories et des hypothèses pour le moins surprenantes, la science-fiction s'impose comme genre littéraire. Le terme « science-fiction » lui-même a pour synonyme et concurrent direct le mot « anticipation ». Celle-ci met en scène des univers où se déroulent des faits impossibles ou non avérés en l’état actuel des techniques, mais qui correspondent à des découvertes pouvant advenir un jour. Le progrès technique devient alors un objet fantasmatique où s'expriment toutes sortes d'attentes et d'inquiétudes.

Le processus de miniaturisation des composants se poursuit, permettant la réduction des coûts de production, tandis que les langages de programmation sont de plus en plus élaborés, grâce à des algorithmes toujours plus sophistiqués. Le processus de commercialisation des ordinateurs s'amorce, mais ne concerne alors que le secteur de l'entreprise.

En 1961, démarrent les recherches qui aboutiront, vingt ans plus tard, à la naissance d'Internet. Leonard Kleinrock, étudiant au MIT, publie une théorie sur l'utilisation de la commutation de paquets pour transférer des données. En 1969, grâce à ses recherches, est conçu le projet ARPANET ("Advanced Research Projects Agency Network"), premier « réseau à transfert de paquets ». La connexion s'établit entre les laboratoires de quatre grandes universités américaines, pour le compte du Département américain de la Défense. La mise en place du dispositif ARPANET s'inscrit dans le contexte de la Guerre froide. L'objectif est de créer un réseau de télécommunications militaire à structure décentralisée capable de fonctionner malgré des coupures de lignes ou la destruction de certains systèmes. L'utilisation civile du réseau ARPANET n'était nullement envisagée à l'époque où il a été conçu.

En 1961, le Soviétique Gagarine effectue le premier vol spatial, mais peu à peu, c'est l'homme du commun qui adopte une nouvelle vision du monde. En 1967, deux ans avant que les Américains ne marchent sur la Lune, le sociologue canadien Marshall McLuhan utilise l'expression « village planétaire » pour exprimer l'idée que tout un chacun va de plus en plus éprouver le sentiment que le monde entier lui est « accessible » et que les médias ne constituent pas un moyen d'information « neutre », mais qu'ils exercent une sorte de fascination sur la conscience et modifient en profondeur le processus de la perception :

La même année, sur un autre registre, l'écrivain français Guy Debord affirme :

Son approche préfigure le concept de monde virtuel qui sera utilisé alors que des millions d'individus délaisseront de plus en plus le «monde réel» pour focaliser leur attention sur ses représentations.

Les ordinateurs ont été conçus pour jouer les auxiliaires de l'homme, mais l'importance de leurs capacités de mémoire et d'intelligence suscitent tour à tour fascination et inquiétude dans l'imaginaire collectif. Popularisé en 1960 par le neurophysiologiste Manfred Clynes et le chimiste Nathan Kline, le terme "cyborg" renvoie au concept d'humain « amélioré », mi-humain, mi-machine. En 1968, le film de Stanley Kubrick "2001, l'Odyssée de l'espace" met en scène deux astronautes en conflit avec un superordinateur qui cesse de leur obéir.

Le développement de l'informatique dans tous les domaines de la société (science, économie, armée, santé, finance, commerce...) se traduit par une augmentation sans cesse croissante de la demande en traitement des informations dans les foyers. Les jeux vidéo sont si popularisés que naît une nouvelle industrie : l'industrie vidéoludique. Or, c'est dans le domaine militaire, avec les simulateurs de vol, qu'apparaissent les premières images de synthèse.

En 1971, deux événements distincts se produisent et porteront, ensemble, la « révolution numérique » : l'invention du microprocesseur et la mise en réseau d'une vingtaine d'ordinateurs éloignés géographiquement, préfiguration d'Internet (qui ne deviendra opérationnel qu'en 1983).

La fin de la décennie voit poindre les premières inquiétudes relatives à l'impact de la numérisation des fichiers administratifs sur les libertés. En 1978 naît en France la Commission nationale de l'informatique et des libertés (CNIL)), chargée de veiller à ce que l’informatique reste au service du citoyen et qu’elle ne porte atteinte ni à l'identité humaine, ni aux droits de l’homme, ni à la vie privée.

Les sociologues commencent alors à s'inquiéter des conséquences du développement informatique et du progrès technique sur les libertés. En 1977, Jacques Ellul publie "Le Système technicien", le second volet de son triptyque consacré à l'étude de la technique qui, selon lui, est désormais constituée en un « système » menaçant les libertés fondamentales puisqu'il « formate » l'ensemble des activités humaines :
Pour Ellul, l'informatique constitue le nœud de ce système. Elle ne constitue pas un « problème en soi », mais le fait que l'on ne considère pas qu'elle n'est qu'un ensemble de représentations du réel et non le réel lui-même crée une césure entre monde réel et monde virtuel qui, in fine, menace la liberté de l'humanité tout entière si celle-ci ne la repère pas :

En 1981, l'ordinateur personnel fait irruption dans les foyers. Premier concurrent de l'Apple II, l'IBM PC est produit à plusieurs millions d'exemplaires. En 1984, Sony sort le premier baladeur numérique, deux ans après que le disque compact (CD) ait été commercialisé et ait supplanté le disque vinyle. En 1985, la NES, de la société japonaise Nintendo, domine le marché vidéo-ludique.

Face au nombre croissant de personnes s'isolant du réel au profit des univers virtuels (ordinateur, console de jeux, baladeurs, etc.), certains philosophes et sociologues s'interrogent. Tandis que Gilles Lipovetsky voit dans les contacts rapprochés avec les écrans l'une des principales raisons de la montée en puissance de l'individualisme, d'autres (notamment au sein de la sociologie des usages) y décèlent au contraire l'éclosion de nouvelles formes de sociabilité.

Le numérique transforme radicalement le rapport des hommes aux images :

1983 est une date historique : le protocole TCP/IP est officiellement adopté et le mot « Internet » fait son apparition. 562 ordinateurs sont connectés en août (on en comptera en 1984, en 1987 et en 1989). L'année suivante, la société Cisco Systems commence la conception et la commercialisation des premiers routeurs, permettant d'interconnecter divers réseaux entre eux.

L'année 1983 est marquée par un autre événement majeur : la commercialisation du premier téléphone mobile par la firme Motorola : le Motorola DynaTac 8000.

En France, comme ailleurs, le fichage électronique n'est pas vécu comme une atteinte aux libertés fondamentales, mais comme une simple commodité. La carte à puce (qui avait été brevetée en France en 1974) est diffusée au grand public comme carte téléphonique : à la fin de la décennie, le GIE "Carte bancaire" en commande 16 millions d'exemplaires.

En 1982, dans son livre "Changer de révolution", Jacques Ellul estime que le micro-ordinateur pourrait servir de vecteur à une véritable et profonde émancipation des hommes, car il favorise à la fois l'expression de leurs idées et leur coordination. Mais il faudrait selon lui agir « avant que la micro-informatique ne soit « prise » (au sens d’une banquise ou d’une mayonnaise) par le système technicien, car alors, il sera rigoureusement trop tard ». Six ans plus tard, toutefois, dans "Le Bluff technologique", il se ravise : « Actuellement, j’estime que la partie est perdue. Et que le système technicien, exalté par la puissance informatique, a échappé définitivement à la volonté directionnelle de l’homme ».

En 1984 est édité le roman "Neuromancien" (titre original : "Neuromancer") de William Gibson, premier ouvrage de science-fiction. Il est généralement considéré comme le roman fondateur du mouvement cyberpunk ayant inspiré un très grand nombre d'œuvres. La même année sort "Terminator", film d’action d’anticipation américano-britannique de James Cameron, dont le personnage principal est un cyborg assassin venu du futur et où il est question d'un système doté d'une d'intelligence artificielle faisant la guerre à l'humanité afin de l'éradiquer et d'assurer la suprématie des machines. L'œuvre connaît un succès international : quatre autres épisodes suivront jusqu'en 2015.

En 1990, ARPANET disparaît tandis que le "World Wide Web", système hypertexte public, fait son apparition. Il permet de consulter, avec un navigateur, des pages accessibles sur des sites. L’image de la toile d'araignée vient précisément des hyperliens qui lient les pages web entre elles. En 1991, l'application Gopher (aujourd'hui disparue) permet d'accéder en ligne à toutes sortes de documents et de les télécharger, ce qui constitue un événement majeur dans le domaine universitaire. En 1992, on dénombre un million d'ordinateurs connectés et quatre ans plus tard. Le protocole HTTP devient la "lingua franca" d'un réseau qui ne compte alors que 130 sites, qui se positionnent souvent en contrepoint des médias traditionnels. Mais très rapidement, cet archipel devient un labyrinthe. En quatre ans à peine, le nombre de sites explose : on en recense rapidement plus d'un million. Dès lors, l'enjeu est de se repérer dans cette masse énorme de données. Amazon est fondé en 1995, Google en 1998 et bientôt s'ouvre la bataille autour des portails d'information.

Conçu par IBM en 1992 et commercialisé deux ans plus tard, le smartphone constitue l'objet qui fait le mieux voir l'ampleur des changements ayant cours à l'ère du numérique : tenant dans la main et pouvant être utilisé presque n'importe où, il concentre toutes sortes de fonctions : téléphone, appareil photo, ordinateur, poste de radio, etc.

Parallèlement aux avancées d'Internet se poursuivent les recherches en intelligence artificielle (IA), qui inspirent nombre de futurologues. Ainsi, en 1993, le penseur transhumaniste Vernor Vinge introduit-il le concept de « singularité technologique » pour formuler l'idée qu'un jour viendra où les capacités humaines seront dépassées par celles de l'IA. Comme pour lui donner raison, en 1997, l'ordinateur Deep Blue (conçu par IBM) gagne une partie d'échecs contre Garry Kasparov, champion du monde en titre.

La même année, l'industrie vidéoludique génère pour la première fois un revenu plus important que le cinéma.

À mesure que les matériels se perfectionnent et se multiplient, quelques pathologies se développent ; en premier lieu, la dépendance.

Le progrès technique façonne alors littéralement l'économie : la multiplication des outils et le fait qu'ils soient de plus en plus sophistiqués et réunis en réseaux stimulent la « nouvelle économie », dont les maîtres mots sont « innovation » et « croissance ». Solange Ghernaouti-Hélie et Arnaud Dufour décrivent le moment d'emportement de l'économie qui débouche, en mars 2000, sur la bulle Internet : 

En 1999 sort "Matrix", film australo-américain qui connaît un succès considérable et qui raconte l'histoire d'un jeune informaticien contacté, via son ordinateur, par ce qu’il pense être un groupe de hackers, lesquels lui font comprendre que le monde dans lequel il vit n’est qu’un monde virtuel dans lequel les êtres humains sont gardés sous contrôle. Le film le décrit comme un nouveau messie : « l’Élu » qui peut sauver l'ensemble des êtres humains du joug des robots. Deux autres épisodes suivront en 2003.

En 2000, alors qu'Internet passe au haut débit, 368 millions d'ordinateurs sont connectés dans le monde. Selon le sociologue Dominique Boullier, les années 2000 « voient émerger une activité quotidienne de publication de masse sur de nouveaux terminaux qui touche la grande majorité de la population et sort l'informatique et les réseaux de leur monde professionnel ». Le réseau se "démocratise", un grand nombre d'individus se l'approprient, ils ouvrent leurs propres sites, leurs blogs, y créent "directement" de nouveaux outils sans nécessairement posséder de compétences particulières en informatique. Ce nouvel essor est promu sous l'appellation web 2.0. On ne parle plus, comme dans la décennie précédente, d"'autoroutes de l'information", mais d'une « société de communication » ou d'un web participatif.

En 2000, l'application de l'informatique et d'un Microsystème opto-électro-mécanique (MOEMS) à l'industrie du cinéma permet la réalisation par le français Philippe Binant de la première projection cinéma numérique européenne.

En 2001, dans un rapport qu'ils remettent à la National Science Foundation, les Américains William S. Bainbridge et Mihail Roco créent l'acronyme NBIC pour désigner ce qu'ils considèrent comme la « nécessaire convergence » entre les nanotechnologies, les biotechnologies, l'informatique et les sciences cognitives, c'est-à-dire l’interconnexion entre l'étude de l'infiniment petit, la fabrication du vivant, les recherches en intelligence artificielle et celles menées sur le cerveau humain. Cette convergence exigeant des mises de fonds considérables, des stratégies de développement sont conjointement élaborées par les États et le monde industriel. De même que, chez les individus, les NTIC tendent à briser les frontières traditionnelles entre vie publique et vie privée, dans la sphère économico-politique, elles contribuent à associer de plus en plus étroitement le secteur public et le secteur privé.

En 2000, la « révolution numérique » censée symboliser l'émancipation de l'humanité se montre sous le visage du chaos : la bulle Internet explose :
Alors que les entreprises réalisent de bonnes affaires, les investisseurs exagèrent l'importance du « très long terme » dans leurs estimations et négligent de prendre en compte le fait que la plupart d'entre elles consomment trop vite leur capital.

Si la bulle financière est fatale à bon nombre de dirigeants de start-ups, d'autres s'en sortent et vont même faire fortune. En 2001, Jimmy Wales et Larry Sanger fondent Wikipédia, première encyclopédie collaborative. Puis les premiers réseaux sociaux font leur apparition : en 2004, Mark Zuckerberg crée Facebook ; deux ans plus tard, Jack Dorsey met en place Twitter... après avoir irrigué la sphère professionnelle, Internet s'immisce dans tous les domaines de la vie.

Par delà les simples nuisances et désagréments spécifiques à Internet (ex. multiplication des courriels, nécessité des mises à jour...) ainsi que des dysfonctionnements à répétition (virus, spam) planent de réelles inquiétudes : les fondements traditionnels de l'éthique et de la liberté semblent menacés aussi bien par les institutions étatiques et les fournisseurs d'accès (par exemple dans le cas du déni de service) que par de simples particuliers, voire des robots.

Les réactions sur ces questions sont multiples. En 2008, en France, le projet de fichier de police informatisé Edvige soulève inquiétudes et colère dans une partie de l'opinion publique, de même que vis-à-vis de la radio-identification et des TIC en général.



En 2009 le film de science-fiction "Avatar", l’un des plus coûteux de l’histoire du cinéma et aussi l'un de ses plus gros succès, raconte l'histoire d'un homme dont la conscience est téléchargée dans le clone d'un habitant d'une planète lointaine.

Les premières années de la décennie 2010 sont caractérisées d'une part par le fait que ne cesse de s'estomper la traditionnelle distinction entre vie privée et vie publique, d'autre part que, le flux des informations circulant sur Internet ne cessant de croître, les bases de données sont de plus en plus volumineuses et coûteuses en énergie : c'est le phénomène « "big data" » (mégadonnées en français).

En 2010, Mark Zuckerberg, fondateur de Facebook, estime que les 350 millions d'utilisateurs de son site n'attachent plus autant d'importance à la protection de leurs données personnelles et considère que « la protection de la vie privée n'est plus la norme » :

L'avènement des "big data" est lié au fait que l'ensemble des informations stockées et circulant dans le monde est devenu si volumineux qu'il exige de nouveaux outils. Le "cloud computing" exprime un basculement de tendance : au lieu d'obtenir de la puissance de calcul par acquisition de matériel et de logiciel, les consommateurs se servent de la puissance mise à disposition par les fournisseurs d'accès. Le symbole de ce virage est le centre de données, extraordinairement coûteux en énergie :

La considérable avance prise par les États-Unis en matière technologique les met en situation de supériorité sur le reste de la planète, comme le montrent les nombreuses révélations d'Edward Snowden à partir de 2013, qui révèlent l’ampleur des programmes d’espionnage menés par l'Agence nationale de la sécurité américaine sur l'ensemble de la planète. Les responsables politiques ont de plus en plus de mal à exprimer leur impuissance face à la surveillance globale de la sphère numérique.

Les avancées dans les domaines de la robotique, de l'intelligence artificielle et des nanotechnologies ont pour effet de rendre l'environnement des hommes intelligent : 

En 2016, Katharine Viner, rédactrice en chef du "Guardian", énonce une théorie qui sera longuement commentée dans les semaines et les mois suivants. Elle affirme que si l'on est entré dans l'ère post-vérité, c'est en premier lieu parce que les blogs et surtout les réseaux sociaux ont ébranlé notre rapport aux faits : 

En 2017, certains critiques imaginent "un monde où la communication verbale n’est plus requise" et considèrent que "la communication cerveau à cerveau chez les humains pourrait bientôt devenir une réalité". De fait, l'année suivante, une équipe de chercheurs américains crée le réseau "BrainNet", première interface non-invasive passant directement de cerveau en cerveau. Ayant anticipé "BrainNet" dès 2014, le futurologue américain Michio Kaku, estime que son enjeu est de "remplacer Internet".

En 2014, dans le film anglo-américain "Transcendance", un ingénieur qui vient de mourir se manifeste à travers un ordinateur qu'il a préalablement conçu et qui, capable de se connecter de lui-même à tous les réseaux numériques de la planète, se révèle peu à peu omniscient et omnipotent.

Le mot « révolution » étant fortement connoté, l'expression « révolution numérique » ne fait pas consensus. Certains voient dans le progrès technique le vecteur et la condition même du progrès social ; d'autres y décèlent au contraire l'expression d'une tendance prométhéenne et le signe d'une aliénation conduisant l'humanité à sa perte.

Entre ces deux positions extrêmes, différentes attitudes et grilles de lecture sont repérables qui, chacune à sa matière, invitent à repenser l'éthique et réévaluer les notions de modernité, de liberté, de croyance, de lucidité et de responsabilité.

L'expression « révolution numérique » a été créée et est utilisée par des penseurs de sensibilité technophile et qui identifient le progrès technique au progrès de l'humanité : « "Il n'est sans doute pas exagéré de comparer la révolution numérique d'aujourd'hui à la révolution industrielle d'hier. De nouvelles barrières aux échanges sautent. Les structures, les hiérarchies et les divisions habituelles se fragilisent. Un monde dans lequel communiquer à des milliers de kilomètres et avec des milliers d'interlocuteurs devient possible sans délai, et où cela ne coûte pratiquement rien, ne fonctionne certainement plus comme le monde auquel nous sommes habitués" ».

Elle est également célébrée par les milieux libéraux qui voient en elle le moyen principal de stimuler le système capitaliste : « "La révolution numérique de l'université constitue un formidable enjeu. Sur le plan économique, l'éducation est le principal levier pour dégager des gains de productivité dans un système de production dominé par la connaissance" ».
Toutefois, c'est chez les penseurs transhumanistes que s'exprime l'éloge le plus exalté de la « révolution numérique » (et du progrès technique, de façon plus générale) puisqu'ils attendent de la convergence NBIC qu'elle transforme radicalement l'espèce humaine : « "Le transhumanisme est plus qu'une simple croyance abstraite que nous sommes sur le point de transcender nos limitations biologiques au travers de la technologie. C'est aussi une tentative pour réévaluer la définition entière de l'être humain comme on la conçoit habituellement" ».

Les transhumanistes attendent en particulier des avancées en informatique que l'on puisse un jour télécharger intégralement le contenu d'un cerveau : « "Si nous pouvions scanner la matrice synaptique d’un cerveau humain et la simuler sur un ordinateur, il serait possible pour nous de migrer de notre enveloppe biologique vers un monde totalement digital. En s’assurant que nous ayons toujours des copies de remplacement, nous pourrions effectivement jouir d’une durée de vie illimitée" ».

Aussi hallucinante que puisse paraître cette idée qui trouve son origine dans les livres de science-fiction, elle tend aujourd'hui à être appliquée. Ainsi, le « Projet du cerveau humain » (que l'Union européenne soutient financièrement depuis 2013 à hauteur d'un milliard d'euros) vise à simuler le fonctionnement du cerveau grâce à un superordinateur. L'argument avancé est de développer de nouvelles thérapies sur les maladies neurologiques.

Les milieux religieux ne sont pas forcément les plus critiques envers la « révolution numérique ». En janvier 2014, appelant les catholiques à être des « citoyens du numérique », le pape François qualifie Internet de « don de Dieu ».

Les penseurs libéraux perçoivent la « révolution numérique » comme un fait accompli et "allant de soi". Ils n'en sous-estiment pas les effets contre-productifs, voire pervers, mais ils considèrent que les hommes l'ayant « adoptée », ils doivent impérativement prendre le parti de s'y « adapter » pour en retirer le meilleur :Vue sous cet angle, « la révolution numérique » est un processus qui, étant déjà enclenché, agit sur les hommes comme une « main invisible » (au sens qu'Adam Smith donnait à cette expression pour définir le marché) : « elle ne se refuse pas » signifie qu'il n'y a pas lieu d'en critiquer les fondements. « S'en saisir », en revanche, c'est se montrer technophile non pas par idéalisme (technolâtrie), mais par pragmatisme, position que résume l'adage populaire « on n'arrête pas le progrès » et qui est aujourd'hui dominante.

De fait, l'économie planétaire étant elle-même tout entière soumise à la doctrine libérale, l'ensemble de la classe politique (de la droite institutionnelle à la social-démocratie) ainsi que les principaux acteurs économiques s'inscrivent dans cet état d'esprit. Les pouvoirs publics autant que les fournisseurs d'accès entendent réduire la fracture numérique et élargir indéfiniment l'accès à Internet : les premiers invoquent des motifs égalitaires, les seconds entendent gagner de nouvelles parts de marchés, mais les uns et les autres agissent de concert. La « révolution numérique » ne se développe donc plus comme elle s'était amorcée, de façon improvisée, mais sur la base d'une étroite collaboration entre l'État et le monde de l'industrie, non seulement dans le domaine de l'informatique, mais également dans le monde des nanotechnologies, des biotechnologies et des sciences cognitives. La convergence NBIC renforce l'esprit de consortium entre les secteurs public et privé, servant de base à des projets extrêmement ambitieux et coûteux.

Selon le sociologue Vincent Caradec, il serait faux de penser que les personnes âgées sont rétives au numérique. Catherine Gucher souligne qu'elles y recourent au contraire volontiers. Il existe de fait un véritable marché pour les personnes âgées. Quatre usages principaux ressortent : la "compagnie" (l’ordinateur s'apparente à une présence de substitution) ; la "médiation de distance" (accès à l’information, aide aux démarches administratives) ; l"'affiliation" (support de maintien dans la famille et, plus largement, au monde) ; le "divertissement" (moyen de lutter contre la routine et l’ennui).

Un très grand nombre de penseurs en sciences humaines, que ce soit en sociologie, en psychologie ou en philosophie, "s'adaptent" également à la « révolution numérique ». Leur approche se résume à l'adage « la technique n'est ni bonne ni mauvaise, tout dépend de l'usage que l'on en fait ». En France, Serge Tisseron est le plus représentatif de cette « sociologie des usages ». Celle-ci s’est développée au début des années 1980 avec le besoin d’étudier les TIC dans le monde du travail puis dans le contexte de la vie privée. Percevant l’avènement du numérique comme facteur de changements fondamentaux dans les domaines culturel, cognitif et psychologique, Tisseron propose l'expression "culture de l’écran", en regard de celle de "culture du livre". Selon lui, il n'y a pas lieu de dévaloriser la première par rapport à la seconde. Il considère par exemple que le choix de pseudos et d’avatars sur les forums et dans les jeux vidéo relève d'une quête expérimentale et constructive de son identité.

Plus explicite encore de cette adaptation à la « révolution numérique », le philosophe Michel Serres s'accommode non seulement des bouleversements intergénérationnels causés par la révolution numérique, mais il y voit le signe d'une avancée de l'humanité : 

"S'adapter à la « révolution numérique »", selon ces penseurs, revient à s'adapter au progrès technique dans son ensemble : on ne peut critiquer celui-ci que depuis ses "conséquences" (lesquelles doivent être corrigées lorsqu'elles sont négatives et anticipées pour qu'elles ne le deviennent pas, selon le principe de précaution). En revanche, les "causes" ne sont pas critiquables : « on n'arrête pas le progrès » signifie que l'on part du principe que l'homme moderne est suffisamment adulte pour le contrôler, depuis une éthique qu'il se forge lui-même librement.

Or c'est précisément ce postulat que contestent les penseurs critiques (cf paragraphe suivant).

Le phénomène « révolution numérique » participe du phénomène « progrès technique » qui, au , a provoqué différentes réactions, parmi lesquelles celle d'Herbert Marcuse, pour qui la « technoscience » est un processus n'ayant d'autre finalité que de servir le capitalisme, et celle de Jacques Ellul, qui voit dans l"'adaptation" à « la technique » précédemment décrite la marque d'un conformisme d'un nouveau type : Ellul est mort en 1994, au moment où commençait à se généraliser l'expression « révolution numérique », mais son œuvre est éclairante dans la mesure où elle comprend trois analyses détaillées du concept de révolution et trois autres du phénomène technicien. Il perçoit dans l'association des mots « révolution » et « technique » une contorsion du langage : « l'homme moderne » s'évertue à croire qu'il dirige et contrôle un processus qui, en définitive, le submerge et le contraint à se plier à ses exigences. Et s'il "sacralise" la technique, c'est parce qu'elle est porteuse d'une valeur qui surplombe toutes les anciennes valeurs (raison, liberté, égalité...) et se substitue peu à peu à elles. Tant qu'il ne l'a pas admis et compris, il ne peut prétendre contrôler le phénomène technique par les seules vertus de sa volonté.
Selon les membres de l'association "Technologos", le fait que bon nombre de discours en faveur des « nouvelles technologies » fassent aujourd'hui état d'une obligation de s'y "adapter" accrédite la thèse ellulienne que ce qui est généralement présenté comme un "progrès" relève en définitive d'une "aliénation". Exemple : L'analyse ellulienne invite à repenser le phénomène numérique dans le cadre plus large du progrès technique et celui, plus étendu encore, de la modernité : quelles sont les motivations profondes de l'être humain lorsqu'il étend et perfectionne sans cesse le parc de ses équipements ? Sont-elles conscientes et assumées ou bien relèvent-elles de l'idéologie ?

Sur le plan industriel, la révolution numérique, aussi parfois qualifiée de Industrielle, se heurte à des réserves quant à la capacité des machines à remplacer les Hommes. Alors qu'Erik Brynjolfosson et Andrew McAfee annoncent que les robots vont remplacer les Hommes, certains intellectuels et d'industriels pensent que l'Homme ne peut être remplacé. Le robot ne serait voué qu'à faire évoluer leur métier. C'est notamment l'avis de Bruno Bonnel et de Yann Le Galès, rédacteur en chef adjoint au service économie du Figaro : « Il y avait moins d’emplois dans l’Internet en 1996 quand j’ai créé Infonie, le premier fournisseur d’accès Internet que dans la robotique aujourd’hui ». Ainsi, « Hier, comme aujourd’hui, ce sont les hommes qui dirigent les machines ; sur des marchés internationaux volatiles, il faut certes des cadences de production ajustables et donc des robots performants, mais aussi, mais surtout, des hommes pour prendre les bonnes décisions d’ajustement ».

Les analyses telles que celles de Jacques Ellul restent encore assez peu étudiées. Le politologue Patrick Troude-Chastenet explique cette faible réception par le fait que, bien qu'Ellul ait mené une carrière universitaire, son discours s'écarte sensiblement des codes traditionnels marqués par l'objectivisme caractéristique des sciences sociales.

Toujours est-il que l'essor du numérique est le vecteur d'un paradoxe : il génère autant de dysfonctionnements (aux plans écologique, politique, économique, juridique, psychosocial, etc.) qu'il est régulièrement présenté comme « révolutionnaire ». Ce qui pose différentes questions : que nous "apporte" réellement cette révolution ? De quoi nous prive-t-elle et de quoi nous menace-t-elle ?

En 1999, quelques mois avant que n'éclate la bulle Internet, Dominique Wolton (spécialiste des médias et des rapports entre sciences, techniques et société) émet des réserves quant à la « révolution numérique » :
Des phénomènes tels que la vidéosurveillance, le fichage biométrique et la géolocalisation suscitent l'inquiétude qu'émerge un nouveau type de totalitarisme, tel que l'écrivain George Orwell, en 1949, dans son roman d'anticipation "1984", en faisait la description. De fait, les révélations faites en 2013 par l'informaticien Edward Snowden, ancien employé de la CIA et de la NSA, confortent la théorie « Big Brother ». Selon le romancier Marc Dugain et le journaliste Claude Labbé, « il existe un pacte secret scellé par les big data avec l'appareil le plus puissant de la planète ».

L'anthropologue Paul Jorion considère toutefois que le problème ne se pose pas de façon unilatérale : si l'État peut s'immiscer dans les communications des particuliers, l'inverse est vrai également. Ce qui, selon lui, se profile par conséquent au , c'est une « guerre civile numérique ».

Les effets négatifs de la « révolution numérique » sur l'écologie planétaire sont assez rarement soulignés. Étant donné qu'elle « dématérialise » les activités humaines, elle est souvent considérée comme susceptible de réduire l’impact de la croissance sur la biosphère, voire à résoudre la crise environnementale. Ce pourrait toutefois être le contraire : « si le monde numérique semble virtuel, les nuisances qu'il provoque, elles, sont bien réelles : la consommation des centres de données dépasse celle du trafic aérien, une recherche sur Google produit autant de que de porter à ébullition de l’eau avec une bouilloire, la fabrication des équipements nécessite l’utilisation d’une quantité considérable de matières premières, l'obsolescence des produits ne cesse d'accroître la mise au rebut de composants électroniques extrêmement polluants ». Selon Frédéric Bordage, expert en GreenIT et en sobriété numérique, le bilan environnemental du numérique est rarement meilleur que celui du papier.

La part des émissions dues au numérique est :

L'empreinte énergétique mondiale du numérique croît de 9 % par an.

Le numérique contribue à l'épuisement du stock de ressources abiotiques (minerais)

La « révolution numérique » bouleverse complètement les cadres juridiques traditionnels. La mise en ligne d'œuvres artistiques (photos, films, livres, musique...), par exemple, oblige une révision complète de la notion de propriété intellectuelle. Internet, de façon générale, inaugure de nouveaux types de crimes et délits : les infractions aux cartes bancaires (piratage), le blanchiment d'argent et l'évasion fiscale (du fait qu'il est techniquement possible à un simple particulier de rendre opaque certaines transactions), et développe certains pans de la criminalité « classique : incitation à la haine raciale ou au terrorisme, pédophilie... Par voie de conséquence, les professionnels de la police et de la justice sont donc de plus en plus formés aux techniques informatiques, qui sont toujours plus nombreuses et complexes du fait que les cybercriminels eux-mêmes progressent en niveau d'expertise.

La généralisation d'Internet et du téléphone portable, tant dans le monde du travail que dans celui de la vie quotidienne, fait apparaître des risques sanitaires (effets nocifs des ondes électromagnétiques sur le cerveau) et de nuisances. En particulier, certains penseurs considèrent la multiplication des messages comme étant chronophage, source de dépendances ou de stress, destructrice de liens sociaux et génératrice d'anxiété sociale, malgré le succès des réseaux sociaux et des forums électroniques et du fait d'une confusion généralisée entre le monde réel et ses représentations. La généralisation de l'usage de l'anonymat sur Internet invite à repenser la notion de responsabilité tandis que l'expansion des comportements addictifs oblige à reconsidérer celle de liberté, que la prolifération des informations (vérifiées ou non) rend toujours plus difficile l'exercice de l'esprit critique et que le libre accès aux sites pornographiques, malgré l'usage des filtres, bouleverse l'ensemble du champ éthique. À Nantes, en 2014 (pour la première fois en France), le milieu universitaire traite la question de l'accès au numérique en termes d'addiction, formation assurée sous la forme d'un cours en ligne.

Marc Dugain et Claude Labbé considèrent que les différents types de dysfonctionnements (juridiques, économiques, écologiques, sanitaires...) liés à ce qu'ils appellent « la dictature invisible du numérique » ont pour conséquence un relâchement lent et progressif du questionnement éthique : « cette révolution numérique ne se contente pas de modeler notre mode de vie vers plus d'information, plus de vitesse de connexion, elle nous dirige vers un état de docilité, de servitude volontaire, de transparence, dont le résultat final est la disparition de la vie privée et un renoncement irréversible à notre liberté. Leur position rejoint celle de Jacques Ellul tout en s'opposant pourtant diamétralement à elle, Ellul considérant en effet que le relâchement éthique n'est pas tant une conséquence qu'une "cause" : « l’homme [moderne] n’est pas du tout passionné par la liberté, comme il le prétend. La liberté n’est pas un besoin inhérent à la personne. Beaucoup plus constants et profonds sont les besoins de sécurité, de conformité, d’adaptation, de bonheur, d’économie des efforts (...) et l’homme est prêt à sacrifier sa liberté pour satisfaire ces besoins.

Une grande majorité des partis politiques institutionnels — hantée par « le spectre du chômage » — voit dans les nouvelles technologies le principal levier de la croissance, le secteur le plus générateur d'emplois. De nombreux débats ont lieu sur les questions de bioéthique, de propriété intellectuelle et sur les moyens de gouverner Internet, notamment pour contrer le phénomène de la cybercriminalité. Toutefois, ils restent « internes », confidentiels, réservés aux experts et aux technocrates, ne donnant lieu à aucune consultation démocratique du fait que la majorité des individus concentrent leurs intérêts sur la politique spectacle.

Les associations militantes, notamment dans la mouvance altermondialiste, comme Attac, ne s'engagent pas davantage sur la question du numérique et des technologies en général. Tout au plus est dénoncée la bienveillance avec laquelle certains gouvernements, toutes sensibilités confondues, considèrent que les entreprises "high-tech" et la façon dont elles dirigent le secteur de la recherche (qui relève du service public) s'alignent sur leurs attentes, alors que celles-ci n'ont d'autre objectif que d'accumuler les profits. La fascination des individus devant les smartphones, tablettes, jeux vidéo et autres est reconnue, mais il semble que l'on cultive parfois le vœu qu'à force d'éducation populaire, les consommateurs deviennent « consom'acteurs » ("sic") et citoyens.

Quelques sociologues s'efforcent d'analyser l'absence d'engagement critique de « la gauche » sur les questions relatives aux répercussions de la technique sur le quotidien : 

La question de l'omniprésence du numérique (et du progrès technique en général) ne suscite finalement que quelques prises de position de la part d'associations ou de groupements militants. En France, on peut repérer deux courants assez opposés, l'un plutôt "favorable" à la « révolution numérique » (notamment les milieux d'affaires), l'autre au contraire plutôt "critique" à son endroit.

Un courant "libéral" aborde la question des technologies sans remettre en cause les cadres idéologiques dans lequel elles s'inscrivent, à savoir le libéralisme et le productivisme. À l'intérieur de ce courant, on distingue deux tendances :

Un courant "critique", ou technocritique (du grec "krinein" : « trier »), traite au contraire des technologies en les contextualisant dans le champ de l'idéologie dominante, le libéralisme économique. Là également, on repère deux orientations :

Légèrement en marge de ces positionnements axés sur l'analyse de la technique et du phénomène numérique s'inscrivent des associations où l'on considère que l'évolution actuelle de nos sociétés est essentiellement déterminée par les cheminements de la science et des choix politiques qui en découlent. Cette approche est principalement défendue par l'association Vivagora (créée en 2003), la Fondation Sciences citoyennes (créée en 2006) et l'association Avicenn (créée en 2010). Ces formations se donnent pour principaux objectifs de bâtir des expertises et de lancer des signaux d'alerte.




